{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import csv\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer,AutoModelForSequenceClassification, AutoConfig, get_linear_schedule_with_warmup\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "device = 'cuda'\n",
    "MODEL_NAME = 'bert-base-chinese'\n",
    "MAX_LEN = 32\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 32 \n",
    "LR = 5e-5 \n",
    "WARMUP_STEP = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建load_dataset function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filepath, max_len):\n",
    "    label = []\n",
    "    sentences = []\n",
    "    # load dataset\n",
    "    f = open(filepath, 'r', encoding='utf-8')\n",
    "    r = csv.reader(f)\n",
    "    for item in r:\n",
    "        if r.line_num == 1:\n",
    "            continue\n",
    "        label.append(int(item[0]))\n",
    "        sentences.append(item[1])\n",
    "        \n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for data in sentences:\n",
    "        encoded_data = tokenizer.encode_plus(\n",
    "            text=data,                      # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=max_len,             # Max length to truncate/pad\n",
    "            padding='max_length',           # Pad sentence to max length\n",
    "            return_attention_mask=True,      # Return attention mask\n",
    "            truncation= True\n",
    "            )\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_data.get('input_ids'))\n",
    "        attention_masks.append(encoded_data.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "    labels = torch.tensor(label)\n",
    "    return input_ids, attention_masks, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(pre, label):\n",
    "    pre = pre.argmax(dim=1)\n",
    "    correct = torch.eq(pre, label).sum().float().item()\n",
    "    accuracy = correct / float(len(label))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.path.abspath(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(f'{path}/data/train.csv', max_len = MAX_LEN)\n",
    "valid_dataset = load_dataset(f'{path}/data/dev.csv', max_len = MAX_LEN)\n",
    "test_dataset = load_dataset(f'{path}/data/test.csv', max_len = MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "train_data = TensorDataset(train_dataset[0], train_dataset[1],train_dataset[2])\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_loader = DataLoader(train_data, sampler=train_sampler, batch_size = BATCH_SIZE)\n",
    "\n",
    "val_data = TensorDataset(valid_dataset[0],valid_dataset[1],valid_dataset[2])\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_loader = DataLoader(val_data,sampler=val_sampler, batch_size = BATCH_SIZE)\n",
    "\n",
    "test_data = TensorDataset(test_dataset[0],test_dataset[1],test_dataset[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "config.num_labels = 3\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "model.cuda()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=WARMUP_STEP,num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练...\n",
      "epoch:0 | step:20 | avg_batch_acc:0.760417 | avg_batch_loss:0.751246\n",
      "epoch:0 | step:40 | avg_batch_acc:0.751563 | avg_batch_loss:0.713393\n",
      "epoch:0 | step:60 | avg_batch_acc:0.770312 | avg_batch_loss:0.592269\n",
      "epoch:0 | step:80 | avg_batch_acc:0.815625 | avg_batch_loss:0.493874\n",
      "epoch:0 | step:100 | avg_batch_acc:0.821875 | avg_batch_loss:0.483407\n",
      "epoch:0 | step:120 | avg_batch_acc:0.815625 | avg_batch_loss:0.485651\n",
      "epoch:0 | step:140 | avg_batch_acc:0.837500 | avg_batch_loss:0.447687\n",
      "epoch:0 | step:160 | avg_batch_acc:0.801562 | avg_batch_loss:0.485109\n",
      "epoch:0 | step:180 | avg_batch_acc:0.803125 | avg_batch_loss:0.465790\n",
      "epoch:0 | step:200 | avg_batch_acc:0.809375 | avg_batch_loss:0.451604\n",
      "epoch:0 | step:220 | avg_batch_acc:0.831250 | avg_batch_loss:0.404070\n",
      "epoch:0 | step:240 | avg_batch_acc:0.790625 | avg_batch_loss:0.473081\n",
      "epoch:0 | step:260 | avg_batch_acc:0.829688 | avg_batch_loss:0.429144\n",
      "epoch:0 | step:280 | avg_batch_acc:0.832812 | avg_batch_loss:0.412479\n",
      "epoch:0 | step:300 | avg_batch_acc:0.835938 | avg_batch_loss:0.412244\n",
      "epoch:0 | step:320 | avg_batch_acc:0.840625 | avg_batch_loss:0.414643\n",
      "epoch:0 | step:340 | avg_batch_acc:0.845313 | avg_batch_loss:0.409814\n",
      "epoch:0 | step:360 | avg_batch_acc:0.828125 | avg_batch_loss:0.452770\n",
      "epoch:0 | step:380 | avg_batch_acc:0.804688 | avg_batch_loss:0.483692\n",
      "epoch:0 | step:400 | avg_batch_acc:0.859375 | avg_batch_loss:0.364493\n",
      "epoch:0 | step:420 | avg_batch_acc:0.831250 | avg_batch_loss:0.396621\n",
      "epoch:0 | step:440 | avg_batch_acc:0.810937 | avg_batch_loss:0.478549\n",
      "epoch:0 | step:460 | avg_batch_acc:0.831250 | avg_batch_loss:0.440495\n",
      "epoch:0 | step:480 | avg_batch_acc:0.854688 | avg_batch_loss:0.379479\n",
      "epoch:0 | step:500 | avg_batch_acc:0.843750 | avg_batch_loss:0.410076\n",
      "epoch:0 | step:520 | avg_batch_acc:0.848437 | avg_batch_loss:0.363517\n",
      "epoch:0 | step:540 | avg_batch_acc:0.829688 | avg_batch_loss:0.400477\n",
      "epoch:0 | step:560 | avg_batch_acc:0.846875 | avg_batch_loss:0.367248\n",
      "epoch:0 | step:580 | avg_batch_acc:0.828125 | avg_batch_loss:0.433943\n",
      "epoch:0 | step:600 | avg_batch_acc:0.839063 | avg_batch_loss:0.425926\n",
      "epoch:0 | step:620 | avg_batch_acc:0.856250 | avg_batch_loss:0.360918\n",
      "epoch:0 | step:640 | avg_batch_acc:0.832812 | avg_batch_loss:0.418736\n",
      "epoch:0 | step:660 | avg_batch_acc:0.826562 | avg_batch_loss:0.382342\n",
      "epoch:0 | step:680 | avg_batch_acc:0.846875 | avg_batch_loss:0.396469\n",
      "epoch:0 | step:700 | avg_batch_acc:0.826562 | avg_batch_loss:0.394598\n",
      "epoch:0 | step:714 | avg_batch_acc:0.806774 | avg_batch_loss:0.407622\n",
      "epoch:0 | avg_train_loss:0.4475175093103956 | val_loss:0.3856001188164776 | val_accuracy:0.8467839805825242\n",
      "epoch:1 | step:20 | avg_batch_acc:0.861607 | avg_batch_loss:0.320696\n",
      "epoch:1 | step:40 | avg_batch_acc:0.882812 | avg_batch_loss:0.289613\n",
      "epoch:1 | step:60 | avg_batch_acc:0.882812 | avg_batch_loss:0.256119\n",
      "epoch:1 | step:80 | avg_batch_acc:0.851562 | avg_batch_loss:0.336983\n",
      "epoch:1 | step:100 | avg_batch_acc:0.860938 | avg_batch_loss:0.326872\n",
      "epoch:1 | step:120 | avg_batch_acc:0.885938 | avg_batch_loss:0.316595\n",
      "epoch:1 | step:140 | avg_batch_acc:0.882812 | avg_batch_loss:0.306445\n",
      "epoch:1 | step:160 | avg_batch_acc:0.890625 | avg_batch_loss:0.269705\n",
      "epoch:1 | step:180 | avg_batch_acc:0.873437 | avg_batch_loss:0.305649\n",
      "epoch:1 | step:200 | avg_batch_acc:0.890625 | avg_batch_loss:0.297655\n",
      "epoch:1 | step:220 | avg_batch_acc:0.876563 | avg_batch_loss:0.312899\n",
      "epoch:1 | step:240 | avg_batch_acc:0.889062 | avg_batch_loss:0.291518\n",
      "epoch:1 | step:260 | avg_batch_acc:0.848437 | avg_batch_loss:0.345555\n",
      "epoch:1 | step:280 | avg_batch_acc:0.848437 | avg_batch_loss:0.306195\n",
      "epoch:1 | step:300 | avg_batch_acc:0.854688 | avg_batch_loss:0.345358\n",
      "epoch:1 | step:320 | avg_batch_acc:0.885938 | avg_batch_loss:0.291381\n",
      "epoch:1 | step:340 | avg_batch_acc:0.862500 | avg_batch_loss:0.319680\n",
      "epoch:1 | step:360 | avg_batch_acc:0.864062 | avg_batch_loss:0.320665\n",
      "epoch:1 | step:380 | avg_batch_acc:0.867188 | avg_batch_loss:0.309440\n",
      "epoch:1 | step:400 | avg_batch_acc:0.873437 | avg_batch_loss:0.327338\n",
      "epoch:1 | step:420 | avg_batch_acc:0.873437 | avg_batch_loss:0.336759\n",
      "epoch:1 | step:440 | avg_batch_acc:0.859375 | avg_batch_loss:0.341474\n",
      "epoch:1 | step:460 | avg_batch_acc:0.870313 | avg_batch_loss:0.315996\n",
      "epoch:1 | step:480 | avg_batch_acc:0.871875 | avg_batch_loss:0.292229\n",
      "epoch:1 | step:500 | avg_batch_acc:0.898438 | avg_batch_loss:0.249867\n",
      "epoch:1 | step:520 | avg_batch_acc:0.867188 | avg_batch_loss:0.344738\n",
      "epoch:1 | step:540 | avg_batch_acc:0.868750 | avg_batch_loss:0.325359\n",
      "epoch:1 | step:560 | avg_batch_acc:0.882812 | avg_batch_loss:0.273085\n",
      "epoch:1 | step:580 | avg_batch_acc:0.909375 | avg_batch_loss:0.268383\n",
      "epoch:1 | step:600 | avg_batch_acc:0.857812 | avg_batch_loss:0.355941\n",
      "epoch:1 | step:620 | avg_batch_acc:0.854688 | avg_batch_loss:0.344118\n",
      "epoch:1 | step:640 | avg_batch_acc:0.893750 | avg_batch_loss:0.278972\n",
      "epoch:1 | step:660 | avg_batch_acc:0.873437 | avg_batch_loss:0.285809\n",
      "epoch:1 | step:680 | avg_batch_acc:0.884375 | avg_batch_loss:0.301067\n",
      "epoch:1 | step:700 | avg_batch_acc:0.864062 | avg_batch_loss:0.318042\n",
      "epoch:1 | step:714 | avg_batch_acc:0.895575 | avg_batch_loss:0.300214\n",
      "epoch:1 | avg_train_loss:0.30921352374386957 | val_loss:0.38546383084313385 | val_accuracy:0.8543689320388349\n",
      "epoch:2 | step:20 | avg_batch_acc:0.924107 | avg_batch_loss:0.199098\n",
      "epoch:2 | step:40 | avg_batch_acc:0.917188 | avg_batch_loss:0.182578\n",
      "epoch:2 | step:60 | avg_batch_acc:0.893750 | avg_batch_loss:0.250092\n",
      "epoch:2 | step:80 | avg_batch_acc:0.903125 | avg_batch_loss:0.240178\n",
      "epoch:2 | step:100 | avg_batch_acc:0.923438 | avg_batch_loss:0.187414\n",
      "epoch:2 | step:120 | avg_batch_acc:0.918750 | avg_batch_loss:0.201101\n",
      "epoch:2 | step:140 | avg_batch_acc:0.929688 | avg_batch_loss:0.161835\n",
      "epoch:2 | step:160 | avg_batch_acc:0.925000 | avg_batch_loss:0.193201\n",
      "epoch:2 | step:180 | avg_batch_acc:0.925000 | avg_batch_loss:0.188614\n",
      "epoch:2 | step:200 | avg_batch_acc:0.912500 | avg_batch_loss:0.215113\n",
      "epoch:2 | step:220 | avg_batch_acc:0.921875 | avg_batch_loss:0.209482\n",
      "epoch:2 | step:240 | avg_batch_acc:0.925000 | avg_batch_loss:0.197444\n",
      "epoch:2 | step:260 | avg_batch_acc:0.925000 | avg_batch_loss:0.205234\n",
      "epoch:2 | step:280 | avg_batch_acc:0.910937 | avg_batch_loss:0.216641\n",
      "epoch:2 | step:300 | avg_batch_acc:0.932813 | avg_batch_loss:0.185943\n",
      "epoch:2 | step:320 | avg_batch_acc:0.896875 | avg_batch_loss:0.265096\n",
      "epoch:2 | step:340 | avg_batch_acc:0.909375 | avg_batch_loss:0.214090\n",
      "epoch:2 | step:360 | avg_batch_acc:0.928125 | avg_batch_loss:0.194249\n",
      "epoch:2 | step:380 | avg_batch_acc:0.928125 | avg_batch_loss:0.184294\n",
      "epoch:2 | step:400 | avg_batch_acc:0.915625 | avg_batch_loss:0.187726\n",
      "epoch:2 | step:420 | avg_batch_acc:0.929688 | avg_batch_loss:0.161341\n",
      "epoch:2 | step:440 | avg_batch_acc:0.909375 | avg_batch_loss:0.244826\n",
      "epoch:2 | step:460 | avg_batch_acc:0.904687 | avg_batch_loss:0.224134\n",
      "epoch:2 | step:480 | avg_batch_acc:0.907813 | avg_batch_loss:0.213908\n",
      "epoch:2 | step:500 | avg_batch_acc:0.917188 | avg_batch_loss:0.220215\n",
      "epoch:2 | step:520 | avg_batch_acc:0.912500 | avg_batch_loss:0.211501\n",
      "epoch:2 | step:540 | avg_batch_acc:0.912500 | avg_batch_loss:0.205855\n",
      "epoch:2 | step:560 | avg_batch_acc:0.923438 | avg_batch_loss:0.214749\n",
      "epoch:2 | step:580 | avg_batch_acc:0.932813 | avg_batch_loss:0.167878\n",
      "epoch:2 | step:600 | avg_batch_acc:0.935937 | avg_batch_loss:0.171332\n",
      "epoch:2 | step:620 | avg_batch_acc:0.932813 | avg_batch_loss:0.185161\n",
      "epoch:2 | step:640 | avg_batch_acc:0.918750 | avg_batch_loss:0.234596\n",
      "epoch:2 | step:660 | avg_batch_acc:0.890625 | avg_batch_loss:0.250351\n",
      "epoch:2 | step:680 | avg_batch_acc:0.906250 | avg_batch_loss:0.245978\n",
      "epoch:2 | step:700 | avg_batch_acc:0.903125 | avg_batch_loss:0.221824\n",
      "epoch:2 | step:714 | avg_batch_acc:0.901786 | avg_batch_loss:0.207055\n",
      "epoch:2 | avg_train_loss:0.20721581362813085 | val_loss:0.4211678989593265 | val_accuracy:0.8528519417475728\n",
      "epoch:3 | step:20 | avg_batch_acc:0.964286 | avg_batch_loss:0.101662\n",
      "epoch:3 | step:40 | avg_batch_acc:0.951562 | avg_batch_loss:0.124573\n",
      "epoch:3 | step:60 | avg_batch_acc:0.970313 | avg_batch_loss:0.088915\n",
      "epoch:3 | step:80 | avg_batch_acc:0.973437 | avg_batch_loss:0.079256\n",
      "epoch:3 | step:100 | avg_batch_acc:0.971875 | avg_batch_loss:0.078645\n",
      "epoch:3 | step:120 | avg_batch_acc:0.950000 | avg_batch_loss:0.134719\n",
      "epoch:3 | step:140 | avg_batch_acc:0.967187 | avg_batch_loss:0.108711\n",
      "epoch:3 | step:160 | avg_batch_acc:0.960938 | avg_batch_loss:0.112580\n",
      "epoch:3 | step:180 | avg_batch_acc:0.973437 | avg_batch_loss:0.101187\n",
      "epoch:3 | step:200 | avg_batch_acc:0.939063 | avg_batch_loss:0.131889\n",
      "epoch:3 | step:220 | avg_batch_acc:0.948438 | avg_batch_loss:0.138784\n",
      "epoch:3 | step:240 | avg_batch_acc:0.970313 | avg_batch_loss:0.097540\n",
      "epoch:3 | step:260 | avg_batch_acc:0.959375 | avg_batch_loss:0.110263\n",
      "epoch:3 | step:280 | avg_batch_acc:0.953125 | avg_batch_loss:0.119311\n",
      "epoch:3 | step:300 | avg_batch_acc:0.945312 | avg_batch_loss:0.157426\n",
      "epoch:3 | step:320 | avg_batch_acc:0.935937 | avg_batch_loss:0.147719\n",
      "epoch:3 | step:340 | avg_batch_acc:0.965625 | avg_batch_loss:0.113727\n",
      "epoch:3 | step:360 | avg_batch_acc:0.953125 | avg_batch_loss:0.120201\n",
      "epoch:3 | step:380 | avg_batch_acc:0.959375 | avg_batch_loss:0.101186\n",
      "epoch:3 | step:400 | avg_batch_acc:0.946875 | avg_batch_loss:0.131686\n",
      "epoch:3 | step:420 | avg_batch_acc:0.950000 | avg_batch_loss:0.121148\n",
      "epoch:3 | step:440 | avg_batch_acc:0.954688 | avg_batch_loss:0.133528\n",
      "epoch:3 | step:460 | avg_batch_acc:0.943750 | avg_batch_loss:0.119511\n",
      "epoch:3 | step:480 | avg_batch_acc:0.940625 | avg_batch_loss:0.150097\n",
      "epoch:3 | step:500 | avg_batch_acc:0.959375 | avg_batch_loss:0.109711\n",
      "epoch:3 | step:520 | avg_batch_acc:0.946875 | avg_batch_loss:0.123828\n",
      "epoch:3 | step:540 | avg_batch_acc:0.948438 | avg_batch_loss:0.121879\n",
      "epoch:3 | step:560 | avg_batch_acc:0.940625 | avg_batch_loss:0.149263\n",
      "epoch:3 | step:580 | avg_batch_acc:0.950000 | avg_batch_loss:0.133453\n",
      "epoch:3 | step:600 | avg_batch_acc:0.934375 | avg_batch_loss:0.150010\n",
      "epoch:3 | step:620 | avg_batch_acc:0.959375 | avg_batch_loss:0.134526\n",
      "epoch:3 | step:640 | avg_batch_acc:0.956250 | avg_batch_loss:0.110994\n",
      "epoch:3 | step:660 | avg_batch_acc:0.962500 | avg_batch_loss:0.105393\n",
      "epoch:3 | step:680 | avg_batch_acc:0.959375 | avg_batch_loss:0.097743\n",
      "epoch:3 | step:700 | avg_batch_acc:0.971875 | avg_batch_loss:0.106127\n",
      "epoch:3 | step:714 | avg_batch_acc:0.966518 | avg_batch_loss:0.097318\n",
      "epoch:3 | avg_train_loss:0.11861248145830798 | val_loss:0.5631809905533073 | val_accuracy:0.8567961165048543\n",
      "epoch:4 | step:20 | avg_batch_acc:0.980655 | avg_batch_loss:0.065274\n",
      "epoch:4 | step:40 | avg_batch_acc:0.975000 | avg_batch_loss:0.061874\n",
      "epoch:4 | step:60 | avg_batch_acc:0.985938 | avg_batch_loss:0.057031\n",
      "epoch:4 | step:80 | avg_batch_acc:0.979688 | avg_batch_loss:0.067747\n",
      "epoch:4 | step:100 | avg_batch_acc:0.971875 | avg_batch_loss:0.082609\n",
      "epoch:4 | step:120 | avg_batch_acc:0.979688 | avg_batch_loss:0.051894\n",
      "epoch:4 | step:140 | avg_batch_acc:0.976562 | avg_batch_loss:0.072543\n",
      "epoch:4 | step:160 | avg_batch_acc:0.971875 | avg_batch_loss:0.065332\n",
      "epoch:4 | step:180 | avg_batch_acc:0.979688 | avg_batch_loss:0.051257\n",
      "epoch:4 | step:200 | avg_batch_acc:0.982812 | avg_batch_loss:0.066326\n",
      "epoch:4 | step:220 | avg_batch_acc:0.976562 | avg_batch_loss:0.080798\n",
      "epoch:4 | step:240 | avg_batch_acc:0.981250 | avg_batch_loss:0.056719\n",
      "epoch:4 | step:260 | avg_batch_acc:0.982812 | avg_batch_loss:0.050616\n",
      "epoch:4 | step:280 | avg_batch_acc:0.973437 | avg_batch_loss:0.073426\n",
      "epoch:4 | step:300 | avg_batch_acc:0.982812 | avg_batch_loss:0.049458\n",
      "epoch:4 | step:320 | avg_batch_acc:0.978125 | avg_batch_loss:0.060615\n",
      "epoch:4 | step:340 | avg_batch_acc:0.975000 | avg_batch_loss:0.082700\n",
      "epoch:4 | step:360 | avg_batch_acc:0.971875 | avg_batch_loss:0.073107\n",
      "epoch:4 | step:380 | avg_batch_acc:0.987500 | avg_batch_loss:0.038626\n",
      "epoch:4 | step:400 | avg_batch_acc:0.985938 | avg_batch_loss:0.039745\n",
      "epoch:4 | step:420 | avg_batch_acc:0.973437 | avg_batch_loss:0.074122\n",
      "epoch:4 | step:440 | avg_batch_acc:0.975000 | avg_batch_loss:0.065574\n",
      "epoch:4 | step:460 | avg_batch_acc:0.984375 | avg_batch_loss:0.056198\n",
      "epoch:4 | step:480 | avg_batch_acc:0.965625 | avg_batch_loss:0.088279\n",
      "epoch:4 | step:500 | avg_batch_acc:0.973437 | avg_batch_loss:0.085075\n",
      "epoch:4 | step:520 | avg_batch_acc:0.973437 | avg_batch_loss:0.063121\n",
      "epoch:4 | step:540 | avg_batch_acc:0.985938 | avg_batch_loss:0.044136\n",
      "epoch:4 | step:560 | avg_batch_acc:0.970313 | avg_batch_loss:0.089107\n",
      "epoch:4 | step:580 | avg_batch_acc:0.976562 | avg_batch_loss:0.073232\n",
      "epoch:4 | step:600 | avg_batch_acc:0.976562 | avg_batch_loss:0.058463\n",
      "epoch:4 | step:620 | avg_batch_acc:0.984375 | avg_batch_loss:0.062678\n",
      "epoch:4 | step:640 | avg_batch_acc:0.987500 | avg_batch_loss:0.050255\n",
      "epoch:4 | step:660 | avg_batch_acc:0.981250 | avg_batch_loss:0.069237\n",
      "epoch:4 | step:680 | avg_batch_acc:0.964063 | avg_batch_loss:0.106191\n",
      "epoch:4 | step:700 | avg_batch_acc:0.984375 | avg_batch_loss:0.045013\n",
      "epoch:4 | step:714 | avg_batch_acc:0.970109 | avg_batch_loss:0.079949\n",
      "epoch:4 | avg_train_loss:0.06538750983797456 | val_loss:0.6449074013140595 | val_accuracy:0.8558859223300971\n",
      "保存训练完成的model...\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "print('开始训练...')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    loss_t, batch_loss, batch_acc, batch_counts = 0, 0, 0, 0\n",
    "    \n",
    "    model.train()\n",
    "    for step,batch in enumerate(train_loader):\n",
    "        batch_counts +=1\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "        outputs = model(b_input_ids, b_attn_mask, labels=b_labels)\n",
    "        loss, logits = outputs[:2]\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss_t += loss.item()\n",
    "        batch_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        acc = batch_accuracy(logits, b_labels)\n",
    "        batch_acc += acc\n",
    "        \n",
    "        if (step % 20 == 0 and step != 0) or (step == len(train_loader) - 1):\n",
    "            print(f'epoch:{epoch} | step:{step} | avg_batch_acc:{batch_acc/batch_counts:^.6f} | avg_batch_loss:{batch_loss/batch_counts:^.6f}')\n",
    "            batch_acc, batch_loss, batch_counts = 0, 0, 0\n",
    "        \n",
    "    avg_train_loss = loss_t / len(train_loader)\n",
    "    \n",
    "    #evaluate \n",
    "    val_acc, val_loss = [],[]\n",
    "    model.eval()\n",
    "    for batch in val_loader:\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, b_attn_mask, labels=b_labels)\n",
    "        loss, logits = outputs[:2]\n",
    "        val_loss.append(loss.item())\n",
    "        acc = batch_accuracy(logits, b_labels)\n",
    "        val_acc.append(acc)\n",
    "        \n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_acc)\n",
    "            \n",
    "    print(f'epoch:{epoch} | avg_train_loss:{avg_train_loss} | val_loss:{val_loss} | val_accuracy:{val_accuracy}')\n",
    "    \n",
    "torch.save(model.state_dict(), 'bert_cla.ckpt')\n",
    "print('保存训练完成的model...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始加载训练完成的model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('开始加载训练完成的model...')\n",
    "model.load_state_dict(torch.load('bert_cla.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始测试...\n"
     ]
    }
   ],
   "source": [
    "print('开始测试...')\n",
    "model.eval()\n",
    "test_result = []\n",
    "for data in test_data:\n",
    "    b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in data)\n",
    "    b_input = b_input_ids.unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input)\n",
    "        pre = outputs.logits.argmax(dim=1)\n",
    "        test_result.append([b_labels.item(), pre.item(), tokenizer.convert_ids_to_tokens(b_input_ids)])\n",
    "\n",
    "# 写入csv文件\n",
    "df = pd.DataFrame(test_result)\n",
    "df.to_csv('test_result.csv',index=False, header=['id', 'label','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('test_result.csv')\n",
    "df_e = df[df.id!=df.label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>['[CLS]', '屁', '民', '也', '是', '民', '[SEP]', '[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['[CLS]', '加', '油', '吧', '[SEP]', '[PAD]', '[P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['[CLS]', '下', '场', '加', '油', '吧', '[SEP]', '[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>['[CLS]', '舔', '屎', '[SEP]', '[PAD]', '[PAD]',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>['[CLS]', '确', '实', '[UNK]', '了', '点', '[SEP]'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6506</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['[CLS]', '讲', '详', '细', '点', '[SEP]', '[PAD]'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6512</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>['[CLS]', '富', '得', '不', '明', '显', '[SEP]', '[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6524</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['[CLS]', '应', '该', '叫', '先', '知', '[SEP]', '[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6525</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>['[CLS]', '3', '输', '了', '。', '[SEP]', '[PAD]'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6526</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>['[CLS]', '就', '是', '太', '贵', '[SEP]', '[PAD]'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>730 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  label                                               text\n",
       "3      2      0  ['[CLS]', '屁', '民', '也', '是', '民', '[SEP]', '[...\n",
       "4      1      0  ['[CLS]', '加', '油', '吧', '[SEP]', '[PAD]', '[P...\n",
       "11     1      0  ['[CLS]', '下', '场', '加', '油', '吧', '[SEP]', '[...\n",
       "32     2      0  ['[CLS]', '舔', '屎', '[SEP]', '[PAD]', '[PAD]',...\n",
       "33     2      0  ['[CLS]', '确', '实', '[UNK]', '了', '点', '[SEP]'...\n",
       "...   ..    ...                                                ...\n",
       "6506   1      0  ['[CLS]', '讲', '详', '细', '点', '[SEP]', '[PAD]'...\n",
       "6512   2      0  ['[CLS]', '富', '得', '不', '明', '显', '[SEP]', '[...\n",
       "6524   1      0  ['[CLS]', '应', '该', '叫', '先', '知', '[SEP]', '[...\n",
       "6525   2      0  ['[CLS]', '3', '输', '了', '。', '[SEP]', '[PAD]'...\n",
       "6526   2      0  ['[CLS]', '就', '是', '太', '贵', '[SEP]', '[PAD]'...\n",
       "\n",
       "[730 rows x 3 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_e[df_e.label==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>['[CLS]', '屁', '民', '也', '是', '民', '[SEP]', '[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['[CLS]', '加', '油', '吧', '[SEP]', '[PAD]', '[P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['[CLS]', '乱', '世', '佳', '人', '[SEP]', '[PAD]'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['[CLS]', '下', '场', '加', '油', '吧', '[SEP]', '[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['[CLS]', '我', '要', '尊', '严', '[SEP]', '[PAD]'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6522</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>['[CLS]', '78', '##9', '玩', '不', '起', '[SEP]',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6524</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['[CLS]', '应', '该', '叫', '先', '知', '[SEP]', '[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6525</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>['[CLS]', '3', '输', '了', '。', '[SEP]', '[PAD]'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6526</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>['[CLS]', '就', '是', '太', '贵', '[SEP]', '[PAD]'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6529</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>['[CLS]', '数', '据', '太', 'nb', '了', '[SEP]', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1011 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  label                                               text\n",
       "3      2      0  ['[CLS]', '屁', '民', '也', '是', '民', '[SEP]', '[...\n",
       "4      1      0  ['[CLS]', '加', '油', '吧', '[SEP]', '[PAD]', '[P...\n",
       "6      0      1  ['[CLS]', '乱', '世', '佳', '人', '[SEP]', '[PAD]'...\n",
       "11     1      0  ['[CLS]', '下', '场', '加', '油', '吧', '[SEP]', '[...\n",
       "30     0      1  ['[CLS]', '我', '要', '尊', '严', '[SEP]', '[PAD]'...\n",
       "...   ..    ...                                                ...\n",
       "6522   0      2  ['[CLS]', '78', '##9', '玩', '不', '起', '[SEP]',...\n",
       "6524   1      0  ['[CLS]', '应', '该', '叫', '先', '知', '[SEP]', '[...\n",
       "6525   2      0  ['[CLS]', '3', '输', '了', '。', '[SEP]', '[PAD]'...\n",
       "6526   2      0  ['[CLS]', '就', '是', '太', '贵', '[SEP]', '[PAD]'...\n",
       "6529   1      2  ['[CLS]', '数', '据', '太', 'nb', '了', '[SEP]', '...\n",
       "\n",
       "[1011 rows x 3 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8452945677123183"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1- len(df_e)/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['[CLS]', '乱', '世', '佳', '人', '[SEP]', '[PAD]'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['[CLS]', '领', '先', '[SEP]', '[PAD]', '[PAD]',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['[CLS]', '大', '兄', '弟', '你', '好', '[SEP]', '[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['[CLS]', '我', '要', '尊', '严', '[SEP]', '[PAD]'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>['[CLS]', '好', '鸡', '肋', '[SEP]', '[PAD]', '[P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6451</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['[CLS]', '还', '是', '不', '错', '8', '[SEP]', '[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6457</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['[CLS]', '关', '键', '便', '宜', '啊', '[SEP]', '[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6466</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['[CLS]', '这', '部', '经', '典', '！', '[SEP]', '[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6505</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['[CLS]', '有', '腿', '不', '错', '[SEP]', '[PAD]'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6508</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['[CLS]', '动', '漫', '美', '少', '女', '[SEP]', '[...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>436 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  label                                               text\n",
       "6      0      1  ['[CLS]', '乱', '世', '佳', '人', '[SEP]', '[PAD]'...\n",
       "16     1      1  ['[CLS]', '领', '先', '[SEP]', '[PAD]', '[PAD]',...\n",
       "25     1      1  ['[CLS]', '大', '兄', '弟', '你', '好', '[SEP]', '[...\n",
       "30     0      1  ['[CLS]', '我', '要', '尊', '严', '[SEP]', '[PAD]'...\n",
       "71     2      1  ['[CLS]', '好', '鸡', '肋', '[SEP]', '[PAD]', '[P...\n",
       "...   ..    ...                                                ...\n",
       "6451   1      1  ['[CLS]', '还', '是', '不', '错', '8', '[SEP]', '[...\n",
       "6457   1      1  ['[CLS]', '关', '键', '便', '宜', '啊', '[SEP]', '[...\n",
       "6466   1      1  ['[CLS]', '这', '部', '经', '典', '！', '[SEP]', '[...\n",
       "6505   0      1  ['[CLS]', '有', '腿', '不', '错', '[SEP]', '[PAD]'...\n",
       "6508   0      1  ['[CLS]', '动', '漫', '美', '少', '女', '[SEP]', '[...\n",
       "\n",
       "[436 rows x 3 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.label==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce613df70ec087c2b4dda2bc280e25d341f72f59d81afb32edf1d298cbbb8087"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('bert')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
