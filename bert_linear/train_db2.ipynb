{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import csv\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer,AutoModelForSequenceClassification, AutoConfig, get_linear_schedule_with_warmup\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "device = 'cuda'\n",
    "MODEL_NAME = 'bert-base-chinese'\n",
    "MAX_LEN = 32\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 32 \n",
    "LR = 5e-5 \n",
    "WARMUP_STEP = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建load_dataset function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filepath, max_len):\n",
    "    label = []\n",
    "    sentences = []\n",
    "    # load dataset\n",
    "    f = open(filepath, 'r', encoding='utf-8')\n",
    "    r = csv.reader(f)\n",
    "    for item in r:\n",
    "        if r.line_num == 1:\n",
    "            continue\n",
    "        label.append(int(item[0]))\n",
    "        sentences.append(item[1])\n",
    "        \n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for data in sentences:\n",
    "        encoded_data = tokenizer.encode_plus(\n",
    "            text=data,                      # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=max_len,             # Max length to truncate/pad\n",
    "            padding='max_length',           # Pad sentence to max length\n",
    "            return_attention_mask=True,      # Return attention mask\n",
    "            truncation= True\n",
    "            )\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_data.get('input_ids'))\n",
    "        attention_masks.append(encoded_data.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "    labels = torch.tensor(label)\n",
    "    return input_ids, attention_masks, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(pre, label):\n",
    "    pre = pre.argmax(dim=1)\n",
    "    correct = torch.eq(pre, label).sum().float().item()\n",
    "    accuracy = correct / float(len(label))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.path.abspath(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(f'{path}/data/db2/train.csv', max_len = MAX_LEN)\n",
    "valid_dataset = load_dataset(f'{path}/data/db2/dev.csv', max_len = MAX_LEN)\n",
    "test_dataset = load_dataset(f'{path}/data/db2/test.csv', max_len = MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "train_data = TensorDataset(train_dataset[0], train_dataset[1],train_dataset[2])\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_loader = DataLoader(train_data, sampler=train_sampler, batch_size = BATCH_SIZE)\n",
    "\n",
    "val_data = TensorDataset(valid_dataset[0],valid_dataset[1],valid_dataset[2])\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_loader = DataLoader(val_data,sampler=val_sampler, batch_size = BATCH_SIZE)\n",
    "\n",
    "test_data = TensorDataset(test_dataset[0],test_dataset[1],test_dataset[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "config.num_labels = 3\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "model.cuda()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=WARMUP_STEP,num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练...\n",
      "epoch:0 | step:20 | avg_batch_acc:0.425595 | avg_batch_loss:1.076826\n",
      "epoch:0 | step:40 | avg_batch_acc:0.596875 | avg_batch_loss:0.883806\n",
      "epoch:0 | step:60 | avg_batch_acc:0.770312 | avg_batch_loss:0.627835\n",
      "epoch:0 | step:80 | avg_batch_acc:0.839063 | avg_batch_loss:0.462763\n",
      "epoch:0 | step:100 | avg_batch_acc:0.831250 | avg_batch_loss:0.458916\n",
      "epoch:0 | step:120 | avg_batch_acc:0.832812 | avg_batch_loss:0.435846\n",
      "epoch:0 | step:140 | avg_batch_acc:0.840625 | avg_batch_loss:0.426829\n",
      "epoch:0 | step:160 | avg_batch_acc:0.839063 | avg_batch_loss:0.445748\n",
      "epoch:0 | step:180 | avg_batch_acc:0.848437 | avg_batch_loss:0.392678\n",
      "epoch:0 | step:200 | avg_batch_acc:0.878125 | avg_batch_loss:0.333829\n",
      "epoch:0 | step:220 | avg_batch_acc:0.865625 | avg_batch_loss:0.388672\n",
      "epoch:0 | step:240 | avg_batch_acc:0.882812 | avg_batch_loss:0.342150\n",
      "epoch:0 | step:260 | avg_batch_acc:0.867188 | avg_batch_loss:0.365514\n",
      "epoch:0 | step:280 | avg_batch_acc:0.851562 | avg_batch_loss:0.372468\n",
      "epoch:0 | step:300 | avg_batch_acc:0.845313 | avg_batch_loss:0.364000\n",
      "epoch:0 | step:320 | avg_batch_acc:0.837500 | avg_batch_loss:0.391446\n",
      "epoch:0 | step:340 | avg_batch_acc:0.860938 | avg_batch_loss:0.337426\n",
      "epoch:0 | step:360 | avg_batch_acc:0.871875 | avg_batch_loss:0.362057\n",
      "epoch:0 | step:380 | avg_batch_acc:0.864062 | avg_batch_loss:0.351781\n",
      "epoch:0 | step:400 | avg_batch_acc:0.868750 | avg_batch_loss:0.322694\n",
      "epoch:0 | step:420 | avg_batch_acc:0.839063 | avg_batch_loss:0.399317\n",
      "epoch:0 | step:440 | avg_batch_acc:0.845313 | avg_batch_loss:0.375061\n",
      "epoch:0 | step:460 | avg_batch_acc:0.887500 | avg_batch_loss:0.293413\n",
      "epoch:0 | step:480 | avg_batch_acc:0.884375 | avg_batch_loss:0.297930\n",
      "epoch:0 | step:500 | avg_batch_acc:0.878125 | avg_batch_loss:0.343123\n",
      "epoch:0 | step:520 | avg_batch_acc:0.862500 | avg_batch_loss:0.356651\n",
      "epoch:0 | step:540 | avg_batch_acc:0.885938 | avg_batch_loss:0.323603\n",
      "epoch:0 | step:560 | avg_batch_acc:0.881250 | avg_batch_loss:0.333355\n",
      "epoch:0 | step:580 | avg_batch_acc:0.881250 | avg_batch_loss:0.312049\n",
      "epoch:0 | step:600 | avg_batch_acc:0.875000 | avg_batch_loss:0.328261\n",
      "epoch:0 | step:620 | avg_batch_acc:0.871875 | avg_batch_loss:0.343092\n",
      "epoch:0 | step:640 | avg_batch_acc:0.860938 | avg_batch_loss:0.338908\n",
      "epoch:0 | step:660 | avg_batch_acc:0.882812 | avg_batch_loss:0.301634\n",
      "epoch:0 | step:680 | avg_batch_acc:0.864062 | avg_batch_loss:0.319080\n",
      "epoch:0 | step:700 | avg_batch_acc:0.890625 | avg_batch_loss:0.297179\n",
      "epoch:0 | step:720 | avg_batch_acc:0.865625 | avg_batch_loss:0.342460\n",
      "epoch:0 | step:740 | avg_batch_acc:0.879687 | avg_batch_loss:0.295621\n",
      "epoch:0 | step:760 | avg_batch_acc:0.876563 | avg_batch_loss:0.336220\n",
      "epoch:0 | step:780 | avg_batch_acc:0.871875 | avg_batch_loss:0.311955\n",
      "epoch:0 | step:800 | avg_batch_acc:0.859375 | avg_batch_loss:0.329545\n",
      "epoch:0 | step:820 | avg_batch_acc:0.889062 | avg_batch_loss:0.322547\n",
      "epoch:0 | step:840 | avg_batch_acc:0.860938 | avg_batch_loss:0.377485\n",
      "epoch:0 | step:860 | avg_batch_acc:0.881250 | avg_batch_loss:0.306445\n",
      "epoch:0 | step:880 | avg_batch_acc:0.867188 | avg_batch_loss:0.316924\n",
      "epoch:0 | step:900 | avg_batch_acc:0.876563 | avg_batch_loss:0.312343\n",
      "epoch:0 | step:920 | avg_batch_acc:0.870313 | avg_batch_loss:0.305427\n",
      "epoch:0 | step:940 | avg_batch_acc:0.879687 | avg_batch_loss:0.323422\n",
      "epoch:0 | step:960 | avg_batch_acc:0.860938 | avg_batch_loss:0.339400\n",
      "epoch:0 | step:980 | avg_batch_acc:0.870313 | avg_batch_loss:0.310694\n",
      "epoch:0 | step:1000 | avg_batch_acc:0.862500 | avg_batch_loss:0.340545\n",
      "epoch:0 | step:1020 | avg_batch_acc:0.860938 | avg_batch_loss:0.354541\n",
      "epoch:0 | step:1040 | avg_batch_acc:0.892188 | avg_batch_loss:0.297165\n",
      "epoch:0 | step:1060 | avg_batch_acc:0.887500 | avg_batch_loss:0.285572\n",
      "epoch:0 | step:1080 | avg_batch_acc:0.862500 | avg_batch_loss:0.331616\n",
      "epoch:0 | step:1100 | avg_batch_acc:0.867188 | avg_batch_loss:0.317268\n",
      "epoch:0 | step:1120 | avg_batch_acc:0.856250 | avg_batch_loss:0.358304\n",
      "epoch:0 | step:1140 | avg_batch_acc:0.900000 | avg_batch_loss:0.270414\n",
      "epoch:0 | step:1160 | avg_batch_acc:0.875000 | avg_batch_loss:0.322194\n",
      "epoch:0 | step:1180 | avg_batch_acc:0.856250 | avg_batch_loss:0.347340\n",
      "epoch:0 | step:1200 | avg_batch_acc:0.870313 | avg_batch_loss:0.333988\n",
      "epoch:0 | step:1220 | avg_batch_acc:0.859375 | avg_batch_loss:0.340970\n",
      "epoch:0 | step:1240 | avg_batch_acc:0.885938 | avg_batch_loss:0.288420\n",
      "epoch:0 | step:1260 | avg_batch_acc:0.873437 | avg_batch_loss:0.304277\n",
      "epoch:0 | step:1280 | avg_batch_acc:0.878125 | avg_batch_loss:0.306973\n",
      "epoch:0 | step:1300 | avg_batch_acc:0.879687 | avg_batch_loss:0.271656\n",
      "epoch:0 | step:1320 | avg_batch_acc:0.868750 | avg_batch_loss:0.338305\n",
      "epoch:0 | step:1340 | avg_batch_acc:0.892188 | avg_batch_loss:0.283524\n",
      "epoch:0 | step:1360 | avg_batch_acc:0.860938 | avg_batch_loss:0.327615\n",
      "epoch:0 | step:1380 | avg_batch_acc:0.859375 | avg_batch_loss:0.313328\n",
      "epoch:0 | step:1400 | avg_batch_acc:0.884375 | avg_batch_loss:0.278983\n",
      "epoch:0 | step:1420 | avg_batch_acc:0.876563 | avg_batch_loss:0.309728\n",
      "epoch:0 | step:1440 | avg_batch_acc:0.873437 | avg_batch_loss:0.317045\n",
      "epoch:0 | step:1460 | avg_batch_acc:0.868750 | avg_batch_loss:0.331809\n",
      "epoch:0 | step:1480 | avg_batch_acc:0.857812 | avg_batch_loss:0.343012\n",
      "epoch:0 | step:1500 | avg_batch_acc:0.915625 | avg_batch_loss:0.239143\n",
      "epoch:0 | step:1520 | avg_batch_acc:0.879687 | avg_batch_loss:0.288682\n",
      "epoch:0 | step:1540 | avg_batch_acc:0.884375 | avg_batch_loss:0.278448\n",
      "epoch:0 | step:1560 | avg_batch_acc:0.900000 | avg_batch_loss:0.244083\n",
      "epoch:0 | step:1580 | avg_batch_acc:0.896875 | avg_batch_loss:0.286862\n",
      "epoch:0 | step:1600 | avg_batch_acc:0.881250 | avg_batch_loss:0.272131\n",
      "epoch:0 | step:1620 | avg_batch_acc:0.865625 | avg_batch_loss:0.313618\n",
      "epoch:0 | step:1640 | avg_batch_acc:0.885938 | avg_batch_loss:0.262684\n",
      "epoch:0 | step:1660 | avg_batch_acc:0.895312 | avg_batch_loss:0.248879\n",
      "epoch:0 | step:1680 | avg_batch_acc:0.907813 | avg_batch_loss:0.253415\n",
      "epoch:0 | step:1700 | avg_batch_acc:0.876563 | avg_batch_loss:0.332445\n",
      "epoch:0 | step:1720 | avg_batch_acc:0.871875 | avg_batch_loss:0.307695\n",
      "epoch:0 | step:1740 | avg_batch_acc:0.898438 | avg_batch_loss:0.265193\n",
      "epoch:0 | step:1749 | avg_batch_acc:0.906250 | avg_batch_loss:0.245816\n",
      "epoch:0 | avg_train_loss:0.34638578631196704 | val_loss:0.2866701648682356 | val_accuracy:0.891\n",
      "epoch:1 | step:20 | avg_batch_acc:0.928571 | avg_batch_loss:0.186975\n",
      "epoch:1 | step:40 | avg_batch_acc:0.914062 | avg_batch_loss:0.226862\n",
      "epoch:1 | step:60 | avg_batch_acc:0.925000 | avg_batch_loss:0.185493\n",
      "epoch:1 | step:80 | avg_batch_acc:0.914062 | avg_batch_loss:0.228657\n",
      "epoch:1 | step:100 | avg_batch_acc:0.896875 | avg_batch_loss:0.271538\n",
      "epoch:1 | step:120 | avg_batch_acc:0.926562 | avg_batch_loss:0.229912\n",
      "epoch:1 | step:140 | avg_batch_acc:0.914062 | avg_batch_loss:0.217655\n",
      "epoch:1 | step:160 | avg_batch_acc:0.932813 | avg_batch_loss:0.171862\n",
      "epoch:1 | step:180 | avg_batch_acc:0.895312 | avg_batch_loss:0.242494\n",
      "epoch:1 | step:200 | avg_batch_acc:0.907813 | avg_batch_loss:0.233052\n",
      "epoch:1 | step:220 | avg_batch_acc:0.921875 | avg_batch_loss:0.201925\n",
      "epoch:1 | step:240 | avg_batch_acc:0.921875 | avg_batch_loss:0.217128\n",
      "epoch:1 | step:260 | avg_batch_acc:0.904687 | avg_batch_loss:0.233899\n",
      "epoch:1 | step:280 | avg_batch_acc:0.912500 | avg_batch_loss:0.225857\n",
      "epoch:1 | step:300 | avg_batch_acc:0.937500 | avg_batch_loss:0.176180\n",
      "epoch:1 | step:320 | avg_batch_acc:0.929688 | avg_batch_loss:0.174519\n",
      "epoch:1 | step:340 | avg_batch_acc:0.926562 | avg_batch_loss:0.191586\n",
      "epoch:1 | step:360 | avg_batch_acc:0.925000 | avg_batch_loss:0.182581\n",
      "epoch:1 | step:380 | avg_batch_acc:0.920312 | avg_batch_loss:0.223280\n",
      "epoch:1 | step:400 | avg_batch_acc:0.917188 | avg_batch_loss:0.197202\n",
      "epoch:1 | step:420 | avg_batch_acc:0.898438 | avg_batch_loss:0.253850\n",
      "epoch:1 | step:440 | avg_batch_acc:0.915625 | avg_batch_loss:0.248081\n",
      "epoch:1 | step:460 | avg_batch_acc:0.917188 | avg_batch_loss:0.222351\n",
      "epoch:1 | step:480 | avg_batch_acc:0.918750 | avg_batch_loss:0.200403\n",
      "epoch:1 | step:500 | avg_batch_acc:0.909375 | avg_batch_loss:0.207190\n",
      "epoch:1 | step:520 | avg_batch_acc:0.918750 | avg_batch_loss:0.209744\n",
      "epoch:1 | step:540 | avg_batch_acc:0.921875 | avg_batch_loss:0.207556\n",
      "epoch:1 | step:560 | avg_batch_acc:0.926562 | avg_batch_loss:0.187005\n",
      "epoch:1 | step:580 | avg_batch_acc:0.928125 | avg_batch_loss:0.198185\n",
      "epoch:1 | step:600 | avg_batch_acc:0.906250 | avg_batch_loss:0.223580\n",
      "epoch:1 | step:620 | avg_batch_acc:0.904687 | avg_batch_loss:0.220397\n",
      "epoch:1 | step:640 | avg_batch_acc:0.906250 | avg_batch_loss:0.227604\n",
      "epoch:1 | step:660 | avg_batch_acc:0.904687 | avg_batch_loss:0.226733\n",
      "epoch:1 | step:680 | avg_batch_acc:0.920312 | avg_batch_loss:0.201288\n",
      "epoch:1 | step:700 | avg_batch_acc:0.918750 | avg_batch_loss:0.207059\n",
      "epoch:1 | step:720 | avg_batch_acc:0.923438 | avg_batch_loss:0.194108\n",
      "epoch:1 | step:740 | avg_batch_acc:0.896875 | avg_batch_loss:0.264215\n",
      "epoch:1 | step:760 | avg_batch_acc:0.915625 | avg_batch_loss:0.202543\n",
      "epoch:1 | step:780 | avg_batch_acc:0.917188 | avg_batch_loss:0.218241\n",
      "epoch:1 | step:800 | avg_batch_acc:0.914062 | avg_batch_loss:0.206233\n",
      "epoch:1 | step:820 | avg_batch_acc:0.925000 | avg_batch_loss:0.189650\n",
      "epoch:1 | step:840 | avg_batch_acc:0.898438 | avg_batch_loss:0.264793\n",
      "epoch:1 | step:860 | avg_batch_acc:0.901563 | avg_batch_loss:0.247909\n",
      "epoch:1 | step:880 | avg_batch_acc:0.917188 | avg_batch_loss:0.216659\n",
      "epoch:1 | step:900 | avg_batch_acc:0.926562 | avg_batch_loss:0.179084\n",
      "epoch:1 | step:920 | avg_batch_acc:0.921875 | avg_batch_loss:0.206488\n",
      "epoch:1 | step:940 | avg_batch_acc:0.917188 | avg_batch_loss:0.216613\n",
      "epoch:1 | step:960 | avg_batch_acc:0.948438 | avg_batch_loss:0.157909\n",
      "epoch:1 | step:980 | avg_batch_acc:0.909375 | avg_batch_loss:0.233275\n",
      "epoch:1 | step:1000 | avg_batch_acc:0.929688 | avg_batch_loss:0.185068\n",
      "epoch:1 | step:1020 | avg_batch_acc:0.907813 | avg_batch_loss:0.230923\n",
      "epoch:1 | step:1040 | avg_batch_acc:0.903125 | avg_batch_loss:0.241899\n",
      "epoch:1 | step:1060 | avg_batch_acc:0.912500 | avg_batch_loss:0.214424\n",
      "epoch:1 | step:1080 | avg_batch_acc:0.895312 | avg_batch_loss:0.252238\n",
      "epoch:1 | step:1100 | avg_batch_acc:0.876563 | avg_batch_loss:0.261653\n",
      "epoch:1 | step:1120 | avg_batch_acc:0.929688 | avg_batch_loss:0.189268\n",
      "epoch:1 | step:1140 | avg_batch_acc:0.918750 | avg_batch_loss:0.236894\n",
      "epoch:1 | step:1160 | avg_batch_acc:0.915625 | avg_batch_loss:0.224228\n",
      "epoch:1 | step:1180 | avg_batch_acc:0.928125 | avg_batch_loss:0.182672\n",
      "epoch:1 | step:1200 | avg_batch_acc:0.914062 | avg_batch_loss:0.244293\n",
      "epoch:1 | step:1220 | avg_batch_acc:0.906250 | avg_batch_loss:0.234358\n",
      "epoch:1 | step:1240 | avg_batch_acc:0.912500 | avg_batch_loss:0.225855\n",
      "epoch:1 | step:1260 | avg_batch_acc:0.920312 | avg_batch_loss:0.203726\n",
      "epoch:1 | step:1280 | avg_batch_acc:0.926562 | avg_batch_loss:0.185989\n",
      "epoch:1 | step:1300 | avg_batch_acc:0.904687 | avg_batch_loss:0.254138\n",
      "epoch:1 | step:1320 | avg_batch_acc:0.895312 | avg_batch_loss:0.236133\n",
      "epoch:1 | step:1340 | avg_batch_acc:0.920312 | avg_batch_loss:0.210640\n",
      "epoch:1 | step:1360 | avg_batch_acc:0.914062 | avg_batch_loss:0.229092\n",
      "epoch:1 | step:1380 | avg_batch_acc:0.892188 | avg_batch_loss:0.255097\n",
      "epoch:1 | step:1400 | avg_batch_acc:0.932813 | avg_batch_loss:0.202305\n",
      "epoch:1 | step:1420 | avg_batch_acc:0.923438 | avg_batch_loss:0.201703\n",
      "epoch:1 | step:1440 | avg_batch_acc:0.921875 | avg_batch_loss:0.224143\n",
      "epoch:1 | step:1460 | avg_batch_acc:0.926562 | avg_batch_loss:0.191604\n",
      "epoch:1 | step:1480 | avg_batch_acc:0.896875 | avg_batch_loss:0.258404\n",
      "epoch:1 | step:1500 | avg_batch_acc:0.909375 | avg_batch_loss:0.221359\n",
      "epoch:1 | step:1520 | avg_batch_acc:0.903125 | avg_batch_loss:0.220425\n",
      "epoch:1 | step:1540 | avg_batch_acc:0.918750 | avg_batch_loss:0.221481\n",
      "epoch:1 | step:1560 | avg_batch_acc:0.901563 | avg_batch_loss:0.249648\n",
      "epoch:1 | step:1580 | avg_batch_acc:0.914062 | avg_batch_loss:0.231470\n",
      "epoch:1 | step:1600 | avg_batch_acc:0.925000 | avg_batch_loss:0.206710\n",
      "epoch:1 | step:1620 | avg_batch_acc:0.917188 | avg_batch_loss:0.222286\n",
      "epoch:1 | step:1640 | avg_batch_acc:0.918750 | avg_batch_loss:0.206173\n",
      "epoch:1 | step:1660 | avg_batch_acc:0.931250 | avg_batch_loss:0.194692\n",
      "epoch:1 | step:1680 | avg_batch_acc:0.912500 | avg_batch_loss:0.222928\n",
      "epoch:1 | step:1700 | avg_batch_acc:0.923438 | avg_batch_loss:0.209662\n",
      "epoch:1 | step:1720 | avg_batch_acc:0.912500 | avg_batch_loss:0.230639\n",
      "epoch:1 | step:1740 | avg_batch_acc:0.926562 | avg_batch_loss:0.192947\n",
      "epoch:1 | step:1749 | avg_batch_acc:0.927083 | avg_batch_loss:0.212132\n",
      "epoch:1 | avg_train_loss:0.2167928842242275 | val_loss:0.2572840870022774 | val_accuracy:0.903375\n",
      "epoch:2 | step:20 | avg_batch_acc:0.950893 | avg_batch_loss:0.141438\n",
      "epoch:2 | step:40 | avg_batch_acc:0.960938 | avg_batch_loss:0.102351\n",
      "epoch:2 | step:60 | avg_batch_acc:0.942187 | avg_batch_loss:0.142009\n",
      "epoch:2 | step:80 | avg_batch_acc:0.942187 | avg_batch_loss:0.131758\n",
      "epoch:2 | step:100 | avg_batch_acc:0.957812 | avg_batch_loss:0.121112\n",
      "epoch:2 | step:120 | avg_batch_acc:0.957812 | avg_batch_loss:0.135688\n",
      "epoch:2 | step:140 | avg_batch_acc:0.954688 | avg_batch_loss:0.126053\n",
      "epoch:2 | step:160 | avg_batch_acc:0.960938 | avg_batch_loss:0.118004\n",
      "epoch:2 | step:180 | avg_batch_acc:0.939063 | avg_batch_loss:0.147653\n",
      "epoch:2 | step:200 | avg_batch_acc:0.942187 | avg_batch_loss:0.154412\n",
      "epoch:2 | step:220 | avg_batch_acc:0.929688 | avg_batch_loss:0.185803\n",
      "epoch:2 | step:240 | avg_batch_acc:0.943750 | avg_batch_loss:0.132527\n",
      "epoch:2 | step:260 | avg_batch_acc:0.956250 | avg_batch_loss:0.111930\n",
      "epoch:2 | step:280 | avg_batch_acc:0.968750 | avg_batch_loss:0.086251\n",
      "epoch:2 | step:300 | avg_batch_acc:0.960938 | avg_batch_loss:0.112181\n",
      "epoch:2 | step:320 | avg_batch_acc:0.946875 | avg_batch_loss:0.142804\n",
      "epoch:2 | step:340 | avg_batch_acc:0.956250 | avg_batch_loss:0.126174\n",
      "epoch:2 | step:360 | avg_batch_acc:0.956250 | avg_batch_loss:0.118279\n",
      "epoch:2 | step:380 | avg_batch_acc:0.945312 | avg_batch_loss:0.140646\n",
      "epoch:2 | step:400 | avg_batch_acc:0.950000 | avg_batch_loss:0.144147\n",
      "epoch:2 | step:420 | avg_batch_acc:0.934375 | avg_batch_loss:0.151595\n",
      "epoch:2 | step:440 | avg_batch_acc:0.945312 | avg_batch_loss:0.156327\n",
      "epoch:2 | step:460 | avg_batch_acc:0.945312 | avg_batch_loss:0.164849\n",
      "epoch:2 | step:480 | avg_batch_acc:0.948438 | avg_batch_loss:0.137661\n",
      "epoch:2 | step:500 | avg_batch_acc:0.942187 | avg_batch_loss:0.140413\n",
      "epoch:2 | step:520 | avg_batch_acc:0.946875 | avg_batch_loss:0.134983\n",
      "epoch:2 | step:540 | avg_batch_acc:0.937500 | avg_batch_loss:0.148764\n",
      "epoch:2 | step:560 | avg_batch_acc:0.948438 | avg_batch_loss:0.130101\n",
      "epoch:2 | step:580 | avg_batch_acc:0.956250 | avg_batch_loss:0.120993\n",
      "epoch:2 | step:600 | avg_batch_acc:0.951562 | avg_batch_loss:0.143101\n",
      "epoch:2 | step:620 | avg_batch_acc:0.946875 | avg_batch_loss:0.128555\n",
      "epoch:2 | step:640 | avg_batch_acc:0.951562 | avg_batch_loss:0.135966\n",
      "epoch:2 | step:660 | avg_batch_acc:0.957812 | avg_batch_loss:0.120565\n",
      "epoch:2 | step:680 | avg_batch_acc:0.935937 | avg_batch_loss:0.143575\n",
      "epoch:2 | step:700 | avg_batch_acc:0.945312 | avg_batch_loss:0.139982\n",
      "epoch:2 | step:720 | avg_batch_acc:0.939063 | avg_batch_loss:0.146718\n",
      "epoch:2 | step:740 | avg_batch_acc:0.940625 | avg_batch_loss:0.144658\n",
      "epoch:2 | step:760 | avg_batch_acc:0.940625 | avg_batch_loss:0.159264\n",
      "epoch:2 | step:780 | avg_batch_acc:0.957812 | avg_batch_loss:0.111339\n",
      "epoch:2 | step:800 | avg_batch_acc:0.939063 | avg_batch_loss:0.169216\n",
      "epoch:2 | step:820 | avg_batch_acc:0.940625 | avg_batch_loss:0.154094\n",
      "epoch:2 | step:840 | avg_batch_acc:0.943750 | avg_batch_loss:0.133133\n",
      "epoch:2 | step:860 | avg_batch_acc:0.954688 | avg_batch_loss:0.121799\n",
      "epoch:2 | step:880 | avg_batch_acc:0.957812 | avg_batch_loss:0.117397\n",
      "epoch:2 | step:900 | avg_batch_acc:0.935937 | avg_batch_loss:0.176096\n",
      "epoch:2 | step:920 | avg_batch_acc:0.956250 | avg_batch_loss:0.126895\n",
      "epoch:2 | step:940 | avg_batch_acc:0.950000 | avg_batch_loss:0.117212\n",
      "epoch:2 | step:960 | avg_batch_acc:0.939063 | avg_batch_loss:0.146347\n",
      "epoch:2 | step:980 | avg_batch_acc:0.948438 | avg_batch_loss:0.133254\n",
      "epoch:2 | step:1000 | avg_batch_acc:0.943750 | avg_batch_loss:0.150003\n",
      "epoch:2 | step:1020 | avg_batch_acc:0.956250 | avg_batch_loss:0.118305\n",
      "epoch:2 | step:1040 | avg_batch_acc:0.960938 | avg_batch_loss:0.093316\n",
      "epoch:2 | step:1060 | avg_batch_acc:0.946875 | avg_batch_loss:0.137907\n",
      "epoch:2 | step:1080 | avg_batch_acc:0.928125 | avg_batch_loss:0.167711\n",
      "epoch:2 | step:1100 | avg_batch_acc:0.925000 | avg_batch_loss:0.204861\n",
      "epoch:2 | step:1120 | avg_batch_acc:0.962500 | avg_batch_loss:0.111673\n",
      "epoch:2 | step:1140 | avg_batch_acc:0.957812 | avg_batch_loss:0.117381\n",
      "epoch:2 | step:1160 | avg_batch_acc:0.945312 | avg_batch_loss:0.150093\n",
      "epoch:2 | step:1180 | avg_batch_acc:0.934375 | avg_batch_loss:0.145141\n",
      "epoch:2 | step:1200 | avg_batch_acc:0.945312 | avg_batch_loss:0.134132\n",
      "epoch:2 | step:1220 | avg_batch_acc:0.948438 | avg_batch_loss:0.119628\n",
      "epoch:2 | step:1240 | avg_batch_acc:0.937500 | avg_batch_loss:0.142367\n",
      "epoch:2 | step:1260 | avg_batch_acc:0.934375 | avg_batch_loss:0.150422\n",
      "epoch:2 | step:1280 | avg_batch_acc:0.954688 | avg_batch_loss:0.113846\n",
      "epoch:2 | step:1300 | avg_batch_acc:0.946875 | avg_batch_loss:0.141265\n",
      "epoch:2 | step:1320 | avg_batch_acc:0.956250 | avg_batch_loss:0.125398\n",
      "epoch:2 | step:1340 | avg_batch_acc:0.960938 | avg_batch_loss:0.108709\n",
      "epoch:2 | step:1360 | avg_batch_acc:0.946875 | avg_batch_loss:0.150270\n",
      "epoch:2 | step:1380 | avg_batch_acc:0.939063 | avg_batch_loss:0.136041\n",
      "epoch:2 | step:1400 | avg_batch_acc:0.934375 | avg_batch_loss:0.151818\n",
      "epoch:2 | step:1420 | avg_batch_acc:0.942187 | avg_batch_loss:0.129442\n",
      "epoch:2 | step:1440 | avg_batch_acc:0.942187 | avg_batch_loss:0.147218\n",
      "epoch:2 | step:1460 | avg_batch_acc:0.939063 | avg_batch_loss:0.123107\n",
      "epoch:2 | step:1480 | avg_batch_acc:0.945312 | avg_batch_loss:0.125973\n",
      "epoch:2 | step:1500 | avg_batch_acc:0.937500 | avg_batch_loss:0.146802\n",
      "epoch:2 | step:1520 | avg_batch_acc:0.962500 | avg_batch_loss:0.128934\n",
      "epoch:2 | step:1540 | avg_batch_acc:0.950000 | avg_batch_loss:0.112836\n",
      "epoch:2 | step:1560 | avg_batch_acc:0.934375 | avg_batch_loss:0.162774\n",
      "epoch:2 | step:1580 | avg_batch_acc:0.950000 | avg_batch_loss:0.131634\n",
      "epoch:2 | step:1600 | avg_batch_acc:0.954688 | avg_batch_loss:0.113781\n",
      "epoch:2 | step:1620 | avg_batch_acc:0.951562 | avg_batch_loss:0.113600\n",
      "epoch:2 | step:1640 | avg_batch_acc:0.926562 | avg_batch_loss:0.163478\n",
      "epoch:2 | step:1660 | avg_batch_acc:0.940625 | avg_batch_loss:0.125502\n",
      "epoch:2 | step:1680 | avg_batch_acc:0.950000 | avg_batch_loss:0.129933\n",
      "epoch:2 | step:1700 | avg_batch_acc:0.945312 | avg_batch_loss:0.125962\n",
      "epoch:2 | step:1720 | avg_batch_acc:0.957812 | avg_batch_loss:0.134539\n",
      "epoch:2 | step:1740 | avg_batch_acc:0.937500 | avg_batch_loss:0.162551\n",
      "epoch:2 | step:1749 | avg_batch_acc:0.940972 | avg_batch_loss:0.153801\n",
      "epoch:2 | avg_train_loss:0.13600804185548 | val_loss:0.30099874253571035 | val_accuracy:0.900125\n",
      "epoch:3 | step:20 | avg_batch_acc:0.980655 | avg_batch_loss:0.075599\n",
      "epoch:3 | step:40 | avg_batch_acc:0.978125 | avg_batch_loss:0.052931\n",
      "epoch:3 | step:60 | avg_batch_acc:0.970313 | avg_batch_loss:0.071233\n",
      "epoch:3 | step:80 | avg_batch_acc:0.970313 | avg_batch_loss:0.084397\n",
      "epoch:3 | step:100 | avg_batch_acc:0.968750 | avg_batch_loss:0.072283\n",
      "epoch:3 | step:120 | avg_batch_acc:0.968750 | avg_batch_loss:0.088147\n",
      "epoch:3 | step:140 | avg_batch_acc:0.971875 | avg_batch_loss:0.060009\n",
      "epoch:3 | step:160 | avg_batch_acc:0.973437 | avg_batch_loss:0.063077\n",
      "epoch:3 | step:180 | avg_batch_acc:0.976562 | avg_batch_loss:0.075841\n",
      "epoch:3 | step:200 | avg_batch_acc:0.979688 | avg_batch_loss:0.070115\n",
      "epoch:3 | step:220 | avg_batch_acc:0.978125 | avg_batch_loss:0.068670\n",
      "epoch:3 | step:240 | avg_batch_acc:0.973437 | avg_batch_loss:0.072108\n",
      "epoch:3 | step:260 | avg_batch_acc:0.979688 | avg_batch_loss:0.059273\n",
      "epoch:3 | step:280 | avg_batch_acc:0.976562 | avg_batch_loss:0.077664\n",
      "epoch:3 | step:300 | avg_batch_acc:0.975000 | avg_batch_loss:0.073292\n",
      "epoch:3 | step:320 | avg_batch_acc:0.967187 | avg_batch_loss:0.085792\n",
      "epoch:3 | step:340 | avg_batch_acc:0.971875 | avg_batch_loss:0.083143\n",
      "epoch:3 | step:360 | avg_batch_acc:0.978125 | avg_batch_loss:0.068985\n",
      "epoch:3 | step:380 | avg_batch_acc:0.970313 | avg_batch_loss:0.077533\n",
      "epoch:3 | step:400 | avg_batch_acc:0.967187 | avg_batch_loss:0.074635\n",
      "epoch:3 | step:420 | avg_batch_acc:0.978125 | avg_batch_loss:0.068765\n",
      "epoch:3 | step:440 | avg_batch_acc:0.965625 | avg_batch_loss:0.085486\n",
      "epoch:3 | step:460 | avg_batch_acc:0.970313 | avg_batch_loss:0.076091\n",
      "epoch:3 | step:480 | avg_batch_acc:0.982812 | avg_batch_loss:0.048737\n",
      "epoch:3 | step:500 | avg_batch_acc:0.978125 | avg_batch_loss:0.068499\n",
      "epoch:3 | step:520 | avg_batch_acc:0.967187 | avg_batch_loss:0.082944\n",
      "epoch:3 | step:540 | avg_batch_acc:0.975000 | avg_batch_loss:0.068621\n",
      "epoch:3 | step:560 | avg_batch_acc:0.968750 | avg_batch_loss:0.071653\n",
      "epoch:3 | step:580 | avg_batch_acc:0.978125 | avg_batch_loss:0.064818\n",
      "epoch:3 | step:600 | avg_batch_acc:0.978125 | avg_batch_loss:0.056429\n",
      "epoch:3 | step:620 | avg_batch_acc:0.971875 | avg_batch_loss:0.066924\n",
      "epoch:3 | step:640 | avg_batch_acc:0.984375 | avg_batch_loss:0.035241\n",
      "epoch:3 | step:660 | avg_batch_acc:0.978125 | avg_batch_loss:0.068410\n",
      "epoch:3 | step:680 | avg_batch_acc:0.968750 | avg_batch_loss:0.074690\n",
      "epoch:3 | step:700 | avg_batch_acc:0.967187 | avg_batch_loss:0.083635\n",
      "epoch:3 | step:720 | avg_batch_acc:0.967187 | avg_batch_loss:0.090711\n",
      "epoch:3 | step:740 | avg_batch_acc:0.978125 | avg_batch_loss:0.063718\n",
      "epoch:3 | step:760 | avg_batch_acc:0.973437 | avg_batch_loss:0.082353\n",
      "epoch:3 | step:780 | avg_batch_acc:0.975000 | avg_batch_loss:0.075206\n",
      "epoch:3 | step:800 | avg_batch_acc:0.979688 | avg_batch_loss:0.055954\n",
      "epoch:3 | step:820 | avg_batch_acc:0.973437 | avg_batch_loss:0.068603\n",
      "epoch:3 | step:840 | avg_batch_acc:0.975000 | avg_batch_loss:0.071703\n",
      "epoch:3 | step:860 | avg_batch_acc:0.978125 | avg_batch_loss:0.055294\n",
      "epoch:3 | step:880 | avg_batch_acc:0.973437 | avg_batch_loss:0.077291\n",
      "epoch:3 | step:900 | avg_batch_acc:0.971875 | avg_batch_loss:0.084682\n",
      "epoch:3 | step:920 | avg_batch_acc:0.973437 | avg_batch_loss:0.063527\n",
      "epoch:3 | step:940 | avg_batch_acc:0.979688 | avg_batch_loss:0.069725\n",
      "epoch:3 | step:960 | avg_batch_acc:0.978125 | avg_batch_loss:0.057347\n",
      "epoch:3 | step:980 | avg_batch_acc:0.967187 | avg_batch_loss:0.085974\n",
      "epoch:3 | step:1000 | avg_batch_acc:0.975000 | avg_batch_loss:0.070083\n",
      "epoch:3 | step:1020 | avg_batch_acc:0.968750 | avg_batch_loss:0.080108\n",
      "epoch:3 | step:1040 | avg_batch_acc:0.964063 | avg_batch_loss:0.089945\n",
      "epoch:3 | step:1060 | avg_batch_acc:0.968750 | avg_batch_loss:0.085847\n",
      "epoch:3 | step:1080 | avg_batch_acc:0.985938 | avg_batch_loss:0.050967\n",
      "epoch:3 | step:1100 | avg_batch_acc:0.971875 | avg_batch_loss:0.075443\n",
      "epoch:3 | step:1120 | avg_batch_acc:0.970313 | avg_batch_loss:0.081541\n",
      "epoch:3 | step:1140 | avg_batch_acc:0.978125 | avg_batch_loss:0.057097\n",
      "epoch:3 | step:1160 | avg_batch_acc:0.971875 | avg_batch_loss:0.064609\n",
      "epoch:3 | step:1180 | avg_batch_acc:0.964063 | avg_batch_loss:0.091507\n",
      "epoch:3 | step:1200 | avg_batch_acc:0.973437 | avg_batch_loss:0.068430\n",
      "epoch:3 | step:1220 | avg_batch_acc:0.970313 | avg_batch_loss:0.071073\n",
      "epoch:3 | step:1240 | avg_batch_acc:0.960938 | avg_batch_loss:0.117226\n",
      "epoch:3 | step:1260 | avg_batch_acc:0.964063 | avg_batch_loss:0.075352\n",
      "epoch:3 | step:1280 | avg_batch_acc:0.970313 | avg_batch_loss:0.073992\n",
      "epoch:3 | step:1300 | avg_batch_acc:0.978125 | avg_batch_loss:0.062254\n",
      "epoch:3 | step:1320 | avg_batch_acc:0.973437 | avg_batch_loss:0.064755\n",
      "epoch:3 | step:1340 | avg_batch_acc:0.970313 | avg_batch_loss:0.081745\n",
      "epoch:3 | step:1360 | avg_batch_acc:0.973437 | avg_batch_loss:0.073142\n",
      "epoch:3 | step:1380 | avg_batch_acc:0.985938 | avg_batch_loss:0.054010\n",
      "epoch:3 | step:1400 | avg_batch_acc:0.976562 | avg_batch_loss:0.060267\n",
      "epoch:3 | step:1420 | avg_batch_acc:0.962500 | avg_batch_loss:0.109993\n",
      "epoch:3 | step:1440 | avg_batch_acc:0.985938 | avg_batch_loss:0.048591\n",
      "epoch:3 | step:1460 | avg_batch_acc:0.976562 | avg_batch_loss:0.067845\n",
      "epoch:3 | step:1480 | avg_batch_acc:0.970313 | avg_batch_loss:0.067141\n",
      "epoch:3 | step:1500 | avg_batch_acc:0.989062 | avg_batch_loss:0.048924\n",
      "epoch:3 | step:1520 | avg_batch_acc:0.975000 | avg_batch_loss:0.066485\n",
      "epoch:3 | step:1540 | avg_batch_acc:0.968750 | avg_batch_loss:0.100545\n",
      "epoch:3 | step:1560 | avg_batch_acc:0.971875 | avg_batch_loss:0.070032\n",
      "epoch:3 | step:1580 | avg_batch_acc:0.962500 | avg_batch_loss:0.099604\n",
      "epoch:3 | step:1600 | avg_batch_acc:0.978125 | avg_batch_loss:0.051317\n",
      "epoch:3 | step:1620 | avg_batch_acc:0.981250 | avg_batch_loss:0.062184\n",
      "epoch:3 | step:1640 | avg_batch_acc:0.965625 | avg_batch_loss:0.086970\n",
      "epoch:3 | step:1660 | avg_batch_acc:0.975000 | avg_batch_loss:0.072820\n",
      "epoch:3 | step:1680 | avg_batch_acc:0.975000 | avg_batch_loss:0.061597\n",
      "epoch:3 | step:1700 | avg_batch_acc:0.970313 | avg_batch_loss:0.082434\n",
      "epoch:3 | step:1720 | avg_batch_acc:0.973437 | avg_batch_loss:0.061798\n",
      "epoch:3 | step:1740 | avg_batch_acc:0.968750 | avg_batch_loss:0.076854\n",
      "epoch:3 | step:1749 | avg_batch_acc:0.975694 | avg_batch_loss:0.063788\n",
      "epoch:3 | avg_train_loss:0.071940258900768 | val_loss:0.36027280741184947 | val_accuracy:0.898375\n",
      "epoch:4 | step:20 | avg_batch_acc:0.992560 | avg_batch_loss:0.027399\n",
      "epoch:4 | step:40 | avg_batch_acc:0.982812 | avg_batch_loss:0.044657\n",
      "epoch:4 | step:60 | avg_batch_acc:0.995313 | avg_batch_loss:0.019801\n",
      "epoch:4 | step:80 | avg_batch_acc:0.992188 | avg_batch_loss:0.029105\n",
      "epoch:4 | step:100 | avg_batch_acc:0.982812 | avg_batch_loss:0.045845\n",
      "epoch:4 | step:120 | avg_batch_acc:0.982812 | avg_batch_loss:0.050457\n",
      "epoch:4 | step:140 | avg_batch_acc:0.992188 | avg_batch_loss:0.017842\n",
      "epoch:4 | step:160 | avg_batch_acc:0.985938 | avg_batch_loss:0.038297\n",
      "epoch:4 | step:180 | avg_batch_acc:0.987500 | avg_batch_loss:0.037229\n",
      "epoch:4 | step:200 | avg_batch_acc:0.990625 | avg_batch_loss:0.025138\n",
      "epoch:4 | step:220 | avg_batch_acc:0.990625 | avg_batch_loss:0.036209\n",
      "epoch:4 | step:240 | avg_batch_acc:0.993750 | avg_batch_loss:0.016605\n",
      "epoch:4 | step:260 | avg_batch_acc:0.985938 | avg_batch_loss:0.041759\n",
      "epoch:4 | step:280 | avg_batch_acc:0.989062 | avg_batch_loss:0.028091\n",
      "epoch:4 | step:300 | avg_batch_acc:0.987500 | avg_batch_loss:0.036971\n",
      "epoch:4 | step:320 | avg_batch_acc:0.979688 | avg_batch_loss:0.044438\n",
      "epoch:4 | step:340 | avg_batch_acc:0.992188 | avg_batch_loss:0.031827\n",
      "epoch:4 | step:360 | avg_batch_acc:0.989062 | avg_batch_loss:0.032178\n",
      "epoch:4 | step:380 | avg_batch_acc:0.985938 | avg_batch_loss:0.050395\n",
      "epoch:4 | step:400 | avg_batch_acc:0.987500 | avg_batch_loss:0.040642\n",
      "epoch:4 | step:420 | avg_batch_acc:0.979688 | avg_batch_loss:0.050940\n",
      "epoch:4 | step:440 | avg_batch_acc:0.984375 | avg_batch_loss:0.053676\n",
      "epoch:4 | step:460 | avg_batch_acc:0.984375 | avg_batch_loss:0.039843\n",
      "epoch:4 | step:480 | avg_batch_acc:0.987500 | avg_batch_loss:0.030257\n",
      "epoch:4 | step:500 | avg_batch_acc:0.985938 | avg_batch_loss:0.040353\n",
      "epoch:4 | step:520 | avg_batch_acc:0.984375 | avg_batch_loss:0.047324\n",
      "epoch:4 | step:540 | avg_batch_acc:0.987500 | avg_batch_loss:0.029845\n",
      "epoch:4 | step:560 | avg_batch_acc:0.990625 | avg_batch_loss:0.031177\n",
      "epoch:4 | step:580 | avg_batch_acc:0.987500 | avg_batch_loss:0.046351\n",
      "epoch:4 | step:600 | avg_batch_acc:0.982812 | avg_batch_loss:0.060215\n",
      "epoch:4 | step:620 | avg_batch_acc:0.993750 | avg_batch_loss:0.026708\n",
      "epoch:4 | step:640 | avg_batch_acc:0.989062 | avg_batch_loss:0.032990\n",
      "epoch:4 | step:660 | avg_batch_acc:0.989062 | avg_batch_loss:0.032738\n",
      "epoch:4 | step:680 | avg_batch_acc:0.984375 | avg_batch_loss:0.046226\n",
      "epoch:4 | step:700 | avg_batch_acc:0.984375 | avg_batch_loss:0.041844\n",
      "epoch:4 | step:720 | avg_batch_acc:0.989062 | avg_batch_loss:0.042608\n",
      "epoch:4 | step:740 | avg_batch_acc:0.982812 | avg_batch_loss:0.047868\n",
      "epoch:4 | step:760 | avg_batch_acc:0.984375 | avg_batch_loss:0.032183\n",
      "epoch:4 | step:780 | avg_batch_acc:0.987500 | avg_batch_loss:0.029304\n",
      "epoch:4 | step:800 | avg_batch_acc:0.990625 | avg_batch_loss:0.027271\n",
      "epoch:4 | step:820 | avg_batch_acc:0.993750 | avg_batch_loss:0.024366\n",
      "epoch:4 | step:840 | avg_batch_acc:0.993750 | avg_batch_loss:0.021206\n",
      "epoch:4 | step:860 | avg_batch_acc:0.992188 | avg_batch_loss:0.029594\n",
      "epoch:4 | step:880 | avg_batch_acc:0.985938 | avg_batch_loss:0.039348\n",
      "epoch:4 | step:900 | avg_batch_acc:0.987500 | avg_batch_loss:0.032175\n",
      "epoch:4 | step:920 | avg_batch_acc:0.987500 | avg_batch_loss:0.042510\n",
      "epoch:4 | step:940 | avg_batch_acc:0.987500 | avg_batch_loss:0.028627\n",
      "epoch:4 | step:960 | avg_batch_acc:0.990625 | avg_batch_loss:0.026966\n",
      "epoch:4 | step:980 | avg_batch_acc:0.981250 | avg_batch_loss:0.058797\n",
      "epoch:4 | step:1000 | avg_batch_acc:0.987500 | avg_batch_loss:0.027483\n",
      "epoch:4 | step:1020 | avg_batch_acc:0.985938 | avg_batch_loss:0.040936\n",
      "epoch:4 | step:1040 | avg_batch_acc:0.989062 | avg_batch_loss:0.026096\n",
      "epoch:4 | step:1060 | avg_batch_acc:0.992188 | avg_batch_loss:0.029014\n",
      "epoch:4 | step:1080 | avg_batch_acc:0.987500 | avg_batch_loss:0.030753\n",
      "epoch:4 | step:1100 | avg_batch_acc:0.989062 | avg_batch_loss:0.025878\n",
      "epoch:4 | step:1120 | avg_batch_acc:0.990625 | avg_batch_loss:0.026525\n",
      "epoch:4 | step:1140 | avg_batch_acc:0.992188 | avg_batch_loss:0.022542\n",
      "epoch:4 | step:1160 | avg_batch_acc:0.989062 | avg_batch_loss:0.037488\n",
      "epoch:4 | step:1180 | avg_batch_acc:0.995313 | avg_batch_loss:0.018029\n",
      "epoch:4 | step:1200 | avg_batch_acc:0.982812 | avg_batch_loss:0.038504\n",
      "epoch:4 | step:1220 | avg_batch_acc:0.979688 | avg_batch_loss:0.047196\n",
      "epoch:4 | step:1240 | avg_batch_acc:0.992188 | avg_batch_loss:0.024046\n",
      "epoch:4 | step:1260 | avg_batch_acc:0.989062 | avg_batch_loss:0.029246\n",
      "epoch:4 | step:1280 | avg_batch_acc:0.992188 | avg_batch_loss:0.028892\n",
      "epoch:4 | step:1300 | avg_batch_acc:0.987500 | avg_batch_loss:0.031351\n",
      "epoch:4 | step:1320 | avg_batch_acc:0.992188 | avg_batch_loss:0.026157\n",
      "epoch:4 | step:1340 | avg_batch_acc:0.990625 | avg_batch_loss:0.042661\n",
      "epoch:4 | step:1360 | avg_batch_acc:0.987500 | avg_batch_loss:0.033196\n",
      "epoch:4 | step:1380 | avg_batch_acc:0.993750 | avg_batch_loss:0.031502\n",
      "epoch:4 | step:1400 | avg_batch_acc:0.989062 | avg_batch_loss:0.036974\n",
      "epoch:4 | step:1420 | avg_batch_acc:0.996875 | avg_batch_loss:0.013196\n",
      "epoch:4 | step:1440 | avg_batch_acc:0.990625 | avg_batch_loss:0.022519\n",
      "epoch:4 | step:1460 | avg_batch_acc:0.993750 | avg_batch_loss:0.021202\n",
      "epoch:4 | step:1480 | avg_batch_acc:0.987500 | avg_batch_loss:0.031361\n",
      "epoch:4 | step:1500 | avg_batch_acc:0.989062 | avg_batch_loss:0.037514\n",
      "epoch:4 | step:1520 | avg_batch_acc:0.990625 | avg_batch_loss:0.021989\n",
      "epoch:4 | step:1540 | avg_batch_acc:0.984375 | avg_batch_loss:0.031279\n",
      "epoch:4 | step:1560 | avg_batch_acc:0.984375 | avg_batch_loss:0.043529\n",
      "epoch:4 | step:1580 | avg_batch_acc:0.985938 | avg_batch_loss:0.037417\n",
      "epoch:4 | step:1600 | avg_batch_acc:0.985938 | avg_batch_loss:0.029969\n",
      "epoch:4 | step:1620 | avg_batch_acc:0.990625 | avg_batch_loss:0.035052\n",
      "epoch:4 | step:1640 | avg_batch_acc:0.990625 | avg_batch_loss:0.023109\n",
      "epoch:4 | step:1660 | avg_batch_acc:0.996875 | avg_batch_loss:0.024933\n",
      "epoch:4 | step:1680 | avg_batch_acc:0.993750 | avg_batch_loss:0.013984\n",
      "epoch:4 | step:1700 | avg_batch_acc:0.993750 | avg_batch_loss:0.016028\n",
      "epoch:4 | step:1720 | avg_batch_acc:0.982812 | avg_batch_loss:0.051899\n",
      "epoch:4 | step:1740 | avg_batch_acc:0.993750 | avg_batch_loss:0.017909\n",
      "epoch:4 | step:1749 | avg_batch_acc:0.982639 | avg_batch_loss:0.035015\n",
      "epoch:4 | avg_train_loss:0.03351638631529308 | val_loss:0.46093123947177084 | val_accuracy:0.90275\n",
      "保存训练完成的model...\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "print('开始训练...')\n",
    "loss_graph = []\n",
    "for epoch in range(EPOCHS):\n",
    "    loss_t, batch_loss, batch_acc, batch_counts = 0, 0, 0, 0\n",
    "    \n",
    "    model.train()\n",
    "    for step,batch in enumerate(train_loader):\n",
    "        batch_counts +=1\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "        outputs = model(b_input_ids, b_attn_mask, labels=b_labels)\n",
    "        loss, logits = outputs[:2]\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss_t += loss.item()\n",
    "        batch_loss += loss.item()\n",
    "        loss_graph.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        acc = batch_accuracy(logits, b_labels)\n",
    "        batch_acc += acc\n",
    "        \n",
    "        if (step % 20 == 0 and step != 0) or (step == len(train_loader) - 1):\n",
    "            print(f'epoch:{epoch} | step:{step} | avg_batch_acc:{batch_acc/batch_counts:^.6f} | avg_batch_loss:{batch_loss/batch_counts:^.6f}')\n",
    "            batch_acc, batch_loss, batch_counts = 0, 0, 0\n",
    "        \n",
    "    avg_train_loss = loss_t / len(train_loader)\n",
    "    \n",
    "    #evaluate \n",
    "    val_acc, val_loss = [],[]\n",
    "    model.eval()\n",
    "    for batch in val_loader:\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, b_attn_mask, labels=b_labels)\n",
    "        loss, logits = outputs[:2]\n",
    "        val_loss.append(loss.item())\n",
    "        acc = batch_accuracy(logits, b_labels)\n",
    "        val_acc.append(acc)\n",
    "        \n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_acc)\n",
    "            \n",
    "    print(f'epoch:{epoch} | avg_train_loss:{avg_train_loss} | val_loss:{val_loss} | val_accuracy:{val_accuracy}')\n",
    "    \n",
    "torch.save(model.state_dict(), 'bert_cla_db2.ckpt')\n",
    "print('保存训练完成的model...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始加载训练完成的model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('开始加载训练完成的model...')\n",
    "model.load_state_dict(torch.load('bert_cla_db2.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始测试...\n"
     ]
    }
   ],
   "source": [
    "print('开始测试...')\n",
    "model.eval()\n",
    "test_result = []\n",
    "for data in test_data:\n",
    "    b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in data)\n",
    "    b_input = b_input_ids.unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input)\n",
    "        pre = outputs.logits.argmax(dim=1)\n",
    "        test_result.append([b_labels.item(), pre.item(), tokenizer.convert_ids_to_tokens(b_input_ids)])\n",
    "\n",
    "# 写入csv文件\n",
    "df = pd.DataFrame(test_result)\n",
    "df.to_csv('test_result_db2.csv',index=False, header=['id', 'label','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('test_result_db2.csv')\n",
    "df_e = df[df.id!=df.label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>['[CLS]', '我', '申', '请', '了', '大', '同', '飞', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['[CLS]', '你', '好', '我', '想', '开', '通', '家', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['[CLS]', '免', '费', '领', '[UNK]', '[SEP]', '[P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>['[CLS]', '不', '开', '了', '可', '以', '吧', '[SEP]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['[CLS]', '好', '的', 'xi', '##ex', '##iel', '##...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15934</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>['[CLS]', 'i', '货', '的', '没', '有', '追', '加', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15943</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['[CLS]', '有', '什', '么', '好', '的', '套', '餐', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15946</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>['[CLS]', '毕', '节', '大', '方', '欧', '曼', '打', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15948</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['[CLS]', '更', '多', '的', '优', '惠', '[SEP]', '[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15987</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['[CLS]', '[UNK]', '[SEP]', '[PAD]', '[PAD]', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1330 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  label                                               text\n",
       "3       2      0  ['[CLS]', '我', '申', '请', '了', '大', '同', '飞', '...\n",
       "7       1      0  ['[CLS]', '你', '好', '我', '想', '开', '通', '家', '...\n",
       "15      1      0  ['[CLS]', '免', '费', '领', '[UNK]', '[SEP]', '[P...\n",
       "32      2      0  ['[CLS]', '不', '开', '了', '可', '以', '吧', '[SEP]...\n",
       "39      1      0  ['[CLS]', '好', '的', 'xi', '##ex', '##iel', '##...\n",
       "...    ..    ...                                                ...\n",
       "15934   2      0  ['[CLS]', 'i', '货', '的', '没', '有', '追', '加', '...\n",
       "15943   1      0  ['[CLS]', '有', '什', '么', '好', '的', '套', '餐', '...\n",
       "15946   2      0  ['[CLS]', '毕', '节', '大', '方', '欧', '曼', '打', '...\n",
       "15948   1      0  ['[CLS]', '更', '多', '的', '优', '惠', '[SEP]', '[...\n",
       "15987   1      0  ['[CLS]', '[UNK]', '[SEP]', '[PAD]', '[PAD]', ...\n",
       "\n",
       "[1330 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_e[df_e.label==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>['[CLS]', '我', '申', '请', '了', '大', '同', '飞', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['[CLS]', '你', '好', '我', '想', '开', '通', '家', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>['[CLS]', '没', '答', '应', '[UNK]', '[SEP]', '[P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>['[CLS]', '我', '知', '道', '我', '指', '的', '是', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['[CLS]', '免', '费', '领', '[UNK]', '[SEP]', '[P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15939</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>['[CLS]', '我', '想', '问', '一', '下', '那', '个', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15943</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['[CLS]', '有', '什', '么', '好', '的', '套', '餐', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15946</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>['[CLS]', '毕', '节', '大', '方', '欧', '曼', '打', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15948</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['[CLS]', '更', '多', '的', '优', '惠', '[SEP]', '[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15987</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['[CLS]', '[UNK]', '[SEP]', '[PAD]', '[PAD]', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2079 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  label                                               text\n",
       "3       2      0  ['[CLS]', '我', '申', '请', '了', '大', '同', '飞', '...\n",
       "7       1      0  ['[CLS]', '你', '好', '我', '想', '开', '通', '家', '...\n",
       "8       0      2  ['[CLS]', '没', '答', '应', '[UNK]', '[SEP]', '[P...\n",
       "9       0      2  ['[CLS]', '我', '知', '道', '我', '指', '的', '是', '...\n",
       "15      1      0  ['[CLS]', '免', '费', '领', '[UNK]', '[SEP]', '[P...\n",
       "...    ..    ...                                                ...\n",
       "15939   0      2  ['[CLS]', '我', '想', '问', '一', '下', '那', '个', '...\n",
       "15943   1      0  ['[CLS]', '有', '什', '么', '好', '的', '套', '餐', '...\n",
       "15946   2      0  ['[CLS]', '毕', '节', '大', '方', '欧', '曼', '打', '...\n",
       "15948   1      0  ['[CLS]', '更', '多', '的', '优', '惠', '[SEP]', '[...\n",
       "15987   1      0  ['[CLS]', '[UNK]', '[SEP]', '[PAD]', '[PAD]', ...\n",
       "\n",
       "[2079 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8700625"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1- len(df_e)/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>['[CLS]', '谢', '谢', '就', '这', '样', '吧', '希', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>['[CLS]', '不', '用', '谢', '不', '用', '谢', '草', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>['[CLS]', '如', '不', '给', '我', '满', '意', '复', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['[CLS]', '最', '便', '宜', '的', '呢', '[SEP]', '[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>['[CLS]', '移', '动', '白', '养', '活', '你', '了', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15583</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>['[CLS]', '我', '刚', '充', '完', '话', '费', '怎', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15607</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>['[CLS]', '真', '是', '无', '奇', '不', '有', '[SEP]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15740</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['[CLS]', '我', '的', '套', '餐', '语', '音', '接', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15857</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>['[CLS]', '只', '要', '不', '停', '机', '我', '就', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15924</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['[CLS]', '你', '好', '我', '在', '新', '疆', '阿', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>225 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  label                                               text\n",
       "84      2      1  ['[CLS]', '谢', '谢', '就', '这', '样', '吧', '希', '...\n",
       "111     2      1  ['[CLS]', '不', '用', '谢', '不', '用', '谢', '草', '...\n",
       "131     2      1  ['[CLS]', '如', '不', '给', '我', '满', '意', '复', '...\n",
       "241     0      1  ['[CLS]', '最', '便', '宜', '的', '呢', '[SEP]', '[...\n",
       "302     2      1  ['[CLS]', '移', '动', '白', '养', '活', '你', '了', '...\n",
       "...    ..    ...                                                ...\n",
       "15583   2      1  ['[CLS]', '我', '刚', '充', '完', '话', '费', '怎', '...\n",
       "15607   2      1  ['[CLS]', '真', '是', '无', '奇', '不', '有', '[SEP]...\n",
       "15740   0      1  ['[CLS]', '我', '的', '套', '餐', '语', '音', '接', '...\n",
       "15857   2      1  ['[CLS]', '只', '要', '不', '停', '机', '我', '就', '...\n",
       "15924   0      1  ['[CLS]', '你', '好', '我', '在', '新', '疆', '阿', '...\n",
       "\n",
       "[225 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.label==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8907 3425 3668\n"
     ]
    }
   ],
   "source": [
    "print(len(df[df.label==0]),len(df[df.label==1]),len(df[df.label==2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce613df70ec087c2b4dda2bc280e25d341f72f59d81afb32edf1d298cbbb8087"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('bert')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
