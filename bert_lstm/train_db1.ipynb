{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dwb4U_C6AuO3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import csv\n",
        "import transformers\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as Data\n",
        "import torch.nn.functional as F\n",
        "import json\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoTokenizer, AutoModel, BertConfig, get_linear_schedule_with_warmup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJNwS3i_AuPJ",
        "outputId": "81223033-a504-40bb-8712-5c6dbb18b8e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Egx-TMmlAuO8"
      },
      "outputs": [],
      "source": [
        "device = 'cuda'\n",
        "#MODEL_NAME = \"hfl/chinese-bert-wwm\"\n",
        "MODEL_NAME = 'bert-base-chinese'\n",
        "MAX_LEN = 32\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 32\n",
        "LR = 5e-5 \n",
        "WARMUP_STEPS = 100\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI8wUOpYAuO9"
      },
      "source": [
        "创建dataset类"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QM9KnoKeIhBB"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gthyrxBpAuPA"
      },
      "source": [
        "创建load_dataset function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pCoBV4CnAuPB"
      },
      "outputs": [],
      "source": [
        "def load_dataset(filepath, max_len):\n",
        "    label = []\n",
        "    sentences = []\n",
        "    # load dataset\n",
        "    f = open(filepath, 'r', encoding='utf-8')\n",
        "    r = csv.reader(f)\n",
        "    for item in r:\n",
        "        if r.line_num == 1:\n",
        "            continue\n",
        "        label.append(int(item[0]))\n",
        "        sentences.append(item[1])\n",
        "        \n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    # For every sentence...\n",
        "    for data in sentences:\n",
        "        encoded_data = tokenizer.encode_plus(\n",
        "            text=data,                      # Preprocess sentence\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            max_length=max_len,             # Max length to truncate/pad\n",
        "            padding='max_length',           # Pad sentence to max length\n",
        "            return_attention_mask=True,      # Return attention mask\n",
        "            truncation= True\n",
        "            )\n",
        "        \n",
        "        # Add the outputs to the lists\n",
        "        input_ids.append(encoded_data.get('input_ids'))\n",
        "        attention_masks.append(encoded_data.get('attention_mask'))\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "    labels = torch.tensor(label)\n",
        "    return input_ids, attention_masks, labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1B6K3IQAuPD"
      },
      "source": [
        "load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "path = os.path.abspath(os.path.dirname(os.getcwd()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AZ17DQ1iAuPE"
      },
      "outputs": [],
      "source": [
        "train_dataset = load_dataset(f'{path}/data/train.csv', max_len = MAX_LEN)\n",
        "valid_dataset = load_dataset(f'{path}/data/dev.csv', max_len = MAX_LEN)\n",
        "test_dataset = load_dataset(f'{path}/data/test.csv', max_len = MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "P_dcsj5QAuPE"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "train_data = TensorDataset(train_dataset[0], train_dataset[1],train_dataset[2])\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_loader = DataLoader(train_data, sampler=train_sampler, batch_size = BATCH_SIZE)\n",
        "\n",
        "val_data = TensorDataset(valid_dataset[0],valid_dataset[1],valid_dataset[2])\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_loader = DataLoader(val_data,sampler=val_sampler, batch_size = BATCH_SIZE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "config = BertConfig.from_pretrained(MODEL_NAME)\n",
        "config.hidden_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyXXbzFKPtai",
        "outputId": "ac00e6a8-2361-4a87-ff12-af45506b06c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 31.2 ms\n",
            "Wall time: 45 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "# Create the BertClassfier class\n",
        "class BertClassifier(nn.Module):\n",
        "    \"\"\"Bert Model for Classification Tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dimension):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.bert = BertModel.from_pretrained(MODEL_NAME)\n",
        "        \n",
        "        embedding_dim = self.bert.config.to_dict()['hidden_size']\n",
        "        \n",
        "        self.LSTM = nn.LSTM(embedding_dim,hidden_dimension,bidirectional=True, batch_first=True)\n",
        "               \n",
        "        self.out = nn.Linear(hidden_dimension * 2, 3)\n",
        "            \n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        \n",
        "       \n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "    \n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        encoded_layers = outputs[0]\n",
        "\n",
        "        #encoded_layers = encoded_layers.permute(1, 0, 2)\n",
        "        \n",
        "        enc_hiddens, (last_hidden, last_cell) = self.LSTM(encoded_layers)\n",
        "        output_hidden = torch.cat((enc_hiddens[:,-1, :256],enc_hiddens[:,0, 256:]),dim=-1)\n",
        "        output_hidden = F.dropout(output_hidden,0.2)\n",
        "        \n",
        "        logits = self.out(output_hidden)\n",
        "        \n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gZYjRSpHP18K"
      },
      "outputs": [],
      "source": [
        "def initialize_model(epochs=EPOCHS):\n",
        "    bert_classifier = BertClassifier(hidden_dimension=256)\n",
        "\n",
        "    # Tell PyTorch to run the model on GPU\n",
        "    bert_classifier.to(device)\n",
        "\n",
        "    # Create the optimizer\n",
        "    optimizer = AdamW(bert_classifier.parameters(),lr=LR)\n",
        "\n",
        "    # Set up the learning rate scheduler\n",
        "    total_steps = len(train_loader) * epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=WARMUP_STEPS,num_training_steps=total_steps)\n",
        "    return bert_classifier, optimizer, scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hXogwg3QQAi_"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "def train(model, train_dataloader, val_dataloader=None, epochs=EPOCHS , evaluation=False):\n",
        "    \"\"\"Train the BertClassifier model.\n",
        "    \"\"\"\n",
        "    # Start training loop\n",
        "    print(\"Start training...\\n\")\n",
        "    for epoch_i in range(epochs):\n",
        "        # Print the header of the result table\n",
        "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        # Measure the elapsed time of each epoch\n",
        "        t0_epoch, t0_batch = time.time(), time.time()\n",
        "\n",
        "        # Reset tracking variables at the beginning of each epoch\n",
        "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for s,batch in enumerate(train_dataloader):\n",
        "            batch_counts +=1\n",
        "            # Load batch to GPU\n",
        "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "            # Compute loss and accumulate the loss values\n",
        "            loss = loss_fn(logits.view(-1, 3), b_labels.view(-1))\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "            \n",
        "            # return loss, logits\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            \n",
        "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Print the loss values and time elapsed for every 20 batches\n",
        "            if (s % 20 == 0 and s != 0) or (s == len(train_dataloader) - 1):\n",
        "                # Calculate time elapsed for 20 batches\n",
        "                time_elapsed = time.time() - t0_batch\n",
        "\n",
        "                # Print training results\n",
        "                print(f\"{epoch_i + 1:^7} | {s:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "                # Reset batch tracking variables\n",
        "                batch_loss, batch_counts = 0, 0\n",
        "                t0_batch = time.time()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        print(\"-\"*70)\n",
        "        if evaluation == True:\n",
        "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
        "            time_elapsed = time.time() - t0_epoch \n",
        "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
        "            print(\"-\"*70)\n",
        "        print(\"\\n\")\n",
        "    \n",
        "    print(\"Training complete!\")\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        # Get the predictions\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "        # Calculate the accuracy rate\n",
        "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_loss, val_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcJjBdElQGuD",
        "outputId": "25427dde-333a-4185-f35d-c9d173cc5f4c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   1    |   20    |   0.937920   |     -      |     -     |   19.48  \n",
            "   1    |   40    |   0.727863   |     -      |     -     |   4.70   \n",
            "   1    |   60    |   0.621904   |     -      |     -     |   4.71   \n",
            "   1    |   80    |   0.509002   |     -      |     -     |   4.72   \n",
            "   1    |   100   |   0.515990   |     -      |     -     |   4.71   \n",
            "   1    |   120   |   0.442569   |     -      |     -     |   4.74   \n",
            "   1    |   140   |   0.483439   |     -      |     -     |   4.75   \n",
            "   1    |   160   |   0.467781   |     -      |     -     |   4.69   \n",
            "   1    |   180   |   0.506320   |     -      |     -     |   4.76   \n",
            "   1    |   200   |   0.418799   |     -      |     -     |   4.77   \n",
            "   1    |   220   |   0.436742   |     -      |     -     |   4.73   \n",
            "   1    |   240   |   0.408579   |     -      |     -     |   4.78   \n",
            "   1    |   260   |   0.444638   |     -      |     -     |   4.91   \n",
            "   1    |   280   |   0.412975   |     -      |     -     |   4.94   \n",
            "   1    |   300   |   0.400925   |     -      |     -     |   4.91   \n",
            "   1    |   320   |   0.360964   |     -      |     -     |   4.91   \n",
            "   1    |   340   |   0.432514   |     -      |     -     |   4.84   \n",
            "   1    |   360   |   0.384769   |     -      |     -     |   4.78   \n",
            "   1    |   380   |   0.400201   |     -      |     -     |   4.79   \n",
            "   1    |   400   |   0.441321   |     -      |     -     |   4.80   \n",
            "   1    |   420   |   0.453725   |     -      |     -     |   4.81   \n",
            "   1    |   440   |   0.347708   |     -      |     -     |   4.79   \n",
            "   1    |   460   |   0.443701   |     -      |     -     |   4.82   \n",
            "   1    |   480   |   0.389667   |     -      |     -     |   4.82   \n",
            "   1    |   500   |   0.400099   |     -      |     -     |   4.79   \n",
            "   1    |   520   |   0.354023   |     -      |     -     |   4.79   \n",
            "   1    |   540   |   0.399786   |     -      |     -     |   4.83   \n",
            "   1    |   560   |   0.389730   |     -      |     -     |   4.83   \n",
            "   1    |   580   |   0.366010   |     -      |     -     |   4.83   \n",
            "   1    |   600   |   0.382780   |     -      |     -     |   4.84   \n",
            "   1    |   620   |   0.338750   |     -      |     -     |   4.81   \n",
            "   1    |   640   |   0.378397   |     -      |     -     |   4.83   \n",
            "   1    |   660   |   0.370121   |     -      |     -     |   4.81   \n",
            "   1    |   680   |   0.364049   |     -      |     -     |   4.79   \n",
            "   1    |   700   |   0.370786   |     -      |     -     |   4.82   \n",
            "   1    |   714   |   0.409979   |     -      |     -     |   3.30   \n",
            "----------------------------------------------------------------------\n",
            "   1    |    -    |   0.443033   |  0.390249  |   84.92   |  192.07  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   2    |   20    |   0.306167   |     -      |     -     |   4.89   \n",
            "   2    |   40    |   0.297790   |     -      |     -     |   4.82   \n",
            "   2    |   60    |   0.283552   |     -      |     -     |   4.83   \n",
            "   2    |   80    |   0.334962   |     -      |     -     |   4.81   \n",
            "   2    |   100   |   0.303551   |     -      |     -     |   4.82   \n",
            "   2    |   120   |   0.271321   |     -      |     -     |   4.85   \n",
            "   2    |   140   |   0.287930   |     -      |     -     |   4.82   \n",
            "   2    |   160   |   0.317884   |     -      |     -     |   4.83   \n",
            "   2    |   180   |   0.271516   |     -      |     -     |   4.82   \n",
            "   2    |   200   |   0.287932   |     -      |     -     |   4.84   \n",
            "   2    |   220   |   0.301791   |     -      |     -     |   4.82   \n",
            "   2    |   240   |   0.317850   |     -      |     -     |   4.82   \n",
            "   2    |   260   |   0.289837   |     -      |     -     |   4.81   \n",
            "   2    |   280   |   0.273123   |     -      |     -     |   4.82   \n",
            "   2    |   300   |   0.316008   |     -      |     -     |   4.83   \n",
            "   2    |   320   |   0.276865   |     -      |     -     |   4.84   \n",
            "   2    |   340   |   0.348822   |     -      |     -     |   4.80   \n",
            "   2    |   360   |   0.260403   |     -      |     -     |   4.80   \n",
            "   2    |   380   |   0.307530   |     -      |     -     |   4.90   \n",
            "   2    |   400   |   0.260560   |     -      |     -     |   4.86   \n",
            "   2    |   420   |   0.266482   |     -      |     -     |   5.02   \n",
            "   2    |   440   |   0.312066   |     -      |     -     |   5.03   \n",
            "   2    |   460   |   0.316808   |     -      |     -     |   4.90   \n",
            "   2    |   480   |   0.320948   |     -      |     -     |   5.14   \n",
            "   2    |   500   |   0.250336   |     -      |     -     |   4.95   \n",
            "   2    |   520   |   0.263582   |     -      |     -     |   4.77   \n",
            "   2    |   540   |   0.337338   |     -      |     -     |   4.86   \n",
            "   2    |   560   |   0.325172   |     -      |     -     |   4.82   \n",
            "   2    |   580   |   0.316031   |     -      |     -     |   4.91   \n",
            "   2    |   600   |   0.309964   |     -      |     -     |   4.86   \n",
            "   2    |   620   |   0.284552   |     -      |     -     |   4.87   \n",
            "   2    |   640   |   0.271213   |     -      |     -     |   4.85   \n",
            "   2    |   660   |   0.269527   |     -      |     -     |   5.08   \n",
            "   2    |   680   |   0.292978   |     -      |     -     |   5.01   \n",
            "   2    |   700   |   0.309968   |     -      |     -     |   4.77   \n",
            "   2    |   714   |   0.328803   |     -      |     -     |   3.16   \n",
            "----------------------------------------------------------------------\n",
            "   2    |    -    |   0.296722   |  0.380133  |   84.53   |  179.55  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   3    |   20    |   0.195956   |     -      |     -     |   4.71   \n",
            "   3    |   40    |   0.209934   |     -      |     -     |   4.56   \n",
            "   3    |   60    |   0.200496   |     -      |     -     |   4.52   \n",
            "   3    |   80    |   0.161509   |     -      |     -     |   4.51   \n",
            "   3    |   100   |   0.214502   |     -      |     -     |   4.68   \n",
            "   3    |   120   |   0.201022   |     -      |     -     |   4.60   \n",
            "   3    |   140   |   0.168789   |     -      |     -     |   4.84   \n",
            "   3    |   160   |   0.191215   |     -      |     -     |   4.65   \n",
            "   3    |   180   |   0.195022   |     -      |     -     |   4.69   \n",
            "   3    |   200   |   0.201784   |     -      |     -     |   4.95   \n",
            "   3    |   220   |   0.175724   |     -      |     -     |   4.96   \n",
            "   3    |   240   |   0.168139   |     -      |     -     |   4.72   \n",
            "   3    |   260   |   0.185113   |     -      |     -     |   4.81   \n",
            "   3    |   280   |   0.168767   |     -      |     -     |   4.61   \n",
            "   3    |   300   |   0.189734   |     -      |     -     |   4.69   \n",
            "   3    |   320   |   0.190512   |     -      |     -     |   4.59   \n",
            "   3    |   340   |   0.245469   |     -      |     -     |   4.68   \n",
            "   3    |   360   |   0.202655   |     -      |     -     |   4.81   \n",
            "   3    |   380   |   0.223717   |     -      |     -     |   4.98   \n",
            "   3    |   400   |   0.196572   |     -      |     -     |   5.20   \n",
            "   3    |   420   |   0.180275   |     -      |     -     |   5.04   \n",
            "   3    |   440   |   0.157304   |     -      |     -     |   5.25   \n",
            "   3    |   460   |   0.170970   |     -      |     -     |   5.21   \n",
            "   3    |   480   |   0.211489   |     -      |     -     |   5.25   \n",
            "   3    |   500   |   0.191803   |     -      |     -     |   5.24   \n",
            "   3    |   520   |   0.155840   |     -      |     -     |   5.15   \n",
            "   3    |   540   |   0.222394   |     -      |     -     |   5.19   \n",
            "   3    |   560   |   0.211535   |     -      |     -     |   5.24   \n",
            "   3    |   580   |   0.237051   |     -      |     -     |   5.15   \n",
            "   3    |   600   |   0.222651   |     -      |     -     |   5.11   \n",
            "   3    |   620   |   0.163181   |     -      |     -     |   5.19   \n",
            "   3    |   640   |   0.258543   |     -      |     -     |   5.06   \n",
            "   3    |   660   |   0.195566   |     -      |     -     |   4.70   \n",
            "   3    |   680   |   0.186586   |     -      |     -     |   5.00   \n",
            "   3    |   700   |   0.185346   |     -      |     -     |   5.11   \n",
            "   3    |   714   |   0.183846   |     -      |     -     |   3.53   \n",
            "----------------------------------------------------------------------\n",
            "   3    |    -    |   0.195123   |  0.450387  |   85.07   |  181.75  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   4    |   20    |   0.105378   |     -      |     -     |   5.31   \n",
            "   4    |   40    |   0.114604   |     -      |     -     |   5.13   \n",
            "   4    |   60    |   0.118270   |     -      |     -     |   5.26   \n",
            "   4    |   80    |   0.124238   |     -      |     -     |   5.32   \n",
            "   4    |   100   |   0.119293   |     -      |     -     |   5.13   \n",
            "   4    |   120   |   0.080212   |     -      |     -     |   5.14   \n",
            "   4    |   140   |   0.097091   |     -      |     -     |   5.01   \n",
            "   4    |   160   |   0.112910   |     -      |     -     |   5.05   \n",
            "   4    |   180   |   0.118541   |     -      |     -     |   4.99   \n",
            "   4    |   200   |   0.108867   |     -      |     -     |   5.01   \n",
            "   4    |   220   |   0.136987   |     -      |     -     |   5.04   \n",
            "   4    |   240   |   0.124076   |     -      |     -     |   5.00   \n",
            "   4    |   260   |   0.114914   |     -      |     -     |   5.12   \n",
            "   4    |   280   |   0.110393   |     -      |     -     |   5.31   \n",
            "   4    |   300   |   0.094938   |     -      |     -     |   5.32   \n",
            "   4    |   320   |   0.111204   |     -      |     -     |   5.26   \n",
            "   4    |   340   |   0.076951   |     -      |     -     |   5.27   \n",
            "   4    |   360   |   0.115070   |     -      |     -     |   5.25   \n",
            "   4    |   380   |   0.169607   |     -      |     -     |   5.14   \n",
            "   4    |   400   |   0.126554   |     -      |     -     |   5.24   \n",
            "   4    |   420   |   0.104251   |     -      |     -     |   5.29   \n",
            "   4    |   440   |   0.134370   |     -      |     -     |   5.09   \n",
            "   4    |   460   |   0.110555   |     -      |     -     |   5.31   \n",
            "   4    |   480   |   0.140600   |     -      |     -     |   5.24   \n",
            "   4    |   500   |   0.113727   |     -      |     -     |   5.24   \n",
            "   4    |   520   |   0.114101   |     -      |     -     |   5.29   \n",
            "   4    |   540   |   0.109661   |     -      |     -     |   5.25   \n",
            "   4    |   560   |   0.089833   |     -      |     -     |   5.30   \n",
            "   4    |   580   |   0.106407   |     -      |     -     |   5.18   \n",
            "   4    |   600   |   0.077531   |     -      |     -     |   5.10   \n",
            "   4    |   620   |   0.137966   |     -      |     -     |   5.02   \n",
            "   4    |   640   |   0.133418   |     -      |     -     |   5.24   \n",
            "   4    |   660   |   0.115435   |     -      |     -     |   5.36   \n",
            "   4    |   680   |   0.117622   |     -      |     -     |   5.29   \n",
            "   4    |   700   |   0.094614   |     -      |     -     |   5.28   \n",
            "   4    |   714   |   0.096427   |     -      |     -     |   3.64   \n",
            "----------------------------------------------------------------------\n",
            "   4    |    -    |   0.113369   |  0.561542  |   85.04   |  192.40  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   5    |   20    |   0.056930   |     -      |     -     |   5.46   \n",
            "   5    |   40    |   0.057706   |     -      |     -     |   5.14   \n",
            "   5    |   60    |   0.070061   |     -      |     -     |   5.27   \n",
            "   5    |   80    |   0.058021   |     -      |     -     |   5.22   \n",
            "   5    |   100   |   0.070715   |     -      |     -     |   5.23   \n",
            "   5    |   120   |   0.063962   |     -      |     -     |   5.36   \n",
            "   5    |   140   |   0.070026   |     -      |     -     |   5.27   \n",
            "   5    |   160   |   0.058217   |     -      |     -     |   5.38   \n",
            "   5    |   180   |   0.057542   |     -      |     -     |   5.26   \n",
            "   5    |   200   |   0.051215   |     -      |     -     |   5.34   \n",
            "   5    |   220   |   0.053470   |     -      |     -     |   5.33   \n",
            "   5    |   240   |   0.071306   |     -      |     -     |   5.26   \n",
            "   5    |   260   |   0.056878   |     -      |     -     |   5.24   \n",
            "   5    |   280   |   0.054198   |     -      |     -     |   5.31   \n",
            "   5    |   300   |   0.037225   |     -      |     -     |   5.34   \n",
            "   5    |   320   |   0.050701   |     -      |     -     |   5.32   \n",
            "   5    |   340   |   0.058716   |     -      |     -     |   5.29   \n",
            "   5    |   360   |   0.058533   |     -      |     -     |   5.34   \n",
            "   5    |   380   |   0.069157   |     -      |     -     |   5.22   \n",
            "   5    |   400   |   0.050191   |     -      |     -     |   5.30   \n",
            "   5    |   420   |   0.046882   |     -      |     -     |   5.15   \n",
            "   5    |   440   |   0.051447   |     -      |     -     |   5.31   \n",
            "   5    |   460   |   0.065846   |     -      |     -     |   5.33   \n",
            "   5    |   480   |   0.055009   |     -      |     -     |   5.33   \n",
            "   5    |   500   |   0.051874   |     -      |     -     |   5.29   \n",
            "   5    |   520   |   0.062548   |     -      |     -     |   5.28   \n",
            "   5    |   540   |   0.050534   |     -      |     -     |   5.26   \n",
            "   5    |   560   |   0.073689   |     -      |     -     |   5.27   \n",
            "   5    |   580   |   0.052010   |     -      |     -     |   5.14   \n",
            "   5    |   600   |   0.067425   |     -      |     -     |   5.17   \n",
            "   5    |   620   |   0.036769   |     -      |     -     |   5.04   \n",
            "   5    |   640   |   0.059374   |     -      |     -     |   5.02   \n",
            "   5    |   660   |   0.078732   |     -      |     -     |   5.09   \n",
            "   5    |   680   |   0.067702   |     -      |     -     |   5.10   \n",
            "   5    |   700   |   0.042502   |     -      |     -     |   5.23   \n",
            "   5    |   714   |   0.047308   |     -      |     -     |   3.72   \n",
            "----------------------------------------------------------------------\n",
            "   5    |    -    |   0.057988   |  0.648685  |   84.98   |  194.29  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=EPOCHS)\n",
        "train(bert_classifier, train_loader, val_loader, epochs=EPOCHS, evaluation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(bert_classifier.state_dict(), 'bert_cla.ckpt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_data = TensorDataset(test_dataset[0],test_dataset[1],test_dataset[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "开始测试...\n"
          ]
        }
      ],
      "source": [
        "print('开始测试...')\n",
        "bert_classifier.eval()\n",
        "test_result = []\n",
        "for data in test_data:\n",
        "    b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in data)\n",
        "    b_input = b_input_ids.unsqueeze(0)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = bert_classifier(b_input)\n",
        "        pre = outputs.argmax(dim=1)\n",
        "        test_result.append([b_labels.item(), pre.item(), tokenizer.convert_ids_to_tokens(b_input_ids)])\n",
        "\n",
        "# 写入csv文件\n",
        "df = pd.DataFrame(test_result)\n",
        "df.to_csv('test_result.csv',index=False, header=['id', 'label','text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>['[CLS]', '屁', '民', '也', '是', '民', '[SEP]', '[...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>['[CLS]', '加', '油', '吧', '[SEP]', '[PAD]', '[P...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>['[CLS]', '下', '场', '加', '油', '吧', '[SEP]', '[...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>['[CLS]', '太', '扎', '眼', '了', '[SEP]', '[PAD]'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>['[CLS]', '领', '先', '[SEP]', '[PAD]', '[PAD]',...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6523</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>['[CLS]', '成', '渣', '了', '[SEP]', '[PAD]', '[P...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6524</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>['[CLS]', '应', '该', '叫', '先', '知', '[SEP]', '[...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6525</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>['[CLS]', '3', '输', '了', '。', '[SEP]', '[PAD]'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6526</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>['[CLS]', '就', '是', '太', '贵', '[SEP]', '[PAD]'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6529</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>['[CLS]', '数', '据', '太', 'nb', '了', '[SEP]', '...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1201 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      id  label                                               text\n",
              "3      2      0  ['[CLS]', '屁', '民', '也', '是', '民', '[SEP]', '[...\n",
              "4      1      0  ['[CLS]', '加', '油', '吧', '[SEP]', '[PAD]', '[P...\n",
              "11     1      0  ['[CLS]', '下', '场', '加', '油', '吧', '[SEP]', '[...\n",
              "14     2      0  ['[CLS]', '太', '扎', '眼', '了', '[SEP]', '[PAD]'...\n",
              "16     1      0  ['[CLS]', '领', '先', '[SEP]', '[PAD]', '[PAD]',...\n",
              "...   ..    ...                                                ...\n",
              "6523   2      0  ['[CLS]', '成', '渣', '了', '[SEP]', '[PAD]', '[P...\n",
              "6524   1      0  ['[CLS]', '应', '该', '叫', '先', '知', '[SEP]', '[...\n",
              "6525   2      0  ['[CLS]', '3', '输', '了', '。', '[SEP]', '[PAD]'...\n",
              "6526   2      0  ['[CLS]', '就', '是', '太', '贵', '[SEP]', '[PAD]'...\n",
              "6529   1      0  ['[CLS]', '数', '据', '太', 'nb', '了', '[SEP]', '...\n",
              "\n",
              "[1201 rows x 3 columns]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('test_result.csv')\n",
        "df[df.id!=df.label]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "7qCrKzdaZGEq"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8162203519510329"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df[df.id==df.label])/len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>['[CLS]', '被', '这', '剧', '毁', '了', '[SEP]', '[...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>['[CLS]', '足', '控', '压', '力', '大', '[SEP]', '[...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>['[CLS]', '好', '鸡', '肋', '[SEP]', '[PAD]', '[P...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>['[CLS]', '很', '自', '满', '[SEP]', '[PAD]', '[P...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>['[CLS]', '好', '人', '啊', '。', '。', '[SEP]', '[...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6362</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>['[CLS]', '师', '尼', '玛', '床', '技', '[SEP]', '[...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6433</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>['[CLS]', '态', '度', '很', '差', '[SEP]', '[PAD]'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6439</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>['[CLS]', '要', '被', '虐', '了', '[SEP]', '[PAD]'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6451</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>['[CLS]', '还', '是', '不', '错', '8', '[SEP]', '[...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6466</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>['[CLS]', '这', '部', '经', '典', '！', '[SEP]', '[...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>436 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      id  label                                               text\n",
              "7      2      2  ['[CLS]', '被', '这', '剧', '毁', '了', '[SEP]', '[...\n",
              "18     0      2  ['[CLS]', '足', '控', '压', '力', '大', '[SEP]', '[...\n",
              "71     2      2  ['[CLS]', '好', '鸡', '肋', '[SEP]', '[PAD]', '[P...\n",
              "72     2      2  ['[CLS]', '很', '自', '满', '[SEP]', '[PAD]', '[P...\n",
              "99     1      1  ['[CLS]', '好', '人', '啊', '。', '。', '[SEP]', '[...\n",
              "...   ..    ...                                                ...\n",
              "6362   2      2  ['[CLS]', '师', '尼', '玛', '床', '技', '[SEP]', '[...\n",
              "6433   2      2  ['[CLS]', '态', '度', '很', '差', '[SEP]', '[PAD]'...\n",
              "6439   2      2  ['[CLS]', '要', '被', '虐', '了', '[SEP]', '[PAD]'...\n",
              "6451   1      1  ['[CLS]', '还', '是', '不', '错', '8', '[SEP]', '[...\n",
              "6466   1      1  ['[CLS]', '这', '部', '经', '典', '！', '[SEP]', '[...\n",
              "\n",
              "[436 rows x 3 columns]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[df.label!=0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of train.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "ce613df70ec087c2b4dda2bc280e25d341f72f59d81afb32edf1d298cbbb8087"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('bert')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
