{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dwb4U_C6AuO3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import csv\n",
        "import transformers\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as Data\n",
        "import torch.nn.functional as F\n",
        "import json\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoTokenizer, AutoModel, BertConfig, get_linear_schedule_with_warmup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJNwS3i_AuPJ",
        "outputId": "81223033-a504-40bb-8712-5c6dbb18b8e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Egx-TMmlAuO8"
      },
      "outputs": [],
      "source": [
        "device = 'cuda'\n",
        "#MODEL_NAME = \"hfl/chinese-bert-wwm\"\n",
        "MODEL_NAME = 'bert-base-chinese'\n",
        "MAX_LEN = 32\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 32\n",
        "LR = 5e-5 \n",
        "WARMUP_STEPS = 100\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI8wUOpYAuO9"
      },
      "source": [
        "创建dataset类"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QM9KnoKeIhBB"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gthyrxBpAuPA"
      },
      "source": [
        "创建load_dataset function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pCoBV4CnAuPB"
      },
      "outputs": [],
      "source": [
        "def load_dataset(filepath, max_len):\n",
        "    label = []\n",
        "    sentences = []\n",
        "    # load dataset\n",
        "    f = open(filepath, 'r', encoding='utf-8')\n",
        "    r = csv.reader(f)\n",
        "    for item in r:\n",
        "        if r.line_num == 1:\n",
        "            continue\n",
        "        label.append(int(item[0]))\n",
        "        sentences.append(item[1])\n",
        "        \n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    # For every sentence...\n",
        "    for data in sentences:\n",
        "        encoded_data = tokenizer.encode_plus(\n",
        "            text=data,                      # Preprocess sentence\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            max_length=max_len,             # Max length to truncate/pad\n",
        "            padding='max_length',           # Pad sentence to max length\n",
        "            return_attention_mask=True,      # Return attention mask\n",
        "            truncation= True\n",
        "            )\n",
        "        \n",
        "        # Add the outputs to the lists\n",
        "        input_ids.append(encoded_data.get('input_ids'))\n",
        "        attention_masks.append(encoded_data.get('attention_mask'))\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "    labels = torch.tensor(label)\n",
        "    return input_ids, attention_masks, labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1B6K3IQAuPD"
      },
      "source": [
        "load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "path = os.path.abspath(os.path.dirname(os.getcwd()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AZ17DQ1iAuPE"
      },
      "outputs": [],
      "source": [
        "train_dataset = load_dataset(f'{path}/data/db2/train.csv', max_len = MAX_LEN)\n",
        "valid_dataset = load_dataset(f'{path}/data/db2/dev.csv', max_len = MAX_LEN)\n",
        "test_dataset = load_dataset(f'{path}/data/db2/test.csv', max_len = MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "P_dcsj5QAuPE"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "train_data = TensorDataset(train_dataset[0], train_dataset[1],train_dataset[2])\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_loader = DataLoader(train_data, sampler=train_sampler, batch_size = BATCH_SIZE)\n",
        "\n",
        "val_data = TensorDataset(valid_dataset[0],valid_dataset[1],valid_dataset[2])\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_loader = DataLoader(val_data,sampler=val_sampler, batch_size = BATCH_SIZE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "config = BertConfig.from_pretrained(MODEL_NAME)\n",
        "config.hidden_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyXXbzFKPtai",
        "outputId": "ac00e6a8-2361-4a87-ff12-af45506b06c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 46.9 ms\n",
            "Wall time: 41 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "# Create the BertClassfier class\n",
        "class BertClassifier(nn.Module):\n",
        "    \"\"\"Bert Model for Classification Tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dimension):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.bert = BertModel.from_pretrained(MODEL_NAME)\n",
        "        \n",
        "        embedding_dim = self.bert.config.to_dict()['hidden_size']\n",
        "        \n",
        "        self.LSTM = nn.LSTM(embedding_dim,hidden_dimension,bidirectional=True, batch_first=True)\n",
        "               \n",
        "        self.out = nn.Linear(hidden_dimension * 2, 3)\n",
        "            \n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        \n",
        "       \n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "    \n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        encoded_layers = outputs[0]\n",
        "\n",
        "        #encoded_layers = encoded_layers.permute(1, 0, 2)\n",
        "        \n",
        "        enc_hiddens, (last_hidden, last_cell) = self.LSTM(encoded_layers)\n",
        "        output_hidden = torch.cat((enc_hiddens[:,-1, :256],enc_hiddens[:,0, 256:]),dim=-1)\n",
        "        output_hidden = F.dropout(output_hidden,0.2)\n",
        "        \n",
        "        logits = self.out(output_hidden)\n",
        "        \n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gZYjRSpHP18K"
      },
      "outputs": [],
      "source": [
        "def initialize_model(epochs=EPOCHS):\n",
        "    bert_classifier = BertClassifier(hidden_dimension=256)\n",
        "\n",
        "    # Tell PyTorch to run the model on GPU\n",
        "    bert_classifier.to(device)\n",
        "\n",
        "    # Create the optimizer\n",
        "    optimizer = AdamW(bert_classifier.parameters(),lr=LR)\n",
        "\n",
        "    # Set up the learning rate scheduler\n",
        "    total_steps = len(train_loader) * epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=WARMUP_STEPS,num_training_steps=total_steps)\n",
        "    return bert_classifier, optimizer, scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hXogwg3QQAi_"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "def train(model, train_dataloader, val_dataloader=None, epochs=EPOCHS , evaluation=False):\n",
        "    \"\"\"Train the BertClassifier model.\n",
        "    \"\"\"\n",
        "    # Start training loop\n",
        "    print(\"Start training...\\n\")\n",
        "    for epoch_i in range(epochs):\n",
        "        # Print the header of the result table\n",
        "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        # Measure the elapsed time of each epoch\n",
        "        t0_epoch, t0_batch = time.time(), time.time()\n",
        "\n",
        "        # Reset tracking variables at the beginning of each epoch\n",
        "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for s,batch in enumerate(train_dataloader):\n",
        "            batch_counts +=1\n",
        "            # Load batch to GPU\n",
        "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "            # Compute loss and accumulate the loss values\n",
        "            loss = loss_fn(logits.view(-1, 3), b_labels.view(-1))\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "            \n",
        "            # return loss, logits\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            \n",
        "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Print the loss values and time elapsed for every 20 batches\n",
        "            if (s % 20 == 0 and s != 0) or (s == len(train_dataloader) - 1):\n",
        "                # Calculate time elapsed for 20 batches\n",
        "                time_elapsed = time.time() - t0_batch\n",
        "\n",
        "                # Print training results\n",
        "                print(f\"{epoch_i + 1:^7} | {s:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "                # Reset batch tracking variables\n",
        "                batch_loss, batch_counts = 0, 0\n",
        "                t0_batch = time.time()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        print(\"-\"*70)\n",
        "        if evaluation == True:\n",
        "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
        "            time_elapsed = time.time() - t0_epoch \n",
        "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
        "            print(\"-\"*70)\n",
        "        print(\"\\n\")\n",
        "    \n",
        "    print(\"Training complete!\")\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        # Get the predictions\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "        # Calculate the accuracy rate\n",
        "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_loss, val_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcJjBdElQGuD",
        "outputId": "25427dde-333a-4185-f35d-c9d173cc5f4c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   1    |   20    |   1.081030   |     -      |     -     |   13.02  \n",
            "   1    |   40    |   0.830642   |     -      |     -     |   4.45   \n",
            "   1    |   60    |   0.583288   |     -      |     -     |   4.47   \n",
            "   1    |   80    |   0.461918   |     -      |     -     |   4.50   \n",
            "   1    |   100   |   0.485318   |     -      |     -     |   4.40   \n",
            "   1    |   120   |   0.419843   |     -      |     -     |   4.47   \n",
            "   1    |   140   |   0.390855   |     -      |     -     |   4.61   \n",
            "   1    |   160   |   0.416521   |     -      |     -     |   4.44   \n",
            "   1    |   180   |   0.372742   |     -      |     -     |   4.62   \n",
            "   1    |   200   |   0.390856   |     -      |     -     |   4.53   \n",
            "   1    |   220   |   0.398018   |     -      |     -     |   4.64   \n",
            "   1    |   240   |   0.393864   |     -      |     -     |   4.80   \n",
            "   1    |   260   |   0.374283   |     -      |     -     |   4.84   \n",
            "   1    |   280   |   0.438435   |     -      |     -     |   4.85   \n",
            "   1    |   300   |   0.359234   |     -      |     -     |   4.80   \n",
            "   1    |   320   |   0.356066   |     -      |     -     |   4.49   \n",
            "   1    |   340   |   0.310426   |     -      |     -     |   4.54   \n",
            "   1    |   360   |   0.339894   |     -      |     -     |   4.44   \n",
            "   1    |   380   |   0.364463   |     -      |     -     |   4.44   \n",
            "   1    |   400   |   0.302899   |     -      |     -     |   4.49   \n",
            "   1    |   420   |   0.326150   |     -      |     -     |   4.46   \n",
            "   1    |   440   |   0.319240   |     -      |     -     |   4.45   \n",
            "   1    |   460   |   0.328072   |     -      |     -     |   4.51   \n",
            "   1    |   480   |   0.325030   |     -      |     -     |   4.56   \n",
            "   1    |   500   |   0.293763   |     -      |     -     |   4.66   \n",
            "   1    |   520   |   0.388166   |     -      |     -     |   4.56   \n",
            "   1    |   540   |   0.365601   |     -      |     -     |   4.49   \n",
            "   1    |   560   |   0.298169   |     -      |     -     |   4.54   \n",
            "   1    |   580   |   0.313682   |     -      |     -     |   4.46   \n",
            "   1    |   600   |   0.328124   |     -      |     -     |   4.50   \n",
            "   1    |   620   |   0.331544   |     -      |     -     |   4.53   \n",
            "   1    |   640   |   0.354599   |     -      |     -     |   4.55   \n",
            "   1    |   660   |   0.328504   |     -      |     -     |   4.50   \n",
            "   1    |   680   |   0.311719   |     -      |     -     |   4.50   \n",
            "   1    |   700   |   0.328772   |     -      |     -     |   4.61   \n",
            "   1    |   720   |   0.333982   |     -      |     -     |   4.56   \n",
            "   1    |   740   |   0.326616   |     -      |     -     |   4.49   \n",
            "   1    |   760   |   0.287717   |     -      |     -     |   4.51   \n",
            "   1    |   780   |   0.331676   |     -      |     -     |   4.55   \n",
            "   1    |   800   |   0.312082   |     -      |     -     |   4.46   \n",
            "   1    |   820   |   0.275509   |     -      |     -     |   4.48   \n",
            "   1    |   840   |   0.304925   |     -      |     -     |   4.47   \n",
            "   1    |   860   |   0.292367   |     -      |     -     |   4.49   \n",
            "   1    |   880   |   0.297413   |     -      |     -     |   4.47   \n",
            "   1    |   900   |   0.357088   |     -      |     -     |   4.47   \n",
            "   1    |   920   |   0.311073   |     -      |     -     |   4.46   \n",
            "   1    |   940   |   0.288109   |     -      |     -     |   4.49   \n",
            "   1    |   960   |   0.337969   |     -      |     -     |   4.48   \n",
            "   1    |   980   |   0.311963   |     -      |     -     |   4.48   \n",
            "   1    |  1000   |   0.284846   |     -      |     -     |   4.49   \n",
            "   1    |  1020   |   0.329287   |     -      |     -     |   4.48   \n",
            "   1    |  1040   |   0.296343   |     -      |     -     |   4.44   \n",
            "   1    |  1060   |   0.274137   |     -      |     -     |   4.51   \n",
            "   1    |  1080   |   0.280623   |     -      |     -     |   4.47   \n",
            "   1    |  1100   |   0.301559   |     -      |     -     |   4.47   \n",
            "   1    |  1120   |   0.268338   |     -      |     -     |   4.49   \n",
            "   1    |  1140   |   0.275839   |     -      |     -     |   4.47   \n",
            "   1    |  1160   |   0.268231   |     -      |     -     |   4.49   \n",
            "   1    |  1180   |   0.284190   |     -      |     -     |   4.48   \n",
            "   1    |  1200   |   0.316549   |     -      |     -     |   4.49   \n",
            "   1    |  1220   |   0.284822   |     -      |     -     |   4.44   \n",
            "   1    |  1240   |   0.327762   |     -      |     -     |   4.51   \n",
            "   1    |  1260   |   0.233166   |     -      |     -     |   4.48   \n",
            "   1    |  1280   |   0.307308   |     -      |     -     |   4.50   \n",
            "   1    |  1300   |   0.307364   |     -      |     -     |   4.50   \n",
            "   1    |  1320   |   0.308282   |     -      |     -     |   4.51   \n",
            "   1    |  1340   |   0.327350   |     -      |     -     |   4.49   \n",
            "   1    |  1360   |   0.292204   |     -      |     -     |   4.51   \n",
            "   1    |  1380   |   0.308830   |     -      |     -     |   4.53   \n",
            "   1    |  1400   |   0.273772   |     -      |     -     |   4.58   \n",
            "   1    |  1420   |   0.322785   |     -      |     -     |   4.48   \n",
            "   1    |  1440   |   0.302781   |     -      |     -     |   4.50   \n",
            "   1    |  1460   |   0.282956   |     -      |     -     |   4.48   \n",
            "   1    |  1480   |   0.264828   |     -      |     -     |   4.48   \n",
            "   1    |  1500   |   0.266962   |     -      |     -     |   4.48   \n",
            "   1    |  1520   |   0.267636   |     -      |     -     |   4.47   \n",
            "   1    |  1540   |   0.295849   |     -      |     -     |   4.49   \n",
            "   1    |  1560   |   0.296629   |     -      |     -     |   4.50   \n",
            "   1    |  1580   |   0.349538   |     -      |     -     |   4.48   \n",
            "   1    |  1600   |   0.266943   |     -      |     -     |   4.49   \n",
            "   1    |  1620   |   0.282761   |     -      |     -     |   4.50   \n",
            "   1    |  1640   |   0.274949   |     -      |     -     |   4.53   \n",
            "   1    |  1660   |   0.351423   |     -      |     -     |   4.50   \n",
            "   1    |  1680   |   0.307665   |     -      |     -     |   4.48   \n",
            "   1    |  1700   |   0.288405   |     -      |     -     |   4.50   \n",
            "   1    |  1720   |   0.299727   |     -      |     -     |   4.48   \n",
            "   1    |  1740   |   0.256549   |     -      |     -     |   4.49   \n",
            "   1    |  1749   |   0.242538   |     -      |     -     |   2.02   \n",
            "----------------------------------------------------------------------\n",
            "   1    |    -    |   0.339115   |  0.258117  |   89.92   |  418.16  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   2    |   20    |   0.170011   |     -      |     -     |   4.97   \n",
            "   2    |   40    |   0.225194   |     -      |     -     |   4.83   \n",
            "   2    |   60    |   0.230266   |     -      |     -     |   4.84   \n",
            "   2    |   80    |   0.244211   |     -      |     -     |   4.85   \n",
            "   2    |   100   |   0.161369   |     -      |     -     |   4.84   \n",
            "   2    |   120   |   0.209730   |     -      |     -     |   4.85   \n",
            "   2    |   140   |   0.240655   |     -      |     -     |   4.85   \n",
            "   2    |   160   |   0.240824   |     -      |     -     |   4.84   \n",
            "   2    |   180   |   0.209932   |     -      |     -     |   4.86   \n",
            "   2    |   200   |   0.249823   |     -      |     -     |   4.84   \n",
            "   2    |   220   |   0.245900   |     -      |     -     |   4.86   \n",
            "   2    |   240   |   0.224194   |     -      |     -     |   4.84   \n",
            "   2    |   260   |   0.185827   |     -      |     -     |   4.85   \n",
            "   2    |   280   |   0.230840   |     -      |     -     |   4.84   \n",
            "   2    |   300   |   0.200392   |     -      |     -     |   4.84   \n",
            "   2    |   320   |   0.242856   |     -      |     -     |   4.85   \n",
            "   2    |   340   |   0.220656   |     -      |     -     |   4.84   \n",
            "   2    |   360   |   0.227241   |     -      |     -     |   4.83   \n",
            "   2    |   380   |   0.203524   |     -      |     -     |   4.86   \n",
            "   2    |   400   |   0.193075   |     -      |     -     |   4.85   \n",
            "   2    |   420   |   0.209177   |     -      |     -     |   4.84   \n",
            "   2    |   440   |   0.162252   |     -      |     -     |   4.83   \n",
            "   2    |   460   |   0.161094   |     -      |     -     |   4.85   \n",
            "   2    |   480   |   0.240113   |     -      |     -     |   4.85   \n",
            "   2    |   500   |   0.214848   |     -      |     -     |   4.85   \n",
            "   2    |   520   |   0.200103   |     -      |     -     |   4.85   \n",
            "   2    |   540   |   0.178732   |     -      |     -     |   4.86   \n",
            "   2    |   560   |   0.176636   |     -      |     -     |   4.85   \n",
            "   2    |   580   |   0.235936   |     -      |     -     |   4.86   \n",
            "   2    |   600   |   0.210622   |     -      |     -     |   4.85   \n",
            "   2    |   620   |   0.210620   |     -      |     -     |   4.87   \n",
            "   2    |   640   |   0.180890   |     -      |     -     |   4.86   \n",
            "   2    |   660   |   0.197823   |     -      |     -     |   4.83   \n",
            "   2    |   680   |   0.204167   |     -      |     -     |   4.84   \n",
            "   2    |   700   |   0.209594   |     -      |     -     |   4.84   \n",
            "   2    |   720   |   0.184247   |     -      |     -     |   4.86   \n",
            "   2    |   740   |   0.243787   |     -      |     -     |   4.87   \n",
            "   2    |   760   |   0.231849   |     -      |     -     |   4.86   \n",
            "   2    |   780   |   0.206722   |     -      |     -     |   4.86   \n",
            "   2    |   800   |   0.244268   |     -      |     -     |   4.86   \n",
            "   2    |   820   |   0.197985   |     -      |     -     |   4.84   \n",
            "   2    |   840   |   0.223494   |     -      |     -     |   4.85   \n",
            "   2    |   860   |   0.244662   |     -      |     -     |   4.83   \n",
            "   2    |   880   |   0.210964   |     -      |     -     |   4.88   \n",
            "   2    |   900   |   0.205164   |     -      |     -     |   4.85   \n",
            "   2    |   920   |   0.228224   |     -      |     -     |   4.85   \n",
            "   2    |   940   |   0.214117   |     -      |     -     |   4.86   \n",
            "   2    |   960   |   0.206553   |     -      |     -     |   4.85   \n",
            "   2    |   980   |   0.194246   |     -      |     -     |   4.85   \n",
            "   2    |  1000   |   0.197233   |     -      |     -     |   4.81   \n",
            "   2    |  1020   |   0.237407   |     -      |     -     |   4.84   \n",
            "   2    |  1040   |   0.198402   |     -      |     -     |   4.87   \n",
            "   2    |  1060   |   0.171842   |     -      |     -     |   4.86   \n",
            "   2    |  1080   |   0.196325   |     -      |     -     |   4.86   \n",
            "   2    |  1100   |   0.220570   |     -      |     -     |   4.87   \n",
            "   2    |  1120   |   0.183034   |     -      |     -     |   4.86   \n",
            "   2    |  1140   |   0.185913   |     -      |     -     |   4.85   \n",
            "   2    |  1160   |   0.184599   |     -      |     -     |   4.83   \n",
            "   2    |  1180   |   0.207493   |     -      |     -     |   4.86   \n",
            "   2    |  1200   |   0.183109   |     -      |     -     |   4.82   \n",
            "   2    |  1220   |   0.175389   |     -      |     -     |   4.86   \n",
            "   2    |  1240   |   0.259383   |     -      |     -     |   4.87   \n",
            "   2    |  1260   |   0.210619   |     -      |     -     |   4.84   \n",
            "   2    |  1280   |   0.204855   |     -      |     -     |   4.85   \n",
            "   2    |  1300   |   0.195375   |     -      |     -     |   4.88   \n",
            "   2    |  1320   |   0.178143   |     -      |     -     |   4.84   \n",
            "   2    |  1340   |   0.220373   |     -      |     -     |   4.87   \n",
            "   2    |  1360   |   0.194945   |     -      |     -     |   4.86   \n",
            "   2    |  1380   |   0.234431   |     -      |     -     |   4.84   \n",
            "   2    |  1400   |   0.261638   |     -      |     -     |   4.84   \n",
            "   2    |  1420   |   0.244701   |     -      |     -     |   4.85   \n",
            "   2    |  1440   |   0.188000   |     -      |     -     |   4.87   \n",
            "   2    |  1460   |   0.207087   |     -      |     -     |   4.86   \n",
            "   2    |  1480   |   0.215165   |     -      |     -     |   4.86   \n",
            "   2    |  1500   |   0.221532   |     -      |     -     |   4.86   \n",
            "   2    |  1520   |   0.220193   |     -      |     -     |   4.87   \n",
            "   2    |  1540   |   0.193385   |     -      |     -     |   4.85   \n",
            "   2    |  1560   |   0.203860   |     -      |     -     |   4.86   \n",
            "   2    |  1580   |   0.202235   |     -      |     -     |   4.86   \n",
            "   2    |  1600   |   0.205700   |     -      |     -     |   4.87   \n",
            "   2    |  1620   |   0.207520   |     -      |     -     |   4.87   \n",
            "   2    |  1640   |   0.219980   |     -      |     -     |   4.88   \n",
            "   2    |  1660   |   0.248322   |     -      |     -     |   4.86   \n",
            "   2    |  1680   |   0.204271   |     -      |     -     |   4.91   \n",
            "   2    |  1700   |   0.213724   |     -      |     -     |   4.87   \n",
            "   2    |  1720   |   0.259906   |     -      |     -     |   4.86   \n",
            "   2    |  1740   |   0.156309   |     -      |     -     |   4.87   \n",
            "   2    |  1749   |   0.236103   |     -      |     -     |   2.21   \n",
            "----------------------------------------------------------------------\n",
            "   2    |    -    |   0.210436   |  0.271807  |   89.81   |  439.87  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   3    |   20    |   0.120613   |     -      |     -     |   4.98   \n",
            "   3    |   40    |   0.133634   |     -      |     -     |   4.87   \n",
            "   3    |   60    |   0.150106   |     -      |     -     |   4.86   \n",
            "   3    |   80    |   0.120581   |     -      |     -     |   4.86   \n",
            "   3    |   100   |   0.090645   |     -      |     -     |   4.88   \n",
            "   3    |   120   |   0.125796   |     -      |     -     |   4.88   \n",
            "   3    |   140   |   0.114774   |     -      |     -     |   4.88   \n",
            "   3    |   160   |   0.144856   |     -      |     -     |   4.87   \n",
            "   3    |   180   |   0.136803   |     -      |     -     |   4.86   \n",
            "   3    |   200   |   0.093905   |     -      |     -     |   4.88   \n",
            "   3    |   220   |   0.136805   |     -      |     -     |   4.86   \n",
            "   3    |   240   |   0.160047   |     -      |     -     |   4.86   \n",
            "   3    |   260   |   0.142685   |     -      |     -     |   4.89   \n",
            "   3    |   280   |   0.140706   |     -      |     -     |   4.78   \n",
            "   3    |   300   |   0.138533   |     -      |     -     |   4.64   \n",
            "   3    |   320   |   0.141906   |     -      |     -     |   4.77   \n",
            "   3    |   340   |   0.146644   |     -      |     -     |   4.82   \n",
            "   3    |   360   |   0.135515   |     -      |     -     |   4.61   \n",
            "   3    |   380   |   0.120540   |     -      |     -     |   4.65   \n",
            "   3    |   400   |   0.143375   |     -      |     -     |   4.83   \n",
            "   3    |   420   |   0.180726   |     -      |     -     |   4.66   \n",
            "   3    |   440   |   0.124343   |     -      |     -     |   4.74   \n",
            "   3    |   460   |   0.105663   |     -      |     -     |   4.55   \n",
            "   3    |   480   |   0.109903   |     -      |     -     |   4.55   \n",
            "   3    |   500   |   0.134536   |     -      |     -     |   4.55   \n",
            "   3    |   520   |   0.151433   |     -      |     -     |   4.56   \n",
            "   3    |   540   |   0.162966   |     -      |     -     |   4.61   \n",
            "   3    |   560   |   0.127057   |     -      |     -     |   4.82   \n",
            "   3    |   580   |   0.148632   |     -      |     -     |   4.72   \n",
            "   3    |   600   |   0.104060   |     -      |     -     |   4.61   \n",
            "   3    |   620   |   0.145182   |     -      |     -     |   4.80   \n",
            "   3    |   640   |   0.154844   |     -      |     -     |   4.81   \n",
            "   3    |   660   |   0.125165   |     -      |     -     |   4.75   \n",
            "   3    |   680   |   0.138151   |     -      |     -     |   4.55   \n",
            "   3    |   700   |   0.108347   |     -      |     -     |   4.58   \n",
            "   3    |   720   |   0.126016   |     -      |     -     |   4.83   \n",
            "   3    |   740   |   0.166664   |     -      |     -     |   4.65   \n",
            "   3    |   760   |   0.115942   |     -      |     -     |   4.54   \n",
            "   3    |   780   |   0.134229   |     -      |     -     |   4.70   \n",
            "   3    |   800   |   0.145644   |     -      |     -     |   4.81   \n",
            "   3    |   820   |   0.139165   |     -      |     -     |   4.77   \n",
            "   3    |   840   |   0.137894   |     -      |     -     |   4.77   \n",
            "   3    |   860   |   0.113228   |     -      |     -     |   4.73   \n",
            "   3    |   880   |   0.156717   |     -      |     -     |   4.57   \n",
            "   3    |   900   |   0.149339   |     -      |     -     |   4.80   \n",
            "   3    |   920   |   0.165802   |     -      |     -     |   4.82   \n",
            "   3    |   940   |   0.135443   |     -      |     -     |   4.63   \n",
            "   3    |   960   |   0.138144   |     -      |     -     |   4.54   \n",
            "   3    |   980   |   0.125105   |     -      |     -     |   4.75   \n",
            "   3    |  1000   |   0.093856   |     -      |     -     |   4.79   \n",
            "   3    |  1020   |   0.164122   |     -      |     -     |   4.77   \n",
            "   3    |  1040   |   0.161397   |     -      |     -     |   4.77   \n",
            "   3    |  1060   |   0.127431   |     -      |     -     |   4.69   \n",
            "   3    |  1080   |   0.134371   |     -      |     -     |   4.71   \n",
            "   3    |  1100   |   0.089950   |     -      |     -     |   4.83   \n",
            "   3    |  1120   |   0.126543   |     -      |     -     |   4.81   \n",
            "   3    |  1140   |   0.144319   |     -      |     -     |   4.84   \n",
            "   3    |  1160   |   0.149634   |     -      |     -     |   4.80   \n",
            "   3    |  1180   |   0.120270   |     -      |     -     |   4.83   \n",
            "   3    |  1200   |   0.134839   |     -      |     -     |   4.62   \n",
            "   3    |  1220   |   0.142265   |     -      |     -     |   4.51   \n",
            "   3    |  1240   |   0.153310   |     -      |     -     |   4.56   \n",
            "   3    |  1260   |   0.121308   |     -      |     -     |   4.55   \n",
            "   3    |  1280   |   0.104148   |     -      |     -     |   4.60   \n",
            "   3    |  1300   |   0.120163   |     -      |     -     |   4.82   \n",
            "   3    |  1320   |   0.128004   |     -      |     -     |   4.78   \n",
            "   3    |  1340   |   0.110227   |     -      |     -     |   4.77   \n",
            "   3    |  1360   |   0.128879   |     -      |     -     |   4.76   \n",
            "   3    |  1380   |   0.126306   |     -      |     -     |   4.76   \n",
            "   3    |  1400   |   0.108119   |     -      |     -     |   4.80   \n",
            "   3    |  1420   |   0.136012   |     -      |     -     |   4.56   \n",
            "   3    |  1440   |   0.085881   |     -      |     -     |   4.77   \n",
            "   3    |  1460   |   0.147124   |     -      |     -     |   4.70   \n",
            "   3    |  1480   |   0.126312   |     -      |     -     |   4.83   \n",
            "   3    |  1500   |   0.126774   |     -      |     -     |   4.77   \n",
            "   3    |  1520   |   0.116540   |     -      |     -     |   4.76   \n",
            "   3    |  1540   |   0.102566   |     -      |     -     |   4.78   \n",
            "   3    |  1560   |   0.121742   |     -      |     -     |   4.81   \n",
            "   3    |  1580   |   0.106336   |     -      |     -     |   4.78   \n",
            "   3    |  1600   |   0.117346   |     -      |     -     |   4.66   \n",
            "   3    |  1620   |   0.176182   |     -      |     -     |   4.79   \n",
            "   3    |  1640   |   0.184898   |     -      |     -     |   4.79   \n",
            "   3    |  1660   |   0.178525   |     -      |     -     |   4.51   \n",
            "   3    |  1680   |   0.105452   |     -      |     -     |   4.83   \n",
            "   3    |  1700   |   0.122594   |     -      |     -     |   4.82   \n",
            "   3    |  1720   |   0.132268   |     -      |     -     |   4.82   \n",
            "   3    |  1740   |   0.132835   |     -      |     -     |   4.86   \n",
            "   3    |  1749   |   0.096328   |     -      |     -     |   2.15   \n",
            "----------------------------------------------------------------------\n",
            "   3    |    -    |   0.132195   |  0.292507  |   89.86   |  429.73  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   4    |   20    |   0.082324   |     -      |     -     |   4.70   \n",
            "   4    |   40    |   0.079018   |     -      |     -     |   4.56   \n",
            "   4    |   60    |   0.073941   |     -      |     -     |   4.57   \n",
            "   4    |   80    |   0.057097   |     -      |     -     |   4.72   \n",
            "   4    |   100   |   0.048703   |     -      |     -     |   4.66   \n",
            "   4    |   120   |   0.070337   |     -      |     -     |   4.74   \n",
            "   4    |   140   |   0.051609   |     -      |     -     |   4.83   \n",
            "   4    |   160   |   0.067886   |     -      |     -     |   4.74   \n",
            "   4    |   180   |   0.071264   |     -      |     -     |   4.58   \n",
            "   4    |   200   |   0.052782   |     -      |     -     |   4.56   \n",
            "   4    |   220   |   0.050718   |     -      |     -     |   4.56   \n",
            "   4    |   240   |   0.085011   |     -      |     -     |   4.70   \n",
            "   4    |   260   |   0.062120   |     -      |     -     |   4.83   \n",
            "   4    |   280   |   0.084961   |     -      |     -     |   4.78   \n",
            "   4    |   300   |   0.085722   |     -      |     -     |   4.79   \n",
            "   4    |   320   |   0.068178   |     -      |     -     |   4.80   \n",
            "   4    |   340   |   0.081631   |     -      |     -     |   4.87   \n",
            "   4    |   360   |   0.077800   |     -      |     -     |   4.84   \n",
            "   4    |   380   |   0.052415   |     -      |     -     |   4.86   \n",
            "   4    |   400   |   0.052939   |     -      |     -     |   4.68   \n",
            "   4    |   420   |   0.103003   |     -      |     -     |   4.87   \n",
            "   4    |   440   |   0.079200   |     -      |     -     |   4.94   \n",
            "   4    |   460   |   0.076977   |     -      |     -     |   4.64   \n",
            "   4    |   480   |   0.083091   |     -      |     -     |   4.73   \n",
            "   4    |   500   |   0.091536   |     -      |     -     |   4.80   \n",
            "   4    |   520   |   0.104817   |     -      |     -     |   4.79   \n",
            "   4    |   540   |   0.062334   |     -      |     -     |   4.70   \n",
            "   4    |   560   |   0.077482   |     -      |     -     |   4.78   \n",
            "   4    |   580   |   0.056807   |     -      |     -     |   4.79   \n",
            "   4    |   600   |   0.082668   |     -      |     -     |   4.78   \n",
            "   4    |   620   |   0.072623   |     -      |     -     |   4.82   \n",
            "   4    |   640   |   0.062331   |     -      |     -     |   4.84   \n",
            "   4    |   660   |   0.062012   |     -      |     -     |   4.83   \n",
            "   4    |   680   |   0.065118   |     -      |     -     |   4.80   \n",
            "   4    |   700   |   0.041294   |     -      |     -     |   4.69   \n",
            "   4    |   720   |   0.094253   |     -      |     -     |   4.84   \n",
            "   4    |   740   |   0.071597   |     -      |     -     |   4.81   \n",
            "   4    |   760   |   0.063833   |     -      |     -     |   4.80   \n",
            "   4    |   780   |   0.048026   |     -      |     -     |   4.82   \n",
            "   4    |   800   |   0.054489   |     -      |     -     |   4.84   \n",
            "   4    |   820   |   0.070375   |     -      |     -     |   4.82   \n",
            "   4    |   840   |   0.085656   |     -      |     -     |   4.76   \n",
            "   4    |   860   |   0.068551   |     -      |     -     |   4.83   \n",
            "   4    |   880   |   0.088718   |     -      |     -     |   4.84   \n",
            "   4    |   900   |   0.093193   |     -      |     -     |   4.84   \n",
            "   4    |   920   |   0.043136   |     -      |     -     |   4.88   \n",
            "   4    |   940   |   0.071796   |     -      |     -     |   4.93   \n",
            "   4    |   960   |   0.093913   |     -      |     -     |   4.77   \n",
            "   4    |   980   |   0.045230   |     -      |     -     |   4.66   \n",
            "   4    |  1000   |   0.041098   |     -      |     -     |   4.86   \n",
            "   4    |  1020   |   0.092623   |     -      |     -     |   4.93   \n",
            "   4    |  1040   |   0.063581   |     -      |     -     |   4.99   \n",
            "   4    |  1060   |   0.055287   |     -      |     -     |   5.04   \n",
            "   4    |  1080   |   0.075525   |     -      |     -     |   4.98   \n",
            "   4    |  1100   |   0.087350   |     -      |     -     |   5.05   \n",
            "   4    |  1120   |   0.083109   |     -      |     -     |   4.87   \n",
            "   4    |  1140   |   0.061197   |     -      |     -     |   5.04   \n",
            "   4    |  1160   |   0.090751   |     -      |     -     |   5.07   \n",
            "   4    |  1180   |   0.067966   |     -      |     -     |   5.00   \n",
            "   4    |  1200   |   0.074026   |     -      |     -     |   4.86   \n",
            "   4    |  1220   |   0.062955   |     -      |     -     |   4.90   \n",
            "   4    |  1240   |   0.068770   |     -      |     -     |   4.86   \n",
            "   4    |  1260   |   0.099327   |     -      |     -     |   4.69   \n",
            "   4    |  1280   |   0.083148   |     -      |     -     |   4.66   \n",
            "   4    |  1300   |   0.069185   |     -      |     -     |   4.67   \n",
            "   4    |  1320   |   0.077561   |     -      |     -     |   4.63   \n",
            "   4    |  1340   |   0.057777   |     -      |     -     |   4.58   \n",
            "   4    |  1360   |   0.076521   |     -      |     -     |   4.60   \n",
            "   4    |  1380   |   0.065877   |     -      |     -     |   4.76   \n",
            "   4    |  1400   |   0.031127   |     -      |     -     |   4.81   \n",
            "   4    |  1420   |   0.072845   |     -      |     -     |   4.67   \n",
            "   4    |  1440   |   0.070801   |     -      |     -     |   4.62   \n",
            "   4    |  1460   |   0.088880   |     -      |     -     |   4.59   \n",
            "   4    |  1480   |   0.050140   |     -      |     -     |   4.79   \n",
            "   4    |  1500   |   0.092728   |     -      |     -     |   4.70   \n",
            "   4    |  1520   |   0.068780   |     -      |     -     |   4.88   \n",
            "   4    |  1540   |   0.083388   |     -      |     -     |   4.78   \n",
            "   4    |  1560   |   0.079060   |     -      |     -     |   4.73   \n",
            "   4    |  1580   |   0.071749   |     -      |     -     |   4.61   \n",
            "   4    |  1600   |   0.070304   |     -      |     -     |   4.59   \n",
            "   4    |  1620   |   0.051858   |     -      |     -     |   4.61   \n",
            "   4    |  1640   |   0.039767   |     -      |     -     |   4.60   \n",
            "   4    |  1660   |   0.062017   |     -      |     -     |   4.63   \n",
            "   4    |  1680   |   0.075204   |     -      |     -     |   4.59   \n",
            "   4    |  1700   |   0.072405   |     -      |     -     |   4.60   \n",
            "   4    |  1720   |   0.071227   |     -      |     -     |   4.61   \n",
            "   4    |  1740   |   0.056818   |     -      |     -     |   4.55   \n",
            "   4    |  1749   |   0.044673   |     -      |     -     |   2.06   \n",
            "----------------------------------------------------------------------\n",
            "   4    |    -    |   0.070371   |  0.357056  |   90.16   |  431.14  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   5    |   20    |   0.035562   |     -      |     -     |   4.79   \n",
            "   5    |   40    |   0.022229   |     -      |     -     |   4.71   \n",
            "   5    |   60    |   0.032034   |     -      |     -     |   4.66   \n",
            "   5    |   80    |   0.047165   |     -      |     -     |   4.66   \n",
            "   5    |   100   |   0.019463   |     -      |     -     |   4.69   \n",
            "   5    |   120   |   0.020163   |     -      |     -     |   4.75   \n",
            "   5    |   140   |   0.030813   |     -      |     -     |   4.71   \n",
            "   5    |   160   |   0.035914   |     -      |     -     |   4.73   \n",
            "   5    |   180   |   0.033011   |     -      |     -     |   4.75   \n",
            "   5    |   200   |   0.032901   |     -      |     -     |   4.76   \n",
            "   5    |   220   |   0.040618   |     -      |     -     |   4.77   \n",
            "   5    |   240   |   0.041653   |     -      |     -     |   4.81   \n",
            "   5    |   260   |   0.027951   |     -      |     -     |   4.78   \n",
            "   5    |   280   |   0.027353   |     -      |     -     |   4.76   \n",
            "   5    |   300   |   0.035472   |     -      |     -     |   4.76   \n",
            "   5    |   320   |   0.022413   |     -      |     -     |   4.78   \n",
            "   5    |   340   |   0.035317   |     -      |     -     |   4.80   \n",
            "   5    |   360   |   0.030735   |     -      |     -     |   4.84   \n",
            "   5    |   380   |   0.029792   |     -      |     -     |   4.80   \n",
            "   5    |   400   |   0.033169   |     -      |     -     |   4.82   \n",
            "   5    |   420   |   0.045176   |     -      |     -     |   4.75   \n",
            "   5    |   440   |   0.022848   |     -      |     -     |   4.77   \n",
            "   5    |   460   |   0.027531   |     -      |     -     |   4.78   \n",
            "   5    |   480   |   0.022888   |     -      |     -     |   4.76   \n",
            "   5    |   500   |   0.033636   |     -      |     -     |   4.77   \n",
            "   5    |   520   |   0.039913   |     -      |     -     |   4.76   \n",
            "   5    |   540   |   0.023863   |     -      |     -     |   4.78   \n",
            "   5    |   560   |   0.025686   |     -      |     -     |   4.77   \n",
            "   5    |   580   |   0.057011   |     -      |     -     |   4.77   \n",
            "   5    |   600   |   0.029710   |     -      |     -     |   4.81   \n",
            "   5    |   620   |   0.036678   |     -      |     -     |   4.86   \n",
            "   5    |   640   |   0.036184   |     -      |     -     |   4.85   \n",
            "   5    |   660   |   0.051558   |     -      |     -     |   4.82   \n",
            "   5    |   680   |   0.026136   |     -      |     -     |   4.81   \n",
            "   5    |   700   |   0.031377   |     -      |     -     |   4.79   \n",
            "   5    |   720   |   0.045991   |     -      |     -     |   4.83   \n",
            "   5    |   740   |   0.035591   |     -      |     -     |   5.00   \n",
            "   5    |   760   |   0.028135   |     -      |     -     |   4.90   \n",
            "   5    |   780   |   0.040579   |     -      |     -     |   4.82   \n",
            "   5    |   800   |   0.037298   |     -      |     -     |   4.87   \n",
            "   5    |   820   |   0.039126   |     -      |     -     |   4.89   \n",
            "   5    |   840   |   0.048091   |     -      |     -     |   4.83   \n",
            "   5    |   860   |   0.044197   |     -      |     -     |   4.97   \n",
            "   5    |   880   |   0.029158   |     -      |     -     |   4.87   \n",
            "   5    |   900   |   0.025972   |     -      |     -     |   4.68   \n",
            "   5    |   920   |   0.022513   |     -      |     -     |   4.88   \n",
            "   5    |   940   |   0.020350   |     -      |     -     |   4.91   \n",
            "   5    |   960   |   0.019736   |     -      |     -     |   4.91   \n",
            "   5    |   980   |   0.040241   |     -      |     -     |   4.82   \n",
            "   5    |  1000   |   0.019176   |     -      |     -     |   5.00   \n",
            "   5    |  1020   |   0.036233   |     -      |     -     |   5.04   \n",
            "   5    |  1040   |   0.024427   |     -      |     -     |   4.93   \n",
            "   5    |  1060   |   0.033918   |     -      |     -     |   4.95   \n",
            "   5    |  1080   |   0.040730   |     -      |     -     |   4.82   \n",
            "   5    |  1100   |   0.028459   |     -      |     -     |   4.78   \n",
            "   5    |  1120   |   0.032753   |     -      |     -     |   4.82   \n",
            "   5    |  1140   |   0.027198   |     -      |     -     |   4.92   \n",
            "   5    |  1160   |   0.046518   |     -      |     -     |   4.77   \n",
            "   5    |  1180   |   0.025504   |     -      |     -     |   4.73   \n",
            "   5    |  1200   |   0.025426   |     -      |     -     |   4.53   \n",
            "   5    |  1220   |   0.019105   |     -      |     -     |   4.76   \n",
            "   5    |  1240   |   0.027238   |     -      |     -     |   4.79   \n",
            "   5    |  1260   |   0.039919   |     -      |     -     |   4.62   \n",
            "   5    |  1280   |   0.022126   |     -      |     -     |   4.67   \n",
            "   5    |  1300   |   0.021283   |     -      |     -     |   4.81   \n",
            "   5    |  1320   |   0.021651   |     -      |     -     |   4.59   \n",
            "   5    |  1340   |   0.022465   |     -      |     -     |   4.56   \n",
            "   5    |  1360   |   0.022297   |     -      |     -     |   4.83   \n",
            "   5    |  1380   |   0.043872   |     -      |     -     |   4.83   \n",
            "   5    |  1400   |   0.029744   |     -      |     -     |   4.85   \n",
            "   5    |  1420   |   0.036056   |     -      |     -     |   4.81   \n",
            "   5    |  1440   |   0.042273   |     -      |     -     |   4.86   \n",
            "   5    |  1460   |   0.036636   |     -      |     -     |   4.82   \n",
            "   5    |  1480   |   0.038263   |     -      |     -     |   4.86   \n",
            "   5    |  1500   |   0.040266   |     -      |     -     |   4.94   \n",
            "   5    |  1520   |   0.025086   |     -      |     -     |   4.84   \n",
            "   5    |  1540   |   0.043384   |     -      |     -     |   4.81   \n",
            "   5    |  1560   |   0.013746   |     -      |     -     |   4.83   \n",
            "   5    |  1580   |   0.065646   |     -      |     -     |   4.81   \n",
            "   5    |  1600   |   0.031809   |     -      |     -     |   4.65   \n",
            "   5    |  1620   |   0.020029   |     -      |     -     |   4.65   \n",
            "   5    |  1640   |   0.025101   |     -      |     -     |   4.79   \n",
            "   5    |  1660   |   0.019047   |     -      |     -     |   4.91   \n",
            "   5    |  1680   |   0.038275   |     -      |     -     |   4.68   \n",
            "   5    |  1700   |   0.035361   |     -      |     -     |   4.60   \n",
            "   5    |  1720   |   0.030717   |     -      |     -     |   4.77   \n",
            "   5    |  1740   |   0.044965   |     -      |     -     |   4.87   \n",
            "   5    |  1749   |   0.031601   |     -      |     -     |   2.18   \n",
            "----------------------------------------------------------------------\n",
            "   5    |    -    |   0.032384   |  0.437853  |   90.45   |  434.74  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=EPOCHS)\n",
        "train(bert_classifier, train_loader, val_loader, epochs=EPOCHS, evaluation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(bert_classifier.state_dict(), 'bert_cla_db2.ckpt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_data = TensorDataset(test_dataset[0],test_dataset[1],test_dataset[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "开始测试...\n"
          ]
        }
      ],
      "source": [
        "print('开始测试...')\n",
        "bert_classifier.eval()\n",
        "test_result = []\n",
        "for data in test_data:\n",
        "    b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in data)\n",
        "    b_input = b_input_ids.unsqueeze(0)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = bert_classifier(b_input)\n",
        "        pre = outputs.argmax(dim=1)\n",
        "        test_result.append([b_labels.item(), pre.item(), tokenizer.convert_ids_to_tokens(b_input_ids)])\n",
        "\n",
        "# 写入csv文件\n",
        "df = pd.DataFrame(test_result)\n",
        "df.to_csv('test_result_db2.csv',index=False, header=['id', 'label','text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>['[CLS]', '我', '申', '请', '了', '大', '同', '飞', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>['[CLS]', '你', '好', '我', '想', '开', '通', '家', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>['[CLS]', '没', '答', '应', '[UNK]', '[SEP]', '[P...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>['[CLS]', '免', '费', '领', '[UNK]', '[SEP]', '[P...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>['[CLS]', '为', '什', '么', '有', '话', '费', '还', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15946</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>['[CLS]', '毕', '节', '大', '方', '欧', '曼', '打', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15961</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>['[CLS]', '我', '知', '道', '你', '很', '累', '但', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15964</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>['[CLS]', '你', '是', '谁', '不', '重', '要', '[SEP]...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15966</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>['[CLS]', '操', '你', '可', '以', '吗', '[SEP]', '[...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15987</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>['[CLS]', '[UNK]', '[SEP]', '[PAD]', '[PAD]', ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2686 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       id  label                                               text\n",
              "3       2      0  ['[CLS]', '我', '申', '请', '了', '大', '同', '飞', '...\n",
              "7       1      0  ['[CLS]', '你', '好', '我', '想', '开', '通', '家', '...\n",
              "8       0      2  ['[CLS]', '没', '答', '应', '[UNK]', '[SEP]', '[P...\n",
              "15      1      0  ['[CLS]', '免', '费', '领', '[UNK]', '[SEP]', '[P...\n",
              "23      2      0  ['[CLS]', '为', '什', '么', '有', '话', '费', '还', '...\n",
              "...    ..    ...                                                ...\n",
              "15946   2      0  ['[CLS]', '毕', '节', '大', '方', '欧', '曼', '打', '...\n",
              "15961   1      2  ['[CLS]', '我', '知', '道', '你', '很', '累', '但', '...\n",
              "15964   2      0  ['[CLS]', '你', '是', '谁', '不', '重', '要', '[SEP]...\n",
              "15966   2      0  ['[CLS]', '操', '你', '可', '以', '吗', '[SEP]', '[...\n",
              "15987   1      0  ['[CLS]', '[UNK]', '[SEP]', '[PAD]', '[PAD]', ...\n",
              "\n",
              "[2686 rows x 3 columns]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('test_result_db2.csv')\n",
        "df[df.id!=df.label]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>['[CLS]', '很', '惊', '叹', '不', '用', '谢', '谢', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>['[CLS]', '你', '好', '我', '购', '票', '失', '败', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>['[CLS]', '好', '多', '次', '这', '样', '了', '[SEP]...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>['[CLS]', '我', '申', '请', '了', '大', '同', '飞', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>['[CLS]', '这', '个', '有', '效', '的', '吧', '[SEP]...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15995</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>['[CLS]', '除', '了', '用', '身', '份', '证', '还', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15996</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>['[CLS]', '我', '到', '了', '亲', '爱', '的', '第', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15997</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>['[CLS]', '山', '家', '庭', '宽', '带', '20', '##m'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15998</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>['[CLS]', '送', '票', '最', '起', '码', '应', '该', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15999</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>['[CLS]', '那', '就', '没', '问', '题', '了', '谢', '...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       id  label                                               text\n",
              "0       1      1  ['[CLS]', '很', '惊', '叹', '不', '用', '谢', '谢', '...\n",
              "1       2      2  ['[CLS]', '你', '好', '我', '购', '票', '失', '败', '...\n",
              "2       0      0  ['[CLS]', '好', '多', '次', '这', '样', '了', '[SEP]...\n",
              "3       2      0  ['[CLS]', '我', '申', '请', '了', '大', '同', '飞', '...\n",
              "4       1      1  ['[CLS]', '这', '个', '有', '效', '的', '吧', '[SEP]...\n",
              "...    ..    ...                                                ...\n",
              "15995   0      0  ['[CLS]', '除', '了', '用', '身', '份', '证', '还', '...\n",
              "15996   1      1  ['[CLS]', '我', '到', '了', '亲', '爱', '的', '第', '...\n",
              "15997   0      0  ['[CLS]', '山', '家', '庭', '宽', '带', '20', '##m'...\n",
              "15998   0      0  ['[CLS]', '送', '票', '最', '起', '码', '应', '该', '...\n",
              "15999   1      1  ['[CLS]', '那', '就', '没', '问', '题', '了', '谢', '...\n",
              "\n",
              "[16000 rows x 3 columns]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "7qCrKzdaZGEq"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.832125"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df[df.id==df.label])/len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>['[CLS]', '很', '惊', '叹', '不', '用', '谢', '谢', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>['[CLS]', '你', '好', '我', '购', '票', '失', '败', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>['[CLS]', '这', '个', '有', '效', '的', '吧', '[SEP]...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>['[CLS]', '嗯', '嗯', '那', '我', '知', '道', '了', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>['[CLS]', '你', '好', '我', '的', '投', '诉', '昨', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15986</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>['[CLS]', '银', '证', '转', '账', '密', '码', '输', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15989</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>['[CLS]', '优', '惠', '返', '还', '话', '费', '三', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15991</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>['[CLS]', '我', '擦', '联', '通', '我', '爱', '你', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15996</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>['[CLS]', '我', '到', '了', '亲', '爱', '的', '第', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15999</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>['[CLS]', '那', '就', '没', '问', '题', '了', '谢', '...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6222 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       id  label                                               text\n",
              "0       1      1  ['[CLS]', '很', '惊', '叹', '不', '用', '谢', '谢', '...\n",
              "1       2      2  ['[CLS]', '你', '好', '我', '购', '票', '失', '败', '...\n",
              "4       1      1  ['[CLS]', '这', '个', '有', '效', '的', '吧', '[SEP]...\n",
              "5       1      1  ['[CLS]', '嗯', '嗯', '那', '我', '知', '道', '了', '...\n",
              "6       1      1  ['[CLS]', '你', '好', '我', '的', '投', '诉', '昨', '...\n",
              "...    ..    ...                                                ...\n",
              "15986   2      2  ['[CLS]', '银', '证', '转', '账', '密', '码', '输', '...\n",
              "15989   2      2  ['[CLS]', '优', '惠', '返', '还', '话', '费', '三', '...\n",
              "15991   1      1  ['[CLS]', '我', '擦', '联', '通', '我', '爱', '你', '...\n",
              "15996   1      1  ['[CLS]', '我', '到', '了', '亲', '爱', '的', '第', '...\n",
              "15999   1      1  ['[CLS]', '那', '就', '没', '问', '题', '了', '谢', '...\n",
              "\n",
              "[6222 rows x 3 columns]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[df.label!=0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of train.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "ce613df70ec087c2b4dda2bc280e25d341f72f59d81afb32edf1d298cbbb8087"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('bert')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
