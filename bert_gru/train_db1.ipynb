{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dwb4U_C6AuO3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import csv\n",
        "import transformers\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as Data\n",
        "import torch.nn.functional as F\n",
        "import json\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoTokenizer, AutoModel, BertConfig, get_linear_schedule_with_warmup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJNwS3i_AuPJ",
        "outputId": "81223033-a504-40bb-8712-5c6dbb18b8e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Egx-TMmlAuO8"
      },
      "outputs": [],
      "source": [
        "device = 'cuda'\n",
        "#MODEL_NAME = \"hfl/chinese-bert-wwm\"\n",
        "MODEL_NAME = 'bert-base-chinese'\n",
        "MAX_LEN = 32\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 32\n",
        "LR = 5e-5 \n",
        "WARMUP_STEPS = 100\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI8wUOpYAuO9"
      },
      "source": [
        "创建dataset类"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QM9KnoKeIhBB"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gthyrxBpAuPA"
      },
      "source": [
        "创建load_dataset function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pCoBV4CnAuPB"
      },
      "outputs": [],
      "source": [
        "def load_dataset(filepath, max_len):\n",
        "    label = []\n",
        "    sentences = []\n",
        "    # load dataset\n",
        "    f = open(filepath, 'r', encoding='utf-8')\n",
        "    r = csv.reader(f)\n",
        "    for item in r:\n",
        "        if r.line_num == 1:\n",
        "            continue\n",
        "        label.append(int(item[0]))\n",
        "        sentences.append(item[1])\n",
        "        \n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    # For every sentence...\n",
        "    for data in sentences:\n",
        "        encoded_data = tokenizer.encode_plus(\n",
        "            text=data,                      # Preprocess sentence\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            max_length=max_len,             # Max length to truncate/pad\n",
        "            padding='max_length',           # Pad sentence to max length\n",
        "            return_attention_mask=True,      # Return attention mask\n",
        "            truncation= True\n",
        "            )\n",
        "        \n",
        "        # Add the outputs to the lists\n",
        "        input_ids.append(encoded_data.get('input_ids'))\n",
        "        attention_masks.append(encoded_data.get('attention_mask'))\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "    labels = torch.tensor(label)\n",
        "    return input_ids, attention_masks, labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1B6K3IQAuPD"
      },
      "source": [
        "load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "path = os.path.abspath(os.path.dirname(os.getcwd()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AZ17DQ1iAuPE"
      },
      "outputs": [],
      "source": [
        "train_dataset = load_dataset(f'{path}/data/train.csv', max_len = MAX_LEN)\n",
        "valid_dataset = load_dataset(f'{path}/data/dev.csv', max_len = MAX_LEN)\n",
        "test_dataset = load_dataset(f'{path}/data/test.csv', max_len = MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "P_dcsj5QAuPE"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "train_data = TensorDataset(train_dataset[0], train_dataset[1],train_dataset[2])\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_loader = DataLoader(train_data, sampler=train_sampler, batch_size = BATCH_SIZE)\n",
        "\n",
        "val_data = TensorDataset(valid_dataset[0],valid_dataset[1],valid_dataset[2])\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_loader = DataLoader(val_data,sampler=val_sampler, batch_size = BATCH_SIZE)\n",
        "\n",
        "test_data = TensorDataset(test_dataset[0],test_dataset[1],test_dataset[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "config = BertConfig.from_pretrained(MODEL_NAME)\n",
        "config.hidden_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyXXbzFKPtai",
        "outputId": "ac00e6a8-2361-4a87-ff12-af45506b06c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 46.9 ms\n",
            "Wall time: 50 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "# Create the BertClassfier class\n",
        "class BertClassifier(nn.Module):\n",
        "    \"\"\"Bert Model for Classification Tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 hidden_dim,\n",
        "                 output_dim,\n",
        "                 n_layers,\n",
        "                 bidirectional,\n",
        "                 dropout):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.bert = BertModel.from_pretrained(MODEL_NAME)\n",
        "        \n",
        "        embedding_dim = self.bert.config.to_dict()['hidden_size']\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.rnn = nn.GRU(embedding_dim,\n",
        "                          hidden_dim,\n",
        "                          num_layers = n_layers,\n",
        "                          bidirectional = bidirectional,\n",
        "                          batch_first = True,\n",
        "                          dropout = dropout)\n",
        "        \n",
        "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        # Feed input to BERT\n",
        "        outputs = self.bert(input_ids=input_ids,attention_mask=attention_mask)\n",
        "        \n",
        "        # Extract the last hidden state of\n",
        "        encoded_layers = outputs[0]\n",
        "        \n",
        "        _, hidden = self.rnn(encoded_layers)\n",
        "\n",
        "        if self.rnn.bidirectional:\n",
        "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "        else:\n",
        "            hidden = self.dropout(hidden[-1,:,:])\n",
        "            \n",
        "        logits = self.out(hidden)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gZYjRSpHP18K"
      },
      "outputs": [],
      "source": [
        "def initialize_model(epochs=EPOCHS):\n",
        "    HIDDEN_DIM = 256\n",
        "    OUTPUT_DIM = 3\n",
        "    N_LAYERS = 2\n",
        "    BIDIRECTIONAL = True\n",
        "    DROPOUT = 0.25\n",
        "    \n",
        "    bert_classifier = BertClassifier(HIDDEN_DIM,\n",
        "                            OUTPUT_DIM,\n",
        "                            N_LAYERS,\n",
        "                            BIDIRECTIONAL,\n",
        "                            DROPOUT)\n",
        "\n",
        "    # Tell PyTorch to run the model on GPU\n",
        "    bert_classifier.to(device)\n",
        "\n",
        "    # Create the optimizer\n",
        "    optimizer = AdamW(bert_classifier.parameters(),lr=LR)\n",
        "\n",
        "    # Set up the learning rate scheduler\n",
        "    total_steps = len(train_loader) * epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=WARMUP_STEPS,num_training_steps=total_steps)\n",
        "    return bert_classifier, optimizer, scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hXogwg3QQAi_"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "def train(model, train_dataloader, val_dataloader=None, epochs=EPOCHS , evaluation=False):\n",
        "    \"\"\"Train the BertClassifier model.\n",
        "    \"\"\"\n",
        "    # Start training loop\n",
        "    print(\"Start training...\\n\")\n",
        "    for epoch_i in range(epochs):\n",
        "        # Print the header of the result table\n",
        "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        # Measure the elapsed time of each epoch\n",
        "        t0_epoch, t0_batch = time.time(), time.time()\n",
        "\n",
        "        # Reset tracking variables at the beginning of each epoch\n",
        "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for s,batch in enumerate(train_dataloader):\n",
        "            batch_counts +=1\n",
        "            # Load batch to GPU\n",
        "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "            # Compute loss and accumulate the loss values\n",
        "            loss = loss_fn(logits.view(-1, 3), b_labels.view(-1))\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "            \n",
        "            # return loss, logits\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            \n",
        "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Print the loss values and time elapsed for every 20 batches\n",
        "            if (s % 20 == 0 and s != 0) or (s == len(train_dataloader) - 1):\n",
        "                # Calculate time elapsed for 20 batches\n",
        "                time_elapsed = time.time() - t0_batch\n",
        "\n",
        "                # Print training results\n",
        "                print(f\"{epoch_i + 1:^7} | {s:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "                # Reset batch tracking variables\n",
        "                batch_loss, batch_counts = 0, 0\n",
        "                t0_batch = time.time()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        print(\"-\"*70)\n",
        "        if evaluation == True:\n",
        "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
        "            time_elapsed = time.time() - t0_epoch \n",
        "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
        "            print(\"-\"*70)\n",
        "        print(\"\\n\")\n",
        "    \n",
        "    print(\"Training complete!\")\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        # Get the predictions\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "        # Calculate the accuracy rate\n",
        "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_loss, val_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcJjBdElQGuD",
        "outputId": "25427dde-333a-4185-f35d-c9d173cc5f4c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   1    |   20    |   0.894581   |     -      |     -     |   6.74   \n",
            "   1    |   40    |   0.684366   |     -      |     -     |   4.86   \n",
            "   1    |   60    |   0.646497   |     -      |     -     |   4.80   \n",
            "   1    |   80    |   0.545208   |     -      |     -     |   4.80   \n",
            "   1    |   100   |   0.535401   |     -      |     -     |   4.85   \n",
            "   1    |   120   |   0.482316   |     -      |     -     |   4.87   \n",
            "   1    |   140   |   0.448800   |     -      |     -     |   4.87   \n",
            "   1    |   160   |   0.435579   |     -      |     -     |   4.87   \n",
            "   1    |   180   |   0.479886   |     -      |     -     |   4.88   \n",
            "   1    |   200   |   0.391496   |     -      |     -     |   4.88   \n",
            "   1    |   220   |   0.429770   |     -      |     -     |   4.89   \n",
            "   1    |   240   |   0.437242   |     -      |     -     |   4.89   \n",
            "   1    |   260   |   0.407509   |     -      |     -     |   4.86   \n",
            "   1    |   280   |   0.460538   |     -      |     -     |   4.88   \n",
            "   1    |   300   |   0.410554   |     -      |     -     |   4.86   \n",
            "   1    |   320   |   0.416764   |     -      |     -     |   4.87   \n",
            "   1    |   340   |   0.414280   |     -      |     -     |   4.89   \n",
            "   1    |   360   |   0.471630   |     -      |     -     |   4.90   \n",
            "   1    |   380   |   0.356769   |     -      |     -     |   4.88   \n",
            "   1    |   400   |   0.407817   |     -      |     -     |   4.89   \n",
            "   1    |   420   |   0.428574   |     -      |     -     |   4.77   \n",
            "   1    |   440   |   0.426577   |     -      |     -     |   4.88   \n",
            "   1    |   460   |   0.430692   |     -      |     -     |   4.90   \n",
            "   1    |   480   |   0.453184   |     -      |     -     |   4.94   \n",
            "   1    |   500   |   0.388705   |     -      |     -     |   4.90   \n",
            "   1    |   520   |   0.370677   |     -      |     -     |   4.89   \n",
            "   1    |   540   |   0.402373   |     -      |     -     |   4.91   \n",
            "   1    |   560   |   0.432362   |     -      |     -     |   4.92   \n",
            "   1    |   580   |   0.418083   |     -      |     -     |   4.89   \n",
            "   1    |   600   |   0.406837   |     -      |     -     |   4.90   \n",
            "   1    |   620   |   0.345169   |     -      |     -     |   4.91   \n",
            "   1    |   640   |   0.396757   |     -      |     -     |   4.90   \n",
            "   1    |   660   |   0.385893   |     -      |     -     |   4.91   \n",
            "   1    |   680   |   0.417491   |     -      |     -     |   4.90   \n",
            "   1    |   700   |   0.414071   |     -      |     -     |   4.89   \n",
            "   1    |   714   |   0.339006   |     -      |     -     |   3.43   \n",
            "----------------------------------------------------------------------\n",
            "   1    |    -    |   0.451930   |  0.363675  |   86.23   |  182.65  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   2    |   20    |   0.281366   |     -      |     -     |   5.05   \n",
            "   2    |   40    |   0.298895   |     -      |     -     |   4.91   \n",
            "   2    |   60    |   0.325465   |     -      |     -     |   4.92   \n",
            "   2    |   80    |   0.269831   |     -      |     -     |   4.91   \n",
            "   2    |   100   |   0.258214   |     -      |     -     |   4.92   \n",
            "   2    |   120   |   0.298361   |     -      |     -     |   4.91   \n",
            "   2    |   140   |   0.285575   |     -      |     -     |   4.91   \n",
            "   2    |   160   |   0.307231   |     -      |     -     |   4.91   \n",
            "   2    |   180   |   0.348970   |     -      |     -     |   4.89   \n",
            "   2    |   200   |   0.305225   |     -      |     -     |   4.90   \n",
            "   2    |   220   |   0.350364   |     -      |     -     |   4.86   \n",
            "   2    |   240   |   0.295178   |     -      |     -     |   4.86   \n",
            "   2    |   260   |   0.253623   |     -      |     -     |   4.91   \n",
            "   2    |   280   |   0.286348   |     -      |     -     |   4.90   \n",
            "   2    |   300   |   0.246230   |     -      |     -     |   4.90   \n",
            "   2    |   320   |   0.290066   |     -      |     -     |   4.90   \n",
            "   2    |   340   |   0.296937   |     -      |     -     |   4.91   \n",
            "   2    |   360   |   0.277227   |     -      |     -     |   4.91   \n",
            "   2    |   380   |   0.281434   |     -      |     -     |   4.92   \n",
            "   2    |   400   |   0.300973   |     -      |     -     |   4.91   \n",
            "   2    |   420   |   0.282919   |     -      |     -     |   4.91   \n",
            "   2    |   440   |   0.299148   |     -      |     -     |   4.91   \n",
            "   2    |   460   |   0.334916   |     -      |     -     |   4.91   \n",
            "   2    |   480   |   0.275106   |     -      |     -     |   4.95   \n",
            "   2    |   500   |   0.305325   |     -      |     -     |   4.92   \n",
            "   2    |   520   |   0.318677   |     -      |     -     |   4.93   \n",
            "   2    |   540   |   0.344732   |     -      |     -     |   4.92   \n",
            "   2    |   560   |   0.324308   |     -      |     -     |   4.96   \n",
            "   2    |   580   |   0.255462   |     -      |     -     |   4.94   \n",
            "   2    |   600   |   0.296014   |     -      |     -     |   4.91   \n",
            "   2    |   620   |   0.344389   |     -      |     -     |   4.93   \n",
            "   2    |   640   |   0.282614   |     -      |     -     |   4.93   \n",
            "   2    |   660   |   0.272392   |     -      |     -     |   4.94   \n",
            "   2    |   680   |   0.265172   |     -      |     -     |   4.93   \n",
            "   2    |   700   |   0.287332   |     -      |     -     |   4.93   \n",
            "   2    |   714   |   0.275725   |     -      |     -     |   3.43   \n",
            "----------------------------------------------------------------------\n",
            "   2    |    -    |   0.295191   |  0.384833  |   85.16   |  181.88  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   3    |   20    |   0.182900   |     -      |     -     |   5.08   \n",
            "   3    |   40    |   0.192525   |     -      |     -     |   4.96   \n",
            "   3    |   60    |   0.215043   |     -      |     -     |   4.93   \n",
            "   3    |   80    |   0.193074   |     -      |     -     |   4.93   \n",
            "   3    |   100   |   0.198828   |     -      |     -     |   4.95   \n",
            "   3    |   120   |   0.194039   |     -      |     -     |   4.93   \n",
            "   3    |   140   |   0.150733   |     -      |     -     |   4.95   \n",
            "   3    |   160   |   0.217417   |     -      |     -     |   4.94   \n",
            "   3    |   180   |   0.172030   |     -      |     -     |   4.93   \n",
            "   3    |   200   |   0.211994   |     -      |     -     |   4.93   \n",
            "   3    |   220   |   0.206246   |     -      |     -     |   4.93   \n",
            "   3    |   240   |   0.152683   |     -      |     -     |   4.94   \n",
            "   3    |   260   |   0.163792   |     -      |     -     |   4.93   \n",
            "   3    |   280   |   0.195405   |     -      |     -     |   4.93   \n",
            "   3    |   300   |   0.176231   |     -      |     -     |   4.92   \n",
            "   3    |   320   |   0.199077   |     -      |     -     |   4.95   \n",
            "   3    |   340   |   0.178456   |     -      |     -     |   4.91   \n",
            "   3    |   360   |   0.196348   |     -      |     -     |   4.92   \n",
            "   3    |   380   |   0.193089   |     -      |     -     |   4.91   \n",
            "   3    |   400   |   0.194800   |     -      |     -     |   4.94   \n",
            "   3    |   420   |   0.183762   |     -      |     -     |   4.98   \n",
            "   3    |   440   |   0.193072   |     -      |     -     |   4.94   \n",
            "   3    |   460   |   0.213861   |     -      |     -     |   4.92   \n",
            "   3    |   480   |   0.200547   |     -      |     -     |   4.92   \n",
            "   3    |   500   |   0.168094   |     -      |     -     |   4.96   \n",
            "   3    |   520   |   0.217768   |     -      |     -     |   4.93   \n",
            "   3    |   540   |   0.235374   |     -      |     -     |   4.93   \n",
            "   3    |   560   |   0.172527   |     -      |     -     |   4.93   \n",
            "   3    |   580   |   0.165903   |     -      |     -     |   4.93   \n",
            "   3    |   600   |   0.183422   |     -      |     -     |   4.93   \n",
            "   3    |   620   |   0.207875   |     -      |     -     |   4.96   \n",
            "   3    |   640   |   0.201886   |     -      |     -     |   4.94   \n",
            "   3    |   660   |   0.170635   |     -      |     -     |   4.94   \n",
            "   3    |   680   |   0.189569   |     -      |     -     |   4.95   \n",
            "   3    |   700   |   0.202734   |     -      |     -     |   4.89   \n",
            "   3    |   714   |   0.120869   |     -      |     -     |   3.46   \n",
            "----------------------------------------------------------------------\n",
            "   3    |    -    |   0.189804   |  0.463603  |   85.29   |  182.68  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   4    |   20    |   0.106785   |     -      |     -     |   5.06   \n",
            "   4    |   40    |   0.090169   |     -      |     -     |   4.94   \n",
            "   4    |   60    |   0.107550   |     -      |     -     |   4.96   \n",
            "   4    |   80    |   0.120983   |     -      |     -     |   4.94   \n",
            "   4    |   100   |   0.096205   |     -      |     -     |   4.94   \n",
            "   4    |   120   |   0.130962   |     -      |     -     |   4.95   \n",
            "   4    |   140   |   0.115355   |     -      |     -     |   4.93   \n",
            "   4    |   160   |   0.096037   |     -      |     -     |   4.96   \n",
            "   4    |   180   |   0.057383   |     -      |     -     |   4.94   \n",
            "   4    |   200   |   0.108710   |     -      |     -     |   4.94   \n",
            "   4    |   220   |   0.083889   |     -      |     -     |   4.97   \n",
            "   4    |   240   |   0.090938   |     -      |     -     |   4.94   \n",
            "   4    |   260   |   0.086041   |     -      |     -     |   4.96   \n",
            "   4    |   280   |   0.117323   |     -      |     -     |   4.95   \n",
            "   4    |   300   |   0.095699   |     -      |     -     |   4.93   \n",
            "   4    |   320   |   0.086101   |     -      |     -     |   4.93   \n",
            "   4    |   340   |   0.110071   |     -      |     -     |   4.94   \n",
            "   4    |   360   |   0.111792   |     -      |     -     |   4.94   \n",
            "   4    |   380   |   0.125942   |     -      |     -     |   4.95   \n",
            "   4    |   400   |   0.098518   |     -      |     -     |   4.93   \n",
            "   4    |   420   |   0.105964   |     -      |     -     |   4.94   \n",
            "   4    |   440   |   0.124177   |     -      |     -     |   4.94   \n",
            "   4    |   460   |   0.099071   |     -      |     -     |   4.94   \n",
            "   4    |   480   |   0.126948   |     -      |     -     |   4.94   \n",
            "   4    |   500   |   0.107897   |     -      |     -     |   4.97   \n",
            "   4    |   520   |   0.117410   |     -      |     -     |   4.95   \n",
            "   4    |   540   |   0.113087   |     -      |     -     |   4.94   \n",
            "   4    |   560   |   0.082526   |     -      |     -     |   4.92   \n",
            "   4    |   580   |   0.112772   |     -      |     -     |   4.95   \n",
            "   4    |   600   |   0.067137   |     -      |     -     |   4.93   \n",
            "   4    |   620   |   0.105197   |     -      |     -     |   4.95   \n",
            "   4    |   640   |   0.100719   |     -      |     -     |   4.94   \n",
            "   4    |   660   |   0.111619   |     -      |     -     |   4.94   \n",
            "   4    |   680   |   0.126686   |     -      |     -     |   4.94   \n",
            "   4    |   700   |   0.108369   |     -      |     -     |   4.95   \n",
            "   4    |   714   |   0.098428   |     -      |     -     |   3.45   \n",
            "----------------------------------------------------------------------\n",
            "   4    |    -    |   0.104064   |  0.542416  |   84.83   |  182.95  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   5    |   20    |   0.054109   |     -      |     -     |   5.08   \n",
            "   5    |   40    |   0.062505   |     -      |     -     |   4.94   \n",
            "   5    |   60    |   0.045368   |     -      |     -     |   4.95   \n",
            "   5    |   80    |   0.054287   |     -      |     -     |   4.93   \n",
            "   5    |   100   |   0.060644   |     -      |     -     |   4.94   \n",
            "   5    |   120   |   0.053194   |     -      |     -     |   4.95   \n",
            "   5    |   140   |   0.057279   |     -      |     -     |   4.90   \n",
            "   5    |   160   |   0.053940   |     -      |     -     |   4.97   \n",
            "   5    |   180   |   0.073238   |     -      |     -     |   4.96   \n",
            "   5    |   200   |   0.051917   |     -      |     -     |   4.94   \n",
            "   5    |   220   |   0.037531   |     -      |     -     |   4.95   \n",
            "   5    |   240   |   0.072839   |     -      |     -     |   4.94   \n",
            "   5    |   260   |   0.027153   |     -      |     -     |   4.95   \n",
            "   5    |   280   |   0.045021   |     -      |     -     |   4.96   \n",
            "   5    |   300   |   0.054060   |     -      |     -     |   4.90   \n",
            "   5    |   320   |   0.065776   |     -      |     -     |   4.95   \n",
            "   5    |   340   |   0.069442   |     -      |     -     |   4.97   \n",
            "   5    |   360   |   0.049090   |     -      |     -     |   4.93   \n",
            "   5    |   380   |   0.060325   |     -      |     -     |   4.95   \n",
            "   5    |   400   |   0.056269   |     -      |     -     |   4.95   \n",
            "   5    |   420   |   0.042255   |     -      |     -     |   4.93   \n",
            "   5    |   440   |   0.036401   |     -      |     -     |   4.95   \n",
            "   5    |   460   |   0.054193   |     -      |     -     |   4.95   \n",
            "   5    |   480   |   0.053757   |     -      |     -     |   4.94   \n",
            "   5    |   500   |   0.035362   |     -      |     -     |   4.95   \n",
            "   5    |   520   |   0.028186   |     -      |     -     |   4.96   \n",
            "   5    |   540   |   0.022611   |     -      |     -     |   4.93   \n",
            "   5    |   560   |   0.066708   |     -      |     -     |   4.92   \n",
            "   5    |   580   |   0.058919   |     -      |     -     |   4.91   \n",
            "   5    |   600   |   0.076418   |     -      |     -     |   4.93   \n",
            "   5    |   620   |   0.062261   |     -      |     -     |   4.92   \n",
            "   5    |   640   |   0.053957   |     -      |     -     |   4.93   \n",
            "   5    |   660   |   0.031881   |     -      |     -     |   4.94   \n",
            "   5    |   680   |   0.046196   |     -      |     -     |   4.95   \n",
            "   5    |   700   |   0.041560   |     -      |     -     |   4.93   \n",
            "   5    |   714   |   0.057666   |     -      |     -     |   3.45   \n",
            "----------------------------------------------------------------------\n",
            "   5    |    -    |   0.051964   |  0.667110  |   84.92   |  182.89  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=EPOCHS)\n",
        "train(bert_classifier, train_loader, val_loader, epochs=EPOCHS, evaluation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(bert_classifier.state_dict(), 'bert_cla.ckpt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "compare linear and gru"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_data = TensorDataset(test_dataset[0],test_dataset[1],test_dataset[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "开始测试...\n"
          ]
        }
      ],
      "source": [
        "print('开始测试...')\n",
        "bert_classifier.eval()\n",
        "test_result = []\n",
        "for data in test_data:\n",
        "    b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in data)\n",
        "    b_input = b_input_ids.unsqueeze(0)  \n",
        "    b_attn_mask = b_attn_mask.unsqueeze(0)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = bert_classifier(b_input,b_attn_mask)\n",
        "        pre = outputs.argmax(dim=1)\n",
        "        test_result.append([b_labels.item(), pre.item(), tokenizer.convert_ids_to_tokens(b_input_ids)])\n",
        "\n",
        "# 写入csv文件\n",
        "df = pd.DataFrame(test_result)\n",
        "df.to_csv('test_result.csv',index=False, header=['id', 'label','text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>['[CLS]', '乱', '世', '佳', '人', '[SEP]', '[PAD]'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>['[CLS]', '下', '场', '加', '油', '吧', '[SEP]', '[...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>['[CLS]', '刘', '军', '不', '服', '[SEP]', '[PAD]'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>['[CLS]', '打', '屎', '棍', '[SEP]', '[PAD]', '[P...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>['[CLS]', '我', '要', '尊', '严', '[SEP]', '[PAD]'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6522</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>['[CLS]', '78', '##9', '玩', '不', '起', '[SEP]',...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6524</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>['[CLS]', '应', '该', '叫', '先', '知', '[SEP]', '[...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6525</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>['[CLS]', '3', '输', '了', '。', '[SEP]', '[PAD]'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6528</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>['[CLS]', '又', '来', '水', '了', '[SEP]', '[PAD]'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6529</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>['[CLS]', '数', '据', '太', 'nb', '了', '[SEP]', '...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>982 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      id  label                                               text\n",
              "6      0      1  ['[CLS]', '乱', '世', '佳', '人', '[SEP]', '[PAD]'...\n",
              "11     1      0  ['[CLS]', '下', '场', '加', '油', '吧', '[SEP]', '[...\n",
              "22     0      2  ['[CLS]', '刘', '军', '不', '服', '[SEP]', '[PAD]'...\n",
              "23     2      0  ['[CLS]', '打', '屎', '棍', '[SEP]', '[PAD]', '[P...\n",
              "30     0      1  ['[CLS]', '我', '要', '尊', '严', '[SEP]', '[PAD]'...\n",
              "...   ..    ...                                                ...\n",
              "6522   0      2  ['[CLS]', '78', '##9', '玩', '不', '起', '[SEP]',...\n",
              "6524   1      0  ['[CLS]', '应', '该', '叫', '先', '知', '[SEP]', '[...\n",
              "6525   2      0  ['[CLS]', '3', '输', '了', '。', '[SEP]', '[PAD]'...\n",
              "6528   0      2  ['[CLS]', '又', '来', '水', '了', '[SEP]', '[PAD]'...\n",
              "6529   1      2  ['[CLS]', '数', '据', '太', 'nb', '了', '[SEP]', '...\n",
              "\n",
              "[982 rows x 3 columns]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('test_result.csv')\n",
        "df[df.id!=df.label]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "7qCrKzdaZGEq"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8497322111706197"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df[df.id==df.label])/len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of train.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "ce613df70ec087c2b4dda2bc280e25d341f72f59d81afb32edf1d298cbbb8087"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('bert')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
