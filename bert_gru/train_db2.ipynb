{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dwb4U_C6AuO3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import csv\n",
        "import transformers\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as Data\n",
        "import torch.nn.functional as F\n",
        "import json\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoTokenizer, AutoModel, BertConfig, get_linear_schedule_with_warmup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJNwS3i_AuPJ",
        "outputId": "81223033-a504-40bb-8712-5c6dbb18b8e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Egx-TMmlAuO8"
      },
      "outputs": [],
      "source": [
        "device = 'cuda'\n",
        "MODEL_NAME = 'bert-base-chinese'\n",
        "MAX_LEN = 32\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 32\n",
        "LR = 5e-5 \n",
        "WARMUP_STEPS = 100\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI8wUOpYAuO9"
      },
      "source": [
        "创建dataset类"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QM9KnoKeIhBB"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gthyrxBpAuPA"
      },
      "source": [
        "创建load_dataset function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pCoBV4CnAuPB"
      },
      "outputs": [],
      "source": [
        "def load_dataset(filepath, max_len):\n",
        "    label = []\n",
        "    sentences = []\n",
        "    # load dataset\n",
        "    f = open(filepath, 'r', encoding='utf-8')\n",
        "    r = csv.reader(f)\n",
        "    for item in r:\n",
        "        if r.line_num == 1:\n",
        "            continue\n",
        "        label.append(int(item[0]))\n",
        "        sentences.append(item[1])\n",
        "        \n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    # For every sentence...\n",
        "    for data in sentences:\n",
        "        encoded_data = tokenizer.encode_plus(\n",
        "            text=data,                      # Preprocess sentence\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            max_length=max_len,             # Max length to truncate/pad\n",
        "            padding='max_length',           # Pad sentence to max length\n",
        "            return_attention_mask=True,      # Return attention mask\n",
        "            truncation= True\n",
        "            )\n",
        "        \n",
        "        # Add the outputs to the lists\n",
        "        input_ids.append(encoded_data.get('input_ids'))\n",
        "        attention_masks.append(encoded_data.get('attention_mask'))\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "    labels = torch.tensor(label)\n",
        "    return input_ids, attention_masks, labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1B6K3IQAuPD"
      },
      "source": [
        "load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "path = os.path.abspath(os.path.dirname(os.getcwd()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AZ17DQ1iAuPE"
      },
      "outputs": [],
      "source": [
        "train_dataset = load_dataset(f'{path}/data/db2/train.csv', max_len = MAX_LEN)\n",
        "valid_dataset = load_dataset(f'{path}/data/db2/dev.csv', max_len = MAX_LEN)\n",
        "test_dataset = load_dataset(f'{path}/data//db2/test.csv', max_len = MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "P_dcsj5QAuPE"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "train_data = TensorDataset(train_dataset[0], train_dataset[1],train_dataset[2])\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_loader = DataLoader(train_data, sampler=train_sampler, batch_size = BATCH_SIZE)\n",
        "\n",
        "val_data = TensorDataset(valid_dataset[0],valid_dataset[1],valid_dataset[2])\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_loader = DataLoader(val_data,sampler=val_sampler, batch_size = BATCH_SIZE)\n",
        "\n",
        "test_data = TensorDataset(test_dataset[0],test_dataset[1],test_dataset[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "config = BertConfig.from_pretrained(MODEL_NAME)\n",
        "config.hidden_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyXXbzFKPtai",
        "outputId": "ac00e6a8-2361-4a87-ff12-af45506b06c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 31.2 ms\n",
            "Wall time: 37 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "# Create the BertClassfier class\n",
        "class BertClassifier(nn.Module):\n",
        "    \"\"\"Bert Model for Classification Tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 hidden_dim,\n",
        "                 output_dim,\n",
        "                 n_layers,\n",
        "                 bidirectional,\n",
        "                 dropout):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.bert = BertModel.from_pretrained(MODEL_NAME)\n",
        "        \n",
        "        embedding_dim = self.bert.config.to_dict()['hidden_size']\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.rnn = nn.GRU(embedding_dim,\n",
        "                          hidden_dim,\n",
        "                          num_layers = n_layers,\n",
        "                          bidirectional = bidirectional,\n",
        "                          batch_first = True,\n",
        "                          dropout = dropout)\n",
        "        \n",
        "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        # Feed input to BERT\n",
        "        outputs = self.bert(input_ids=input_ids,attention_mask=attention_mask)\n",
        "        \n",
        "        # Extract the last hidden state of\n",
        "        encoded_layers = outputs[0]\n",
        "        \n",
        "        _, hidden = self.rnn(encoded_layers)\n",
        "\n",
        "        if self.rnn.bidirectional:\n",
        "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "        else:\n",
        "            hidden = self.dropout(hidden[-1,:,:])\n",
        "            \n",
        "        logits = self.out(hidden)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gZYjRSpHP18K"
      },
      "outputs": [],
      "source": [
        "def initialize_model(epochs=EPOCHS):\n",
        "    HIDDEN_DIM = 256\n",
        "    OUTPUT_DIM = 3\n",
        "    N_LAYERS = 2\n",
        "    BIDIRECTIONAL = True\n",
        "    DROPOUT = 0.25\n",
        "    \n",
        "    bert_classifier = BertClassifier(HIDDEN_DIM,\n",
        "                            OUTPUT_DIM,\n",
        "                            N_LAYERS,\n",
        "                            BIDIRECTIONAL,\n",
        "                            DROPOUT)\n",
        "\n",
        "    # Tell PyTorch to run the model on GPU\n",
        "    bert_classifier.to(device)\n",
        "\n",
        "    # Create the optimizer\n",
        "    optimizer = AdamW(bert_classifier.parameters(),lr=LR)\n",
        "\n",
        "    # Set up the learning rate scheduler\n",
        "    total_steps = len(train_loader) * epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=WARMUP_STEPS,num_training_steps=total_steps)\n",
        "    return bert_classifier, optimizer, scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hXogwg3QQAi_"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "def train(model, train_dataloader, val_dataloader=None, epochs=EPOCHS , evaluation=False):\n",
        "    \"\"\"Train the BertClassifier model.\n",
        "    \"\"\"\n",
        "    # Start training loop\n",
        "    print(\"Start training...\\n\")\n",
        "    for epoch_i in range(epochs):\n",
        "        # Print the header of the result table\n",
        "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        # Measure the elapsed time of each epoch\n",
        "        t0_epoch, t0_batch = time.time(), time.time()\n",
        "\n",
        "        # Reset tracking variables at the beginning of each epoch\n",
        "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for s,batch in enumerate(train_dataloader):\n",
        "            batch_counts +=1\n",
        "            # Load batch to GPU\n",
        "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "            # Compute loss and accumulate the loss values\n",
        "            loss = loss_fn(logits.view(-1, 3), b_labels.view(-1))\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "            \n",
        "            # return loss, logits\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            \n",
        "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Print the loss values and time elapsed for every 20 batches\n",
        "            if (s % 20 == 0 and s != 0) or (s == len(train_dataloader) - 1):\n",
        "                # Calculate time elapsed for 20 batches\n",
        "                time_elapsed = time.time() - t0_batch\n",
        "\n",
        "                # Print training results\n",
        "                print(f\"{epoch_i + 1:^7} | {s:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "                # Reset batch tracking variables\n",
        "                batch_loss, batch_counts = 0, 0\n",
        "                t0_batch = time.time()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        print(\"-\"*70)\n",
        "        if evaluation == True:\n",
        "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
        "            time_elapsed = time.time() - t0_epoch \n",
        "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
        "            print(\"-\"*70)\n",
        "        print(\"\\n\")\n",
        "    \n",
        "    print(\"Training complete!\")\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        # Get the predictions\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "        # Calculate the accuracy rate\n",
        "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_loss, val_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcJjBdElQGuD",
        "outputId": "25427dde-333a-4185-f35d-c9d173cc5f4c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   1    |   20    |   1.022089   |     -      |     -     |   6.22   \n",
            "   1    |   40    |   0.830621   |     -      |     -     |   4.63   \n",
            "   1    |   60    |   0.569835   |     -      |     -     |   4.67   \n",
            "   1    |   80    |   0.429148   |     -      |     -     |   4.67   \n",
            "   1    |   100   |   0.469965   |     -      |     -     |   4.69   \n",
            "   1    |   120   |   0.431294   |     -      |     -     |   4.69   \n",
            "   1    |   140   |   0.394205   |     -      |     -     |   4.68   \n",
            "   1    |   160   |   0.386601   |     -      |     -     |   4.69   \n",
            "   1    |   180   |   0.430472   |     -      |     -     |   4.69   \n",
            "   1    |   200   |   0.378823   |     -      |     -     |   4.70   \n",
            "   1    |   220   |   0.405223   |     -      |     -     |   4.70   \n",
            "   1    |   240   |   0.371042   |     -      |     -     |   4.68   \n",
            "   1    |   260   |   0.375423   |     -      |     -     |   4.68   \n",
            "   1    |   280   |   0.369503   |     -      |     -     |   4.72   \n",
            "   1    |   300   |   0.336756   |     -      |     -     |   4.69   \n",
            "   1    |   320   |   0.336946   |     -      |     -     |   4.71   \n",
            "   1    |   340   |   0.434254   |     -      |     -     |   4.71   \n",
            "   1    |   360   |   0.328655   |     -      |     -     |   4.71   \n",
            "   1    |   380   |   0.378056   |     -      |     -     |   4.71   \n",
            "   1    |   400   |   0.332639   |     -      |     -     |   4.71   \n",
            "   1    |   420   |   0.357687   |     -      |     -     |   4.70   \n",
            "   1    |   440   |   0.309952   |     -      |     -     |   4.71   \n",
            "   1    |   460   |   0.331750   |     -      |     -     |   4.70   \n",
            "   1    |   480   |   0.408335   |     -      |     -     |   4.69   \n",
            "   1    |   500   |   0.340464   |     -      |     -     |   4.70   \n",
            "   1    |   520   |   0.342694   |     -      |     -     |   4.72   \n",
            "   1    |   540   |   0.356949   |     -      |     -     |   4.72   \n",
            "   1    |   560   |   0.289257   |     -      |     -     |   4.72   \n",
            "   1    |   580   |   0.314833   |     -      |     -     |   4.73   \n",
            "   1    |   600   |   0.312910   |     -      |     -     |   4.72   \n",
            "   1    |   620   |   0.317613   |     -      |     -     |   4.72   \n",
            "   1    |   640   |   0.323021   |     -      |     -     |   4.71   \n",
            "   1    |   660   |   0.326863   |     -      |     -     |   4.72   \n",
            "   1    |   680   |   0.284461   |     -      |     -     |   4.72   \n",
            "   1    |   700   |   0.360037   |     -      |     -     |   4.72   \n",
            "   1    |   720   |   0.331578   |     -      |     -     |   4.73   \n",
            "   1    |   740   |   0.313194   |     -      |     -     |   4.72   \n",
            "   1    |   760   |   0.308988   |     -      |     -     |   4.73   \n",
            "   1    |   780   |   0.373606   |     -      |     -     |   4.72   \n",
            "   1    |   800   |   0.335207   |     -      |     -     |   4.74   \n",
            "   1    |   820   |   0.311347   |     -      |     -     |   4.73   \n",
            "   1    |   840   |   0.291080   |     -      |     -     |   4.72   \n",
            "   1    |   860   |   0.284565   |     -      |     -     |   4.71   \n",
            "   1    |   880   |   0.261427   |     -      |     -     |   4.72   \n",
            "   1    |   900   |   0.309220   |     -      |     -     |   4.73   \n",
            "   1    |   920   |   0.296455   |     -      |     -     |   4.73   \n",
            "   1    |   940   |   0.318748   |     -      |     -     |   4.73   \n",
            "   1    |   960   |   0.233043   |     -      |     -     |   4.72   \n",
            "   1    |   980   |   0.334979   |     -      |     -     |   4.74   \n",
            "   1    |  1000   |   0.292697   |     -      |     -     |   4.74   \n",
            "   1    |  1020   |   0.275336   |     -      |     -     |   4.72   \n",
            "   1    |  1040   |   0.343907   |     -      |     -     |   4.73   \n",
            "   1    |  1060   |   0.314576   |     -      |     -     |   4.74   \n",
            "   1    |  1080   |   0.295868   |     -      |     -     |   4.71   \n",
            "   1    |  1100   |   0.288389   |     -      |     -     |   4.72   \n",
            "   1    |  1120   |   0.290050   |     -      |     -     |   4.72   \n",
            "   1    |  1140   |   0.328516   |     -      |     -     |   4.72   \n",
            "   1    |  1160   |   0.289461   |     -      |     -     |   4.72   \n",
            "   1    |  1180   |   0.278691   |     -      |     -     |   4.73   \n",
            "   1    |  1200   |   0.331154   |     -      |     -     |   4.72   \n",
            "   1    |  1220   |   0.302392   |     -      |     -     |   4.73   \n",
            "   1    |  1240   |   0.310501   |     -      |     -     |   4.72   \n",
            "   1    |  1260   |   0.291827   |     -      |     -     |   4.74   \n",
            "   1    |  1280   |   0.260383   |     -      |     -     |   4.72   \n",
            "   1    |  1300   |   0.320530   |     -      |     -     |   4.74   \n",
            "   1    |  1320   |   0.301566   |     -      |     -     |   4.74   \n",
            "   1    |  1340   |   0.326230   |     -      |     -     |   4.70   \n",
            "   1    |  1360   |   0.341644   |     -      |     -     |   4.73   \n",
            "   1    |  1380   |   0.309549   |     -      |     -     |   4.72   \n",
            "   1    |  1400   |   0.306645   |     -      |     -     |   4.73   \n",
            "   1    |  1420   |   0.299664   |     -      |     -     |   4.72   \n",
            "   1    |  1440   |   0.294898   |     -      |     -     |   4.73   \n",
            "   1    |  1460   |   0.325643   |     -      |     -     |   4.73   \n",
            "   1    |  1480   |   0.292680   |     -      |     -     |   4.73   \n",
            "   1    |  1500   |   0.326520   |     -      |     -     |   4.72   \n",
            "   1    |  1520   |   0.285046   |     -      |     -     |   4.73   \n",
            "   1    |  1540   |   0.297720   |     -      |     -     |   4.69   \n",
            "   1    |  1560   |   0.310458   |     -      |     -     |   4.73   \n",
            "   1    |  1580   |   0.302424   |     -      |     -     |   4.73   \n",
            "   1    |  1600   |   0.321219   |     -      |     -     |   4.74   \n",
            "   1    |  1620   |   0.259380   |     -      |     -     |   4.72   \n",
            "   1    |  1640   |   0.263233   |     -      |     -     |   4.72   \n",
            "   1    |  1660   |   0.239243   |     -      |     -     |   4.74   \n",
            "   1    |  1680   |   0.271243   |     -      |     -     |   4.72   \n",
            "   1    |  1700   |   0.263560   |     -      |     -     |   4.72   \n",
            "   1    |  1720   |   0.295423   |     -      |     -     |   4.72   \n",
            "   1    |  1740   |   0.315990   |     -      |     -     |   4.74   \n",
            "   1    |  1749   |   0.301726   |     -      |     -     |   2.13   \n",
            "----------------------------------------------------------------------\n",
            "   1    |    -    |   0.341062   |  0.261501  |   89.80   |  428.72  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   2    |   20    |   0.200589   |     -      |     -     |   4.86   \n",
            "   2    |   40    |   0.225747   |     -      |     -     |   4.72   \n",
            "   2    |   60    |   0.177101   |     -      |     -     |   4.73   \n",
            "   2    |   80    |   0.213716   |     -      |     -     |   4.73   \n",
            "   2    |   100   |   0.196997   |     -      |     -     |   4.74   \n",
            "   2    |   120   |   0.196397   |     -      |     -     |   4.73   \n",
            "   2    |   140   |   0.208878   |     -      |     -     |   4.74   \n",
            "   2    |   160   |   0.185869   |     -      |     -     |   4.73   \n",
            "   2    |   180   |   0.254167   |     -      |     -     |   4.74   \n",
            "   2    |   200   |   0.211148   |     -      |     -     |   4.75   \n",
            "   2    |   220   |   0.240822   |     -      |     -     |   4.73   \n",
            "   2    |   240   |   0.283299   |     -      |     -     |   4.74   \n",
            "   2    |   260   |   0.195829   |     -      |     -     |   4.75   \n",
            "   2    |   280   |   0.237342   |     -      |     -     |   4.72   \n",
            "   2    |   300   |   0.166030   |     -      |     -     |   4.73   \n",
            "   2    |   320   |   0.228153   |     -      |     -     |   4.73   \n",
            "   2    |   340   |   0.189575   |     -      |     -     |   4.72   \n",
            "   2    |   360   |   0.249729   |     -      |     -     |   4.74   \n",
            "   2    |   380   |   0.205671   |     -      |     -     |   4.73   \n",
            "   2    |   400   |   0.213548   |     -      |     -     |   4.74   \n",
            "   2    |   420   |   0.215918   |     -      |     -     |   4.74   \n",
            "   2    |   440   |   0.226347   |     -      |     -     |   4.74   \n",
            "   2    |   460   |   0.203189   |     -      |     -     |   4.73   \n",
            "   2    |   480   |   0.213080   |     -      |     -     |   4.74   \n",
            "   2    |   500   |   0.225198   |     -      |     -     |   4.73   \n",
            "   2    |   520   |   0.201285   |     -      |     -     |   4.74   \n",
            "   2    |   540   |   0.191626   |     -      |     -     |   4.74   \n",
            "   2    |   560   |   0.251715   |     -      |     -     |   4.74   \n",
            "   2    |   580   |   0.212881   |     -      |     -     |   4.74   \n",
            "   2    |   600   |   0.209068   |     -      |     -     |   4.74   \n",
            "   2    |   620   |   0.241599   |     -      |     -     |   4.73   \n",
            "   2    |   640   |   0.252424   |     -      |     -     |   4.74   \n",
            "   2    |   660   |   0.183878   |     -      |     -     |   4.72   \n",
            "   2    |   680   |   0.215080   |     -      |     -     |   4.72   \n",
            "   2    |   700   |   0.229775   |     -      |     -     |   4.74   \n",
            "   2    |   720   |   0.205130   |     -      |     -     |   4.72   \n",
            "   2    |   740   |   0.233657   |     -      |     -     |   4.75   \n",
            "   2    |   760   |   0.209933   |     -      |     -     |   4.73   \n",
            "   2    |   780   |   0.202521   |     -      |     -     |   4.74   \n",
            "   2    |   800   |   0.207619   |     -      |     -     |   4.72   \n",
            "   2    |   820   |   0.204975   |     -      |     -     |   4.77   \n",
            "   2    |   840   |   0.170824   |     -      |     -     |   4.70   \n",
            "   2    |   860   |   0.234545   |     -      |     -     |   4.73   \n",
            "   2    |   880   |   0.241634   |     -      |     -     |   4.72   \n",
            "   2    |   900   |   0.204623   |     -      |     -     |   4.75   \n",
            "   2    |   920   |   0.193921   |     -      |     -     |   4.73   \n",
            "   2    |   940   |   0.186261   |     -      |     -     |   4.73   \n",
            "   2    |   960   |   0.191604   |     -      |     -     |   4.73   \n",
            "   2    |   980   |   0.211735   |     -      |     -     |   4.75   \n",
            "   2    |  1000   |   0.223221   |     -      |     -     |   4.73   \n",
            "   2    |  1020   |   0.217816   |     -      |     -     |   4.72   \n",
            "   2    |  1040   |   0.146757   |     -      |     -     |   4.74   \n",
            "   2    |  1060   |   0.203479   |     -      |     -     |   4.72   \n",
            "   2    |  1080   |   0.241054   |     -      |     -     |   4.74   \n",
            "   2    |  1100   |   0.202519   |     -      |     -     |   4.75   \n",
            "   2    |  1120   |   0.195460   |     -      |     -     |   4.74   \n",
            "   2    |  1140   |   0.214604   |     -      |     -     |   4.74   \n",
            "   2    |  1160   |   0.190964   |     -      |     -     |   4.74   \n",
            "   2    |  1180   |   0.212656   |     -      |     -     |   4.76   \n",
            "   2    |  1200   |   0.212397   |     -      |     -     |   4.74   \n",
            "   2    |  1220   |   0.262911   |     -      |     -     |   4.73   \n",
            "   2    |  1240   |   0.225723   |     -      |     -     |   4.73   \n",
            "   2    |  1260   |   0.203520   |     -      |     -     |   4.74   \n",
            "   2    |  1280   |   0.210632   |     -      |     -     |   4.75   \n",
            "   2    |  1300   |   0.201559   |     -      |     -     |   4.74   \n",
            "   2    |  1320   |   0.204301   |     -      |     -     |   4.74   \n",
            "   2    |  1340   |   0.209987   |     -      |     -     |   4.75   \n",
            "   2    |  1360   |   0.202395   |     -      |     -     |   4.74   \n",
            "   2    |  1380   |   0.246208   |     -      |     -     |   4.73   \n",
            "   2    |  1400   |   0.209329   |     -      |     -     |   4.75   \n",
            "   2    |  1420   |   0.216940   |     -      |     -     |   4.75   \n",
            "   2    |  1440   |   0.285163   |     -      |     -     |   4.74   \n",
            "   2    |  1460   |   0.203967   |     -      |     -     |   4.74   \n",
            "   2    |  1480   |   0.233433   |     -      |     -     |   4.75   \n",
            "   2    |  1500   |   0.183767   |     -      |     -     |   4.74   \n",
            "   2    |  1520   |   0.209038   |     -      |     -     |   4.73   \n",
            "   2    |  1540   |   0.217810   |     -      |     -     |   4.74   \n",
            "   2    |  1560   |   0.208453   |     -      |     -     |   4.75   \n",
            "   2    |  1580   |   0.168250   |     -      |     -     |   4.74   \n",
            "   2    |  1600   |   0.193816   |     -      |     -     |   4.76   \n",
            "   2    |  1620   |   0.200668   |     -      |     -     |   4.77   \n",
            "   2    |  1640   |   0.245510   |     -      |     -     |   4.76   \n",
            "   2    |  1660   |   0.172249   |     -      |     -     |   4.79   \n",
            "   2    |  1680   |   0.196813   |     -      |     -     |   4.75   \n",
            "   2    |  1700   |   0.201041   |     -      |     -     |   4.76   \n",
            "   2    |  1720   |   0.184723   |     -      |     -     |   4.70   \n",
            "   2    |  1740   |   0.212252   |     -      |     -     |   4.75   \n",
            "   2    |  1749   |   0.261207   |     -      |     -     |   2.13   \n",
            "----------------------------------------------------------------------\n",
            "   2    |    -    |   0.212018   |  0.260322  |   89.85   |  429.35  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   3    |   20    |   0.145198   |     -      |     -     |   4.88   \n",
            "   3    |   40    |   0.124275   |     -      |     -     |   4.74   \n",
            "   3    |   60    |   0.131387   |     -      |     -     |   4.75   \n",
            "   3    |   80    |   0.139978   |     -      |     -     |   4.74   \n",
            "   3    |   100   |   0.134617   |     -      |     -     |   4.75   \n",
            "   3    |   120   |   0.137717   |     -      |     -     |   4.73   \n",
            "   3    |   140   |   0.140680   |     -      |     -     |   4.75   \n",
            "   3    |   160   |   0.140529   |     -      |     -     |   4.75   \n",
            "   3    |   180   |   0.125667   |     -      |     -     |   4.74   \n",
            "   3    |   200   |   0.137040   |     -      |     -     |   4.74   \n",
            "   3    |   220   |   0.107308   |     -      |     -     |   4.75   \n",
            "   3    |   240   |   0.120961   |     -      |     -     |   4.74   \n",
            "   3    |   260   |   0.117152   |     -      |     -     |   4.75   \n",
            "   3    |   280   |   0.136958   |     -      |     -     |   4.76   \n",
            "   3    |   300   |   0.119302   |     -      |     -     |   4.75   \n",
            "   3    |   320   |   0.150036   |     -      |     -     |   4.75   \n",
            "   3    |   340   |   0.105685   |     -      |     -     |   4.76   \n",
            "   3    |   360   |   0.148238   |     -      |     -     |   4.76   \n",
            "   3    |   380   |   0.120948   |     -      |     -     |   4.75   \n",
            "   3    |   400   |   0.148856   |     -      |     -     |   4.75   \n",
            "   3    |   420   |   0.135416   |     -      |     -     |   4.73   \n",
            "   3    |   440   |   0.128322   |     -      |     -     |   4.75   \n",
            "   3    |   460   |   0.149094   |     -      |     -     |   4.75   \n",
            "   3    |   480   |   0.144932   |     -      |     -     |   4.75   \n",
            "   3    |   500   |   0.130787   |     -      |     -     |   4.75   \n",
            "   3    |   520   |   0.135469   |     -      |     -     |   4.75   \n",
            "   3    |   540   |   0.141998   |     -      |     -     |   4.74   \n",
            "   3    |   560   |   0.155209   |     -      |     -     |   4.74   \n",
            "   3    |   580   |   0.096605   |     -      |     -     |   4.74   \n",
            "   3    |   600   |   0.146348   |     -      |     -     |   4.74   \n",
            "   3    |   620   |   0.139986   |     -      |     -     |   4.76   \n",
            "   3    |   640   |   0.131780   |     -      |     -     |   4.74   \n",
            "   3    |   660   |   0.133218   |     -      |     -     |   4.74   \n",
            "   3    |   680   |   0.118194   |     -      |     -     |   4.75   \n",
            "   3    |   700   |   0.096472   |     -      |     -     |   4.77   \n",
            "   3    |   720   |   0.117367   |     -      |     -     |   4.75   \n",
            "   3    |   740   |   0.124947   |     -      |     -     |   4.74   \n",
            "   3    |   760   |   0.144729   |     -      |     -     |   4.74   \n",
            "   3    |   780   |   0.140072   |     -      |     -     |   4.76   \n",
            "   3    |   800   |   0.172498   |     -      |     -     |   4.75   \n",
            "   3    |   820   |   0.148295   |     -      |     -     |   4.77   \n",
            "   3    |   840   |   0.143955   |     -      |     -     |   4.76   \n",
            "   3    |   860   |   0.129465   |     -      |     -     |   4.75   \n",
            "   3    |   880   |   0.151139   |     -      |     -     |   4.73   \n",
            "   3    |   900   |   0.133374   |     -      |     -     |   4.75   \n",
            "   3    |   920   |   0.139950   |     -      |     -     |   4.73   \n",
            "   3    |   940   |   0.175831   |     -      |     -     |   4.75   \n",
            "   3    |   960   |   0.124915   |     -      |     -     |   4.76   \n",
            "   3    |   980   |   0.135306   |     -      |     -     |   4.75   \n",
            "   3    |  1000   |   0.183258   |     -      |     -     |   4.75   \n",
            "   3    |  1020   |   0.106244   |     -      |     -     |   4.76   \n",
            "   3    |  1040   |   0.120656   |     -      |     -     |   4.77   \n",
            "   3    |  1060   |   0.129083   |     -      |     -     |   4.75   \n",
            "   3    |  1080   |   0.110822   |     -      |     -     |   4.75   \n",
            "   3    |  1100   |   0.138117   |     -      |     -     |   4.75   \n",
            "   3    |  1120   |   0.118681   |     -      |     -     |   4.75   \n",
            "   3    |  1140   |   0.149445   |     -      |     -     |   4.74   \n",
            "   3    |  1160   |   0.113880   |     -      |     -     |   4.75   \n",
            "   3    |  1180   |   0.125843   |     -      |     -     |   4.75   \n",
            "   3    |  1200   |   0.198105   |     -      |     -     |   4.75   \n",
            "   3    |  1220   |   0.139412   |     -      |     -     |   4.76   \n",
            "   3    |  1240   |   0.146828   |     -      |     -     |   4.75   \n",
            "   3    |  1260   |   0.124629   |     -      |     -     |   4.75   \n",
            "   3    |  1280   |   0.135580   |     -      |     -     |   4.74   \n",
            "   3    |  1300   |   0.127161   |     -      |     -     |   4.75   \n",
            "   3    |  1320   |   0.134037   |     -      |     -     |   4.75   \n",
            "   3    |  1340   |   0.178362   |     -      |     -     |   4.75   \n",
            "   3    |  1360   |   0.142470   |     -      |     -     |   4.74   \n",
            "   3    |  1380   |   0.156025   |     -      |     -     |   4.74   \n",
            "   3    |  1400   |   0.130397   |     -      |     -     |   4.78   \n",
            "   3    |  1420   |   0.112679   |     -      |     -     |   4.77   \n",
            "   3    |  1440   |   0.118253   |     -      |     -     |   4.73   \n",
            "   3    |  1460   |   0.131825   |     -      |     -     |   4.75   \n",
            "   3    |  1480   |   0.146441   |     -      |     -     |   4.75   \n",
            "   3    |  1500   |   0.122590   |     -      |     -     |   4.76   \n",
            "   3    |  1520   |   0.163789   |     -      |     -     |   4.73   \n",
            "   3    |  1540   |   0.122343   |     -      |     -     |   4.75   \n",
            "   3    |  1560   |   0.153063   |     -      |     -     |   4.74   \n",
            "   3    |  1580   |   0.185803   |     -      |     -     |   4.74   \n",
            "   3    |  1600   |   0.118593   |     -      |     -     |   4.76   \n",
            "   3    |  1620   |   0.107549   |     -      |     -     |   4.75   \n",
            "   3    |  1640   |   0.123671   |     -      |     -     |   4.76   \n",
            "   3    |  1660   |   0.110011   |     -      |     -     |   4.75   \n",
            "   3    |  1680   |   0.109415   |     -      |     -     |   4.74   \n",
            "   3    |  1700   |   0.132668   |     -      |     -     |   4.76   \n",
            "   3    |  1720   |   0.123301   |     -      |     -     |   4.75   \n",
            "   3    |  1740   |   0.135357   |     -      |     -     |   4.75   \n",
            "   3    |  1749   |   0.153266   |     -      |     -     |   2.14   \n",
            "----------------------------------------------------------------------\n",
            "   3    |    -    |   0.134867   |  0.305023  |   89.75   |  430.35  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   4    |   20    |   0.074150   |     -      |     -     |   4.89   \n",
            "   4    |   40    |   0.076206   |     -      |     -     |   4.75   \n",
            "   4    |   60    |   0.055879   |     -      |     -     |   4.75   \n",
            "   4    |   80    |   0.095853   |     -      |     -     |   4.72   \n",
            "   4    |   100   |   0.097923   |     -      |     -     |   4.74   \n",
            "   4    |   120   |   0.084334   |     -      |     -     |   4.74   \n",
            "   4    |   140   |   0.074300   |     -      |     -     |   4.75   \n",
            "   4    |   160   |   0.066989   |     -      |     -     |   4.75   \n",
            "   4    |   180   |   0.057610   |     -      |     -     |   4.78   \n",
            "   4    |   200   |   0.062036   |     -      |     -     |   4.73   \n",
            "   4    |   220   |   0.064379   |     -      |     -     |   4.74   \n",
            "   4    |   240   |   0.090822   |     -      |     -     |   4.75   \n",
            "   4    |   260   |   0.047839   |     -      |     -     |   4.77   \n",
            "   4    |   280   |   0.058738   |     -      |     -     |   4.75   \n",
            "   4    |   300   |   0.078668   |     -      |     -     |   4.75   \n",
            "   4    |   320   |   0.067004   |     -      |     -     |   4.75   \n",
            "   4    |   340   |   0.043475   |     -      |     -     |   4.76   \n",
            "   4    |   360   |   0.064083   |     -      |     -     |   4.75   \n",
            "   4    |   380   |   0.051301   |     -      |     -     |   4.74   \n",
            "   4    |   400   |   0.070175   |     -      |     -     |   4.76   \n",
            "   4    |   420   |   0.076967   |     -      |     -     |   4.74   \n",
            "   4    |   440   |   0.065430   |     -      |     -     |   4.76   \n",
            "   4    |   460   |   0.082162   |     -      |     -     |   4.75   \n",
            "   4    |   480   |   0.075120   |     -      |     -     |   4.72   \n",
            "   4    |   500   |   0.051536   |     -      |     -     |   4.76   \n",
            "   4    |   520   |   0.050332   |     -      |     -     |   4.73   \n",
            "   4    |   540   |   0.080679   |     -      |     -     |   4.76   \n",
            "   4    |   560   |   0.065914   |     -      |     -     |   4.74   \n",
            "   4    |   580   |   0.084901   |     -      |     -     |   4.76   \n",
            "   4    |   600   |   0.100613   |     -      |     -     |   4.74   \n",
            "   4    |   620   |   0.057238   |     -      |     -     |   4.76   \n",
            "   4    |   640   |   0.072681   |     -      |     -     |   4.75   \n",
            "   4    |   660   |   0.084530   |     -      |     -     |   4.75   \n",
            "   4    |   680   |   0.101285   |     -      |     -     |   4.74   \n",
            "   4    |   700   |   0.071982   |     -      |     -     |   4.76   \n",
            "   4    |   720   |   0.048245   |     -      |     -     |   4.75   \n",
            "   4    |   740   |   0.085636   |     -      |     -     |   4.75   \n",
            "   4    |   760   |   0.095527   |     -      |     -     |   4.75   \n",
            "   4    |   780   |   0.079147   |     -      |     -     |   4.71   \n",
            "   4    |   800   |   0.086324   |     -      |     -     |   4.73   \n",
            "   4    |   820   |   0.063011   |     -      |     -     |   4.73   \n",
            "   4    |   840   |   0.074316   |     -      |     -     |   4.76   \n",
            "   4    |   860   |   0.064559   |     -      |     -     |   4.75   \n",
            "   4    |   880   |   0.069507   |     -      |     -     |   4.78   \n",
            "   4    |   900   |   0.080956   |     -      |     -     |   4.73   \n",
            "   4    |   920   |   0.090806   |     -      |     -     |   4.76   \n",
            "   4    |   940   |   0.044550   |     -      |     -     |   4.75   \n",
            "   4    |   960   |   0.095979   |     -      |     -     |   4.78   \n",
            "   4    |   980   |   0.042373   |     -      |     -     |   4.75   \n",
            "   4    |  1000   |   0.080718   |     -      |     -     |   4.76   \n",
            "   4    |  1020   |   0.079545   |     -      |     -     |   4.76   \n",
            "   4    |  1040   |   0.061587   |     -      |     -     |   4.74   \n",
            "   4    |  1060   |   0.069493   |     -      |     -     |   4.74   \n",
            "   4    |  1080   |   0.074409   |     -      |     -     |   4.76   \n",
            "   4    |  1100   |   0.086467   |     -      |     -     |   4.77   \n",
            "   4    |  1120   |   0.079772   |     -      |     -     |   4.74   \n",
            "   4    |  1140   |   0.050904   |     -      |     -     |   4.76   \n",
            "   4    |  1160   |   0.053187   |     -      |     -     |   4.76   \n",
            "   4    |  1180   |   0.045928   |     -      |     -     |   4.74   \n",
            "   4    |  1200   |   0.072857   |     -      |     -     |   4.76   \n",
            "   4    |  1220   |   0.085420   |     -      |     -     |   4.74   \n",
            "   4    |  1240   |   0.047561   |     -      |     -     |   4.76   \n",
            "   4    |  1260   |   0.070611   |     -      |     -     |   4.70   \n",
            "   4    |  1280   |   0.073257   |     -      |     -     |   4.77   \n",
            "   4    |  1300   |   0.066110   |     -      |     -     |   4.74   \n",
            "   4    |  1320   |   0.066858   |     -      |     -     |   4.77   \n",
            "   4    |  1340   |   0.039662   |     -      |     -     |   4.74   \n",
            "   4    |  1360   |   0.096823   |     -      |     -     |   4.75   \n",
            "   4    |  1380   |   0.060061   |     -      |     -     |   4.75   \n",
            "   4    |  1400   |   0.070669   |     -      |     -     |   4.74   \n",
            "   4    |  1420   |   0.045050   |     -      |     -     |   4.75   \n",
            "   4    |  1440   |   0.088483   |     -      |     -     |   4.76   \n",
            "   4    |  1460   |   0.066668   |     -      |     -     |   4.75   \n",
            "   4    |  1480   |   0.090532   |     -      |     -     |   4.75   \n",
            "   4    |  1500   |   0.077763   |     -      |     -     |   4.75   \n",
            "   4    |  1520   |   0.075689   |     -      |     -     |   4.75   \n",
            "   4    |  1540   |   0.058529   |     -      |     -     |   4.76   \n",
            "   4    |  1560   |   0.069447   |     -      |     -     |   4.75   \n",
            "   4    |  1580   |   0.061262   |     -      |     -     |   4.75   \n",
            "   4    |  1600   |   0.064303   |     -      |     -     |   4.76   \n",
            "   4    |  1620   |   0.080245   |     -      |     -     |   4.75   \n",
            "   4    |  1640   |   0.050807   |     -      |     -     |   4.76   \n",
            "   4    |  1660   |   0.048542   |     -      |     -     |   4.75   \n",
            "   4    |  1680   |   0.084409   |     -      |     -     |   4.75   \n",
            "   4    |  1700   |   0.072496   |     -      |     -     |   4.74   \n",
            "   4    |  1720   |   0.063256   |     -      |     -     |   4.75   \n",
            "   4    |  1740   |   0.081251   |     -      |     -     |   4.75   \n",
            "   4    |  1749   |   0.114699   |     -      |     -     |   2.14   \n",
            "----------------------------------------------------------------------\n",
            "   4    |    -    |   0.070743   |  0.362769  |   90.04   |  430.41  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   5    |   20    |   0.039795   |     -      |     -     |   4.89   \n",
            "   5    |   40    |   0.033043   |     -      |     -     |   4.74   \n",
            "   5    |   60    |   0.026576   |     -      |     -     |   4.75   \n",
            "   5    |   80    |   0.041138   |     -      |     -     |   4.76   \n",
            "   5    |   100   |   0.027207   |     -      |     -     |   4.75   \n",
            "   5    |   120   |   0.050417   |     -      |     -     |   4.75   \n",
            "   5    |   140   |   0.045148   |     -      |     -     |   4.75   \n",
            "   5    |   160   |   0.031597   |     -      |     -     |   4.75   \n",
            "   5    |   180   |   0.032829   |     -      |     -     |   4.75   \n",
            "   5    |   200   |   0.054155   |     -      |     -     |   4.76   \n",
            "   5    |   220   |   0.021758   |     -      |     -     |   4.74   \n",
            "   5    |   240   |   0.020612   |     -      |     -     |   4.76   \n",
            "   5    |   260   |   0.024164   |     -      |     -     |   4.76   \n",
            "   5    |   280   |   0.042184   |     -      |     -     |   4.76   \n",
            "   5    |   300   |   0.048221   |     -      |     -     |   4.75   \n",
            "   5    |   320   |   0.036448   |     -      |     -     |   4.74   \n",
            "   5    |   340   |   0.040659   |     -      |     -     |   4.75   \n",
            "   5    |   360   |   0.029944   |     -      |     -     |   4.76   \n",
            "   5    |   380   |   0.038338   |     -      |     -     |   4.77   \n",
            "   5    |   400   |   0.035856   |     -      |     -     |   4.75   \n",
            "   5    |   420   |   0.037826   |     -      |     -     |   4.76   \n",
            "   5    |   440   |   0.021261   |     -      |     -     |   4.75   \n",
            "   5    |   460   |   0.030156   |     -      |     -     |   4.76   \n",
            "   5    |   480   |   0.038868   |     -      |     -     |   4.75   \n",
            "   5    |   500   |   0.035045   |     -      |     -     |   4.74   \n",
            "   5    |   520   |   0.017415   |     -      |     -     |   4.75   \n",
            "   5    |   540   |   0.037555   |     -      |     -     |   4.76   \n",
            "   5    |   560   |   0.022256   |     -      |     -     |   4.75   \n",
            "   5    |   580   |   0.034529   |     -      |     -     |   4.75   \n",
            "   5    |   600   |   0.051945   |     -      |     -     |   4.76   \n",
            "   5    |   620   |   0.038014   |     -      |     -     |   4.75   \n",
            "   5    |   640   |   0.023412   |     -      |     -     |   4.75   \n",
            "   5    |   660   |   0.043119   |     -      |     -     |   4.75   \n",
            "   5    |   680   |   0.047511   |     -      |     -     |   4.75   \n",
            "   5    |   700   |   0.032224   |     -      |     -     |   4.76   \n",
            "   5    |   720   |   0.035993   |     -      |     -     |   4.76   \n",
            "   5    |   740   |   0.034469   |     -      |     -     |   4.76   \n",
            "   5    |   760   |   0.032730   |     -      |     -     |   4.76   \n",
            "   5    |   780   |   0.039700   |     -      |     -     |   4.75   \n",
            "   5    |   800   |   0.032620   |     -      |     -     |   4.76   \n",
            "   5    |   820   |   0.036407   |     -      |     -     |   4.74   \n",
            "   5    |   840   |   0.038595   |     -      |     -     |   4.74   \n",
            "   5    |   860   |   0.029316   |     -      |     -     |   4.79   \n",
            "   5    |   880   |   0.035901   |     -      |     -     |   4.78   \n",
            "   5    |   900   |   0.039495   |     -      |     -     |   4.74   \n",
            "   5    |   920   |   0.036317   |     -      |     -     |   4.75   \n",
            "   5    |   940   |   0.042484   |     -      |     -     |   4.77   \n",
            "   5    |   960   |   0.046132   |     -      |     -     |   4.75   \n",
            "   5    |   980   |   0.035685   |     -      |     -     |   4.77   \n",
            "   5    |  1000   |   0.032088   |     -      |     -     |   4.76   \n",
            "   5    |  1020   |   0.056370   |     -      |     -     |   4.76   \n",
            "   5    |  1040   |   0.028376   |     -      |     -     |   4.76   \n",
            "   5    |  1060   |   0.029329   |     -      |     -     |   4.75   \n",
            "   5    |  1080   |   0.026453   |     -      |     -     |   4.76   \n",
            "   5    |  1100   |   0.047546   |     -      |     -     |   4.76   \n",
            "   5    |  1120   |   0.019047   |     -      |     -     |   4.75   \n",
            "   5    |  1140   |   0.021861   |     -      |     -     |   4.74   \n",
            "   5    |  1160   |   0.038500   |     -      |     -     |   4.76   \n",
            "   5    |  1180   |   0.042364   |     -      |     -     |   4.75   \n",
            "   5    |  1200   |   0.028565   |     -      |     -     |   4.76   \n",
            "   5    |  1220   |   0.025280   |     -      |     -     |   4.75   \n",
            "   5    |  1240   |   0.040893   |     -      |     -     |   4.75   \n",
            "   5    |  1260   |   0.016995   |     -      |     -     |   4.74   \n",
            "   5    |  1280   |   0.019968   |     -      |     -     |   4.76   \n",
            "   5    |  1300   |   0.038609   |     -      |     -     |   4.75   \n",
            "   5    |  1320   |   0.027620   |     -      |     -     |   4.76   \n",
            "   5    |  1340   |   0.025632   |     -      |     -     |   4.75   \n",
            "   5    |  1360   |   0.029510   |     -      |     -     |   4.76   \n",
            "   5    |  1380   |   0.030835   |     -      |     -     |   4.75   \n",
            "   5    |  1400   |   0.028569   |     -      |     -     |   4.75   \n",
            "   5    |  1420   |   0.048918   |     -      |     -     |   4.75   \n",
            "   5    |  1440   |   0.026063   |     -      |     -     |   4.76   \n",
            "   5    |  1460   |   0.024514   |     -      |     -     |   4.75   \n",
            "   5    |  1480   |   0.041522   |     -      |     -     |   4.75   \n",
            "   5    |  1500   |   0.023258   |     -      |     -     |   4.76   \n",
            "   5    |  1520   |   0.032820   |     -      |     -     |   4.76   \n",
            "   5    |  1540   |   0.015736   |     -      |     -     |   4.76   \n",
            "   5    |  1560   |   0.033100   |     -      |     -     |   4.76   \n",
            "   5    |  1580   |   0.023091   |     -      |     -     |   4.76   \n",
            "   5    |  1600   |   0.043157   |     -      |     -     |   4.75   \n",
            "   5    |  1620   |   0.033981   |     -      |     -     |   4.76   \n",
            "   5    |  1640   |   0.025769   |     -      |     -     |   4.76   \n",
            "   5    |  1660   |   0.029726   |     -      |     -     |   4.75   \n",
            "   5    |  1680   |   0.026446   |     -      |     -     |   4.77   \n",
            "   5    |  1700   |   0.031805   |     -      |     -     |   4.76   \n",
            "   5    |  1720   |   0.026151   |     -      |     -     |   4.74   \n",
            "   5    |  1740   |   0.022743   |     -      |     -     |   4.74   \n",
            "   5    |  1749   |   0.017088   |     -      |     -     |   2.14   \n",
            "----------------------------------------------------------------------\n",
            "   5    |    -    |   0.033371   |  0.444900  |   90.03   |  430.77  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=EPOCHS)\n",
        "train(bert_classifier, train_loader, val_loader, epochs=EPOCHS, evaluation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(bert_classifier.state_dict(), 'bert_cla_db2.ckpt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_data = TensorDataset(test_dataset[0],test_dataset[1],test_dataset[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "开始测试...\n"
          ]
        }
      ],
      "source": [
        "print('开始测试...')\n",
        "bert_classifier.eval()\n",
        "test_result = []\n",
        "for data in test_data:\n",
        "    b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in data)\n",
        "    b_input = b_input_ids.unsqueeze(0)  \n",
        "    b_attn_mask = b_attn_mask.unsqueeze(0)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = bert_classifier(b_input,b_attn_mask)\n",
        "        pre = outputs.argmax(dim=1)\n",
        "        test_result.append([b_labels.item(), pre.item(), tokenizer.convert_ids_to_tokens(b_input_ids)])\n",
        "\n",
        "# 写入csv文件\n",
        "df = pd.DataFrame(test_result)\n",
        "df.to_csv('test_result_db2.csv',index=False, header=['id', 'label','text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_csv('test_result_db2.csv',index=False, header=['id', 'label','text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>['[CLS]', '好', '多', '次', '这', '样', '了', '[SEP]...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>['[CLS]', '我', '申', '请', '了', '大', '同', '飞', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>['[CLS]', '没', '答', '应', '[UNK]', '[SEP]', '[P...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>['[CLS]', '开', '卡', '登', '记', '的', '不', '是', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>['[CLS]', '上', '班', '不', '去', '车', '间', '没', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15939</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>['[CLS]', '我', '想', '问', '一', '下', '那', '个', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15946</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>['[CLS]', '毕', '节', '大', '方', '欧', '曼', '打', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15983</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>['[CLS]', '每', '月', '用', '不', '完', '的', '套', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15987</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>['[CLS]', '[UNK]', '[SEP]', '[PAD]', '[PAD]', ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15990</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>['[CLS]', '彩', '云', '飞', '信', '广', '东', '手', '...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1642 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       id  label                                               text\n",
              "2       0      2  ['[CLS]', '好', '多', '次', '这', '样', '了', '[SEP]...\n",
              "3       2      0  ['[CLS]', '我', '申', '请', '了', '大', '同', '飞', '...\n",
              "8       0      2  ['[CLS]', '没', '答', '应', '[UNK]', '[SEP]', '[P...\n",
              "28      0      2  ['[CLS]', '开', '卡', '登', '记', '的', '不', '是', '...\n",
              "41      0      2  ['[CLS]', '上', '班', '不', '去', '车', '间', '没', '...\n",
              "...    ..    ...                                                ...\n",
              "15939   0      2  ['[CLS]', '我', '想', '问', '一', '下', '那', '个', '...\n",
              "15946   2      0  ['[CLS]', '毕', '节', '大', '方', '欧', '曼', '打', '...\n",
              "15983   0      2  ['[CLS]', '每', '月', '用', '不', '完', '的', '套', '...\n",
              "15987   1      0  ['[CLS]', '[UNK]', '[SEP]', '[PAD]', '[PAD]', ...\n",
              "15990   0      2  ['[CLS]', '彩', '云', '飞', '信', '广', '东', '手', '...\n",
              "\n",
              "[1642 rows x 3 columns]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('test_result_db2.csv')\n",
        "df[df.id!=df.label]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "7qCrKzdaZGEq"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.897375"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df[df.id==df.label])/len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of train.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "ce613df70ec087c2b4dda2bc280e25d341f72f59d81afb32edf1d298cbbb8087"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('bert')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
